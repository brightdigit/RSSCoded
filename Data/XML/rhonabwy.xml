<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>Rhonabwy</title>
	<atom:link href="https://rhonabwy.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://rhonabwy.com</link>
	<description>Software development and daily life in Seattle</description>
	<lastBuildDate>Sat, 29 May 2021 19:13:01 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>
<cloud domain='rhonabwy.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<image>
		<url>https://s0.wp.com/i/buttonw-com.png</url>
		<title>Rhonabwy</title>
		<link>https://rhonabwy.com</link>
	</image>
	<atom:link rel="search" type="application/opensearchdescription+xml" href="https://rhonabwy.com/osd.xml" title="Rhonabwy" />
	<atom:link rel='hub' href='https://rhonabwy.com/?pushpress=hub'/>
	<item>
		<title>Concurrency, Combine, and Swift 5.5</title>
		<link>https://rhonabwy.com/2021/05/29/concurrency-combine-and-swift-5-5/</link>
					<comments>https://rhonabwy.com/2021/05/29/concurrency-combine-and-swift-5-5/#respond</comments>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 29 May 2021 18:18:03 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[actors]]></category>
		<category><![CDATA[concurrency]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=6083</guid>

					<description><![CDATA[I started a post that brings together all the moving parts that have been discussed in the various concurrency proposals that are going into Swift 5.5. They&#8217;re all accessible through GitHub, and the discussions in the public forums. The combined view of all the moving parts is complex. I was aiming to post something this<a class="more-link" href="https://rhonabwy.com/2021/05/29/concurrency-combine-and-swift-5-5/">Continue reading <span class="screen-reader-text">"Concurrency, Combine, and Swift&#160;5.5"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I started a post that brings together all the moving parts that have been discussed in the various concurrency proposals that are going into Swift 5.5. They&#8217;re all accessible through <a href="https://github.com/apple/swift-evolution/tree/main/proposals">GitHub</a>, and the discussions in the public forums. The combined view  of all the moving parts is complex. I was aiming to post something this weekend, but in the end I trashed the draft after I read Paul Hudson&#8217;s <a href="https://www.hackingwithswift.com/articles/233/whats-new-in-swift-5-5">What&#8217;s New in Swift 5.5</a>. He has outdone himself with a beautiful job of describing, and pulling together, all the moving parts, and I think there&#8217;s little at this stage that I could add — anything I wrote would be a poor shadow, repeating the work he put into that overview. If you haven&#8217;t yet read it, do so. It&#8217;s a stellar overview and detailed description (with examples!) of a lot of the moving parts that we&#8217;ll see this year. And I&#8217;m sure it sets the foundation for quite a bit more to come!</p>



<p>I started closely following the updates to the Swift language, partially because there&#8217;s an obvious overlap with what has been proposed and how Combine operates. Some of what/how Combine does it&#8217;s processing is getting getting pushed down into the language, which is frankly amazing. I don&#8217;t know how Apple&#8217;s <a href="https://developer.apple.com/documentation/combine/">Combine framework</a> will evolve, but I fully expect it to embrace the async/await foundation, as well as the iteration, sequence, and stream-like structures that are in flight. There&#8217;s a massive collection of concurrency proposals and work, and if that alone was what Apple announced and shared at WWDC, it would be huge. I&#8217;m guessing that it&#8217;s only the foundation for quite a lot of other elements, as yet to be seen. So yeah &#8211; I&#8217;m one of the people thinking &#8220;This year&#8217;s gunna be a big one!&#8221; when it comes to WWDC announcements.</p>



<p>For me, the portion of the concurrency proposals that&#8217;s been most intriguing, exciting, and perhaps a little scary, is the addition of <a href="https://github.com/apple/swift-evolution/blob/main/proposals/0306-actors.md">Actors</a> and <a href="https://github.com/apple/swift-evolution/blob/main/proposals/0316-global-actors.md">Global Actors</a>. Building on top of the concurrency features, they provide some amazing coordination and memory-safety constructs that make it far easier to write safe code for an increasingly multi-core system. And I suspect that Actors aren&#8217;t going to be limited to a single device, but that it&#8217;ll be extended to communicating and coordinating across devices. There&#8217;s nothing yet in the public proposals, but I think there&#8217;s still quite a bit more coming.</p>



<p>I&#8217;m looking forward to these hitting the streets, and (hopefully) API use of Global Actors to help provide compiler-enforced expectations around callbacks and functions that need to interact on the main thread. Yeah &#8211; mostly that&#8217;s UI updates. The existing class of bugs related to UI updates on macOS and iOS have been helped by warnings from the amazing Clang <a href="https://clang.llvm.org/docs/ThreadSanitizer.html">Thread Sanitizer</a>, but now it can be enforced &#8211; in Swift at least &#8211; within the compiler. I also expect a surge of complexity due to the annotations and new keywords for concurrency. I suspect it may be overwhelming for a lot of developers, especially new developers getting involved in the platform. While you probably don&#8217;t need it to learn swift, I&#8217;m guessing that it&#8217;ll be pretty front and center to a lot of core app development. And I wouldn&#8217;t be surprised to see a lot of discussion, and confusion, around the techniques of linking the earlier Cocoa delegate call-back style into async methods and functions. I think it may be challenging &#8211; at least at the start.</p>



<p>Like any new tool, I predict that Actors will get over-used in the near future, maybe for quite a while, before collapsing back to a more sane level of usage in constructing software. In any case, I think it&#8217;ll change a lot of conversations about how current software components work together. I hope that the concurrency elements come with equally excellent testing infrastructure and patterns. I&#8217;m still bummed that <a href="https://github.com/pointfreeco/combine-schedulers">Combine-Schedulers</a> wasn&#8217;t something that was included with the Combine framework directly, or at least with XCTest, while at the same time being immensely grateful to the <a href="https://www.pointfree.co">Point•free crew</a> for making that library available.</p>



<p>I&#8217;m not going to make any other WWDC predictions, and I&#8217;m clearing my mental decks for the conference updates and details to come. It&#8217;s getting about time for &#8220;sponge learning&#8221; mode. I&#8217;m looking forward to seeing what so many worked on, and tremendously excited about the potential this year&#8217;s Swift language updates are setting up.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://rhonabwy.com/2021/05/29/concurrency-combine-and-swift-5-5/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Why I don&#8217;t want Xcode on the iPad — macOS and iPadOS</title>
		<link>https://rhonabwy.com/2021/04/24/why-i-dont-want-xcode-on-the-ipad-macos-and-ipados/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 24 Apr 2021 23:14:27 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[ios]]></category>
		<category><![CDATA[ipados]]></category>
		<category><![CDATA[macos]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=6065</guid>

					<description><![CDATA[With the impressive announcement of the latest iPad Pro&#8217;s now being available with the M1 chip, seems like a whole lot of people (in the communities I follow) are talking about the announcement with a general theme of &#8220;WTF are we going to do with that chip in there?&#8221; Often they&#8217;re Apple-platform developers and saying<a class="more-link" href="https://rhonabwy.com/2021/04/24/why-i-dont-want-xcode-on-the-ipad-macos-and-ipados/">Continue reading <span class="screen-reader-text">"Why I don&#8217;t want Xcode on the iPad — macOS and&#160;iPadOS"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>With the impressive announcement of <a href="https://www.apple.com/newsroom/2021/04/apple-unveils-new-ipad-pro-with-m1-chip-and-stunning-liquid-retina-xdr-display/">the latest iPad Pro&#8217;s now being available with the M1 chip</a>, seems like a whole lot of people (in the communities I follow) are talking about the announcement with a general theme of &#8220;WTF are we going to do with that chip in there?&#8221; Often they&#8217;re Apple-platform developers and saying &#8220;Boy, wouldn&#8217;t it be freakin&#8217; awesome if we could use Xcode on the iPad?!&#8221;. I don&#8217;t want Xcode on the iPad. I used to think I did, or that it might be useful, but I&#8217;ve reconsidered. My reasoning stems from the different underlying constraints of iPadOS vs. macOS &#8211; some physical, some philosophical. <strong><em>I don&#8217;t want to these constraints to go away</em>.</strong> I value each in their own context. And I hope that these various different operating systems aren&#8217;t ever fully merged, exclusively choosing one set of constraints over the other.</p>



<h2>Personal vs Multi-user</h2>



<p>iOS — and its younger, larger sibling iPadOS — are all about being a personal device. I love these kinds of devices, using them regularly. These devices are, very intentionally, single-user. My iPhone is MINE, and while I might hand it to someone else, everything on it is associated with me. Any accounts, all the related permissions &#8211; they&#8217;re all about me or echo&#8217;s of me. There&#8217;s no concept of a &#8220;multi-user&#8221; iOS device*.</p>



<p><sub><em>(*) Yes, I know there&#8217;s unix deep under the covers which fully supports a multi-user Operating System. The way the device is exposed to me &#8211; a developer and consumer of it &#8211; is all about a single individual user.</em></sub></p>



<p>macOS, on the other hand, is built from the ground up as a multi-user system. It supports, if you want to use it, multiple people using the same device at the same time. You &#8220;log in&#8221; to macOS, and &#8220;your account&#8221; has all the details about who you are and what permissions, constraints, etc &#8211; you can do. In practice, this ends up mostly being a single person working on a single macOS device, at any given time. The underlying concepts that allow for multiple users are there, dating back to the earliest UNIX shared-system semantics with multiple users. As a side effect of that fundamental concept, macOS supports multiple programs running in parallel as well, which segues into what I think is the major philosophical difference between these two kinds of devices: focus.</p>



<h2>Focus</h2>



<p>Somewhat related to the personal vs. multi-user difference is an element of &#8220;focus&#8221;. I also tend to think of this as overlapping a bit with a concept of &#8220;foreground&#8221; vs. &#8220;background&#8221; applications. iOS and iPadOS are fundamentally more focused devices. The expectation, and design, of the device and operating system is that you&#8217;re doing one thing &#8211; and all of your focus is on that one thing. Everything else is off in the background, or more likely put on a side-shelf out of view and quite possibly paused. </p>



<p>Yeah, there&#8217;s the iPadOS split screen setup that makes my previous statement a hasty generalization. I think it&#8217;s a weak-ass stab at doing more than one thing at once, a worthy attempt to see if would be sufficient , but near useless in most of the scenarios I&#8217;ve tried. The iOS family (iOS, iPadOS, tvOS, and watchOS) all seem to share this concept  of &#8220;doing a single kind of task&#8221;, and translating that into &#8220;focused on a single application&#8221;.</p>



<p>macOS, with it&#8217;s multi-user background, does a bunch of stuff in parallel. There is still a concept of foreground and background, and more usefully a concept of one application that&#8217;s focused and primary. Almost more importantly is how the operating handles the parallel stuff that you might talk about being in the background. The key is that what runs in the background is (generally) under the control of the person using the device. If I want to run half a dozen programs at the same time &#8211; that&#8217;s cool &#8211; the system time-slices to try and give all of them relatively equal waiting and effort. With iOS, you&#8217;re either in the foreground &#8211; and king of the world &#8211; or background &#8211; and extremely limited in what you can do, and often on a clock for the time remaining to do anything. The iOS based operating system can potentially kill a background app at any moment.</p>



<h2>Multiple Windows</h2>



<p>The other way the macOS &#8220;focus&#8221; and &#8220;parallel apps&#8221; concept shows itself is in windowing. On iOS, your app isn&#8217;t resizable, and for all practical purposes, you&#8217;re working in a single window — that takes over all the screen real-estate — to do whatever you&#8217;re working on.</p>



<p>In comparison, macOS has multiple windows, that you can resize and place however you choose. Well, mostly however you choose &#8211; there are limits. The ability to control the size of a window and place it allows you to prioritize the  information contained with it. I end up setting window size and location, almost unconsciously, based on what I&#8217;m doing. You can have a front and center focus, but still have other information — updating and live — easily available with almost no context switching needed. You can choose what you&#8217;re mixing and when. The multiple window paradigm expanded years ago to include the acknowledgement of multiple kinds of spaces &#8211; even switching between them &#8211; as well as multiple physical displays.</p>



<h2>Integrating Knowledge</h2>



<p>The multiple windows — with different apps in various windows, all more-or-less resizable — is why I prefer macOS over iPadOS for almost all of my &#8220;producing&#8221; work. I use multiple programs at once, very frequently because I want to bring together lots of information from disparate sources and use it together. It&#8217;s almost irrespective of what I&#8217;m doing &#8211; being creative, reviewing and analyzes, following up on operational tasks, etc. For me it all boils down to the fact that most of the work I do on a computer is all about integrating knowledge.</p>



<p>Often that combination is a browser, editor, and something else such as a terminal window, spreadsheet, or image editor (such as the super-amazing <a href="https://flyingmeat.com/acorn/">Acorn</a>). Sometimes it&#8217;s multiple separate browser windows and an email program. When I&#8217;m developing and working in Xcode, I frequently have multiple browser windows, a terminal, and Xcode itself all up and operational at the same time. I use the browsers for reading docs, searching Google (or StackOverflow). I often keep others apps running in parallel &#8211; Slack, email, and sometimes Discord. With these apps nearby, but not intrusively visible, I get updates or I can context switch into them to ask a &#8220;WTF am I doing wrong&#8221; question of cohorts available through the nearby applications.</p>



<p>I can&#8217;t begin to do this on iPadOS without a huge amount of friction. Switching contexts and focus on iPadOS and iOS is time consuming and you&#8217;ve got to think about it. It takes so much time and effort, that you&#8217;re breaking out of the context of what you&#8217;re trying do. On top of that, you can&#8217;t really to control the size of the windows on the screen to match the priorities of what you&#8217;re working on &#8211; it&#8217;s more of an &#8220;all or nothing&#8221;.</p>



<p>I think its obvious, by when I&#8217;m doing something that&#8217;s more &#8220;consuming&#8221; focused &#8211; reading, watching, etc &#8211; then iOS based devices can excel. No distractions, and I can focus on just what I&#8217;m trying to read, learn, understand, etc.</p>



<p>What I <em>have</em> found to be effective is using an iPad in addition to macOS. Using the features of <a href="https://www.apple.com/macos/continuity/">Continuity</a> and <a href="https://support.apple.com/guide/mac-help/copy-and-paste-between-devices-mchl70368996/mac">Universal Clipboard</a>, an iPad becomes a separate dedicated screen for a single purpose. It&#8217;s not uncommon for me to have slack, email, marking up a PDF, or a scribble/sketching program (such <a href="https://www.gingerlabs.com">Notability</a> or <a href="https://linea-app.com">Linea Sketch</a>) running on my iPad as almost another monitor for the same system. Then there&#8217;s the even-more personal use of watchOS to interact with the Mac. Authenticating that I&#8217;m there and unlocking macOS, or using it to verify a system permission for doing something. I don&#8217;t know the name of that feature, but I use it all the time.</p>



<p>Not <em>all</em> of my tasks are about integrating knowledge, but almost all my technology or creative work certainly is. In the cases where I&#8217;m working on a single thing and want to exclude all distractions and other information, iOS and iPadOS are beautiful. But that&#8217;s comparatively rare; almost always when I&#8217;m doing something that&#8217;s purely solo-creative such as drawing, writing, taking photo&#8217;s, or filming. Anything where I need, or want, to mix information together, especially any form of collaboration, begs for the multiple windows, sized for their individual tasks, and background programs running on my behalf.</p>



<p>So thanks, but no: I don&#8217;t want Xcode on the iPad.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Second Guessing Yourself</title>
		<link>https://rhonabwy.com/2021/03/28/second-guessing-yourself/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 28 Mar 2021 18:12:25 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[debugging]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=6051</guid>

					<description><![CDATA[I&#8217;m working through the book: Crafting Interpreters. The author — Bob Nystrom — used Java for the first half of the book, a lovely choice, but I wanted to try it using the Swift programming language. Translating it on the fly was a means to exercise my programming and language knowledge muscles, and that does<a class="more-link" href="https://rhonabwy.com/2021/03/28/second-guessing-yourself/">Continue reading <span class="screen-reader-text">"Second Guessing Yourself"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I&#8217;m working through the book: <a href="http://craftinginterpreters.com/">Crafting Interpreters</a>. The author — <a href="https://twitter.com/munificentbob">Bob Nystrom</a> — used Java for the first half of the book, a lovely choice, but I wanted to try it using the <a href="https://swift.org">Swift</a> programming language. Translating it on the fly was a means to exercise my programming and language knowledge muscles, and that does come with challenges. Some <a href="https://rhonabwy.com/2021/03/13/translating-java-into-swift/">translation choices I made</a> worked out well, others have helpfully provided a good learning experience. This post is about happened was while I was hunting, and resolving, programming mistakes this past week.</p>



<p>Recursive functions have long been something that I found difficult. They are a staple of some computer science tasks, and the heart of the techniques that are used in scanning, parsing, and interpreting as shown of this book. It was because I found them difficult that I wanted to do this project; to build on my experience using these techniques.</p>



<p>It shouldn&#8217;t surprise you that I made mistakes while translating from Java examples into Swift. There wasn&#8217;t anything super-esoteric at the start, a little miss here or there. My first run through implementing a <a href="https://en.wikipedia.org/wiki/While_loop">while loop</a> didn&#8217;t actually loop (oops). The one that really frustrated me was a flaw in not properly isolating how I set up environments before invoking enclosing blocks or functions, which meant a little Fibonacci example blew up &#8211; values oscillating instead of growing. It was kind of my worst nightmare, interpreting recursive code within a bunch of compiled recursive code. I made similar errors in the parser, small subtle things, that meant some of the examples wouldn&#8217;t parse.</p>



<h2>Testing and Debugging</h2>



<p>So I backed up, and started writing tests to work the the pieces: scanner, parser, and interpreter. I probably should have built in testing from the start. I think more importantly — I did it when I needed it. The testing provided a framework to repeatedly work sections of code so that I could debug them and understand what was happening. I started out using the debugger in Xcode to work through the issues, but the ten-plus layers of recursively called functions exploded beyond my head&#8217;s capacity to track where I was and what was going on. </p>



<p>I reverted back to more direct tools. I annotated my parser code (and later the interpreter) a flag variable named <code>omgVerbose</code>, printing out each function layer as I stepped into it. With that in place, I could run the test, view the (often quite long) trace, and walk it through for the example. I traced my code in an editor and compared what I thought it should do to what showed in the trace. I even added a little indention mechanism, so that every layer deeper in the recursive parsing would show up as indented. It was what you might call &#8220;ghetto&#8221;, but it got the job done. </p>



<div class="wp-block-media-text alignwide is-stacked-on-mobile"><figure class="wp-block-media-text__media"><img data-attachment-id="6057" data-permalink="https://rhonabwy.com/debug_parsing/" data-orig-file="https://josephheck.files.wordpress.com/2021/03/debug_parsing.png" data-orig-size="742,1048" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="debug_parsing" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2021/03/debug_parsing.png?w=212" data-large-file="https://josephheck.files.wordpress.com/2021/03/debug_parsing.png?w=725" src="https://josephheck.files.wordpress.com/2021/03/debug_parsing.png?w=725" alt="" class="wp-image-6057 size-full" srcset="https://josephheck.files.wordpress.com/2021/03/debug_parsing.png?w=725 725w, https://josephheck.files.wordpress.com/2021/03/debug_parsing.png?w=106 106w, https://josephheck.files.wordpress.com/2021/03/debug_parsing.png?w=212 212w, https://josephheck.files.wordpress.com/2021/03/debug_parsing.png 742w" sizes="(max-width: 725px) 100vw, 725px" /></figure><div class="wp-block-media-text__content">
<p class="has-large-font-size">With the two side by side, I traced what was happening <strong><em>much</em></strong> more easily — the whole point of doing this work. It took some of the stuff that was overwhelming my brain and set it outside, so I could focus on what was changing, and have an immediate reference to what had happened just before. I looked at what happened, what was happening, and made an assessment of what should happen next. Then I compared it to what did happen — and used that to find the spot where I made a programming mistake.</p>
</div></div>



<h2>Do I really know what I know?</h2>



<p>The second guessing hit me while I was debugging the interpreter. It was the worst kind of bug; the kind where the test sample seems to work — it doesn&#8217;t crash or throw an exception — but you get the wrong output. The code snippet was advanced, working the recursive nature of this language I was implementing:</p>



<pre class="wp-block-code"><code>fun fib(n) {
    if (n &lt;= 1) return n;
    return fib(n - 2) + fib(n - 1);
}
for (var i = 0; i &lt; 20; i = i + 1) {
    print fib(i);
}</code></pre>



<p>I added my <code>omgVerbose</code> variable to show me the execution flow within the interpreter. The output from that short little function was incredibly overwhelming. That was the point at which the metaphorical light-bulb <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4a1.png" alt="💡" class="wp-smiley" style="height: 1em; max-height: 1em;" /> went off for me, making it clear and immediately obvious <strong><em>WHY</em></strong> people created debuggers in the first place. The printed execution trace was overwhelming, and a debugger could let me step through it, piece by piece, and inspect. I didn&#8217;t think I was up for making a debugger for my interpreter right then though, so I just fortified myself with a long walk to clear my head, some fresh coffee, and tucked in to trace it through.</p>



<p>I read through the trace, scrolling through my editor with the underlying code, seeing what was executed and what happened next, and trying to nail down where the failure was occurring. While going back to my implementation in Swift, I stared at it so long that the words lost meaning. I struggled so much with finding this error, with my own frustration for not understanding why it was failing, that I started to second guess how even the Swift language was working. For at least an hour, I&#8217;d managed to confuse myself with <a href="https://docs.swift.org/swift-book/LanguageGuide/ErrorHandling.html">exceptions</a> and <a href="https://docs.swift.org/swift-book/LanguageGuide/TheBasics.html#ID330">optionals</a>. At one point, I&#8217;d nearly convinced myself that <code>try</code> before a statement implied that the statement returned an optional value (which it very definitely does <strong>NOT</strong>). I did ultimately find the issue — it had nothing to do with either optional values or error handling. I had made a mistake in scoping when invoking a function within my interpreter &#8211; effects were bleeding between function invocations, but only showing when the same function and parameters were called from the same context. The recursive function I was testing happened to highlight this extremely well.</p>



<p>The point isn&#8217;t the flaw I made, but that anyone can get to second guessing themselves. I&#8217;ve been programming in one language or another for over 30 years, and programming using the Swift language for over five years. I am not the most amazing programming I know, but I&#8217;m comfortably competent. Comfortable enough to ship apps written in it, or reach for the Swift language to explore a problem without worrying about having to learn it first. Even still, I second guessed myself. </p>



<h2>Recovering From Brain Lock</h2>



<p>I suspect it may be <em>more</em> <em>common</em> to encounter this situation when you&#8217;re trying to learn something new. I wasn&#8217;t using this to learn Swift, but to learn techniques about how to build a programming language and interpreter. It is puzzle solving &#8211; exploring a space, making some assumptions, and following the path of where that might go. When you find a mistake, backtrack a bit and take another path. You&#8217;re already in the mode where you&#8217;re questioning your assumptions, so it&#8217;s pretty easy to have that slip back to questioning broader or more baseline knowledge.</p>



<p>This is when it&#8217;s more important than ever to step back and &#8220;get out of your head&#8221;. The good ole &#8216;rubber ducky&#8217; approach isn&#8217;t bad: explain what you think is happening to the rubber duck. Saying it aloud externalizes it enough to illuminate a poor, or mistaken, assumption or prediction. Same thing goes when explaining it to another person &#8211; typing, calling, or whatever &#8211; better even if they have a different perspective from yours and can ask questions you&#8217;re not.</p>



<p>Just as importantly, step away from the problem for a bit. It doesn&#8217;t get easier by just pushing forward when you&#8217;re frustrated and/or confused, <strong><em>it gets</em></strong> <strong><em>harder</em></strong>. Step outside and go for a walk, away from the keyboard and problem. Listen to music, watch a show you enjoy, or the go for the best recovery mechanism of them all: get a good night&#8217;s sleep before coming back to the problem if you can.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2021/03/debug_parsing.png?w=725" medium="image" />
	</item>
		<item>
		<title>Translating Java Into Swift</title>
		<link>https://rhonabwy.com/2021/03/13/translating-java-into-swift/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 13 Mar 2021 21:13:23 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[java]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=6033</guid>

					<description><![CDATA[I&#8217;m working through the online book Crafting Interpreters, (which I highly recommend, if you&#8217;re curious about how such things are built). While going through it, I&#8217;m making a stab at translating the example code in the book (which is in Java) into Swift. This is not something I&#8217;m very familiar with, so I&#8217;m trying a<a class="more-link" href="https://rhonabwy.com/2021/03/13/translating-java-into-swift/">Continue reading <span class="screen-reader-text">"Translating Java Into&#160;Swift"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I&#8217;m working through the online book <a href="http://craftinginterpreters.com/">Crafting Interpreters</a>, (which I highly recommend, if you&#8217;re curious about how such things are built). While going through it, I&#8217;m making a stab at translating the example code in the book (which is in Java) into Swift. This is not something I&#8217;m very familiar with, so I&#8217;m trying a couple different ways of tackling the development. I thought I&#8217;d write about what&#8217;s worked, and where I think I might have painted myself into a corner.</p>



<h3>Replacing the Visitor Pattern with Protocols and Extensions</h3>



<p>A lot of the example code takes advantage of classes and uses a fair number recursive calls. The author tackles some of the awkwardness of that by aggressively using a visitor pattern over Java classes. A clear translation win is leveraging Swift&#8217;s <a href="https://docs.swift.org/swift-book/LanguageGuide/Protocols.html">protocols</a> and <a href="https://docs.swift.org/swift-book/LanguageGuide/Extensions.html">extensions</a> to accommodate the same kind of setup. It makes it extremely easy to define an interface that you&#8217;d otherwise wrap into a visitor pattern (using a protocol), and use an extension to conform classes to the protocol. I think the end result is far easier to understand and read, which is huge for me &#8211; as this is not something I&#8217;ve tried before and I find a lot of the elements difficult to understand to start with.</p>



<h3>Recursive Enumerations Replacing Classes</h3>



<p>I approached this one with the initial thought of &#8220;I&#8217;ve no idea if this&#8217;ll work&#8221;. There are a number of &#8220;representation&#8221; classes that make up the tree structure that you build while parsing and interpreting. The author used a bit of code to generate the classes from a  basic grammar (and then re-uses it as the grammar evolves within the book while we&#8217;re implementing things). This may not have been the wisest choice, but I went for hand-coding these classes, and while I was in there I tried using Enumerations. I have to admit, the section in <a href="https://docs.swift.org/swift-book/LanguageGuide/TheBasics.html">The Swift Programming Language</a> on <a href="https://docs.swift.org/swift-book/LanguageGuide/Enumerations.html">enumerations</a> had an interior section on <a href="https://docs.swift.org/swift-book/LanguageGuide/Enumerations.html#ID536">Recursive Enumerations</a> that used arithmetic expressions as an example. That really pushed me to consider that kind of thing for what in Java were structural classes. I had already known about, and enjoyed using, enumerations with associated types. It seemed like it could match pretty darned closely.</p>



<p>For processing expressions, I think it&#8217;s been a complete win for me. A bit of the code — one of those structural classes — looks akin to this:</p>



<pre class="wp-block-code"><code>public indirect enum Expression {
    case literal(LiteralExpression)
    case unary(UnaryExpression, Expression)
    case binary(Expression, OperatorExpression, Expression)
    case grouping(Expression)
    case variable(Token)
    case assign(Token, Expression)
}</code></pre>



<p>This lines up with the simple grammar rules I was implementing. Not all of the grammar you need works well for this basic structure. I think it&#8217;s been great for representing expressions in particular. </p>



<h3>Results with Explicit Failure Types Replacing Java Exception</h3>



<p><s>Let me start off with:</s> <em>This one may have been a mistake</em>.</p>



<figure class="wp-block-pullquote"><blockquote><p><strong>UPDATE:</strong> Yep, that was definitely a mistake.</p></blockquote></figure>



<p>A lot of the author&#8217;s code uses Java&#8217;s Exceptions as an explicit part of the control flow. You can <em>almost</em> do the same in Swift, but the typing and catching of errors is&#8230; a little stranger. The author was passing additional content back in the errors &#8211; using the exceptions to &#8220;pop all the way out of an evaluation&#8221; which could be many layers of recursion deep. So I thought, what the hell &#8211; lets see how Swift&#8217;s <a href="https://github.com/apple/swift-evolution/blob/master/proposals/0235-add-result.md">Result</a> type works for this.</p>



<p>I took a portion of the interpreter code (the bit that evaluates expressions) and decided to convert it to returning a <code>Result</code> instead of throwing an exception. The protocol I set up for this started as the following:</p>



<pre class="wp-block-code"><code>public protocol Interpretable {
    func evaluate(_ env: Environment) throws -&gt; RuntimeValue
}
</code></pre>



<p>And I updated it to return a <code>Result</code>:</p>



<pre class="wp-block-code"><code>public protocol Interpretable {
    func evaluate(_ env: Environment) -&gt; Result&lt;RuntimeValue, RuntimeError&gt;
}
</code></pre>



<p>In terms of passing back additional data (as associated values within a <code>RuntimeError</code> &#8211; an enumeration), it worked beautifully. I could interrogate and pull out whatever I needed very easily.</p>



<p>The downside is that to access the innards of a returned result means that you have to use <code>switch</code> on it, and then deal with internals as cases. In almost all cases, I found I wanted to simply propagate a failure up from internal functions, so what I really wanted was something more like a JavaScript Promise (or a Swift Promise library) rather than a raw <code>Result</code>. </p>



<p>The <code>Interpretable</code> protocol applies to those recursive enumerations I showed a bit earlier, the end result was indentions-from-hell where I&#8217;m switching on this, that, and the other  &#8211; making the code quite a bit more difficult to read than I think it needs to be. By itself, result doesn&#8217;t add all that much, but when added to switching based on the enumeration that made up the structural representation of the expressions, I had most of the &#8220;work&#8221; indented four to six layers in.</p>



<p>The upside is that everything is explicitly defined. Swift&#8217;s enforcement of exhaustive switch statements makes it so that the compiler &#8220;keeps me honest&#8221;. Just to share a taste of what I inflicted upon myself, here&#8217;s a portion of the <code>evaluate</code> function used within the interpreter:</p>



<pre class="wp-block-code"><code>extension Expression: Interpretable {
    public func evaluate(_ env: Environment) -&gt; Result&lt;RuntimeValue, RuntimeError&gt; {
        switch self {
        case let .literal(litexpr):
            return litexpr.evaluate(env)

        case let .assign(tok, expr):
            switch expr.evaluate(env) {
            case let .success(value):
                do {
                    try env.assign(tok, value)
                    return .success(RuntimeValue.none)
                } catch {
                    return .failure(RuntimeError.undefinedVariable(tok, message: "\(error)"))
                }
            case let .failure(err):
                return .failure(err)
            }
</code></pre>



<p>The layer that uses this, my Parser code, uses the <code>throws</code> type logic, so I have a few places where I end up with an &#8220;impedance mismatch&#8221; (apologies &#8211; you get my way-old idiomatic Electrical Engineering references here sometimes). The <code>Parser</code> class uses the exceptions, and the Expressions that it evaluates return <code>Result</code> types &#8211; so there&#8217;s translation that needs to happen between the two.</p>



<p>I&#8217;m going to keep running with it to see it through, but the jury is definitely out on &#8220;Was this a good idea?&#8221; I have been half-tempted to just go all-in and make the Parser return a result type, but honestly the whole code that ends up something like:</p>



<pre class="wp-block-code"><code>switch result {
case let .success(value):
    // do something with the value
case let .failure(err):
    // often propagating the error:
    return .failure(err)</code></pre>



<p>Just felt so overwhelming that I stopped at the evaluating expressions.</p>



<h3> Swift LOX</h3>



<p>The language I&#8217;m implementing is called LOX,  a creation of the <a href="https://twitter.com/munificentbob">author</a>. The translation is in progress, but operational, and probably flawed in a number of ways. I&#8217;m not all the way through the book &#8211; next up is adding block syntax to the grammar, parser, and interpreter &#8211;  but it&#8217;s coming along reasonably well. </p>



<p>If you want to look further at the translation-atrocities I&#8217;ve committed, feel free: the code is available at GitHub at <a href="https://github.com/heckj/slox">https://github.com/heckj/slox</a>. I&#8217;m going to carry it through at least another chapter or two to get the elements I wanted to learn for another project. Because of that, I expect it&#8217;ll evolve a bit from where it is today, but I may not ever fully enable the language. We&#8217;ll just have to wait and see. </p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Iterating through Strings in Swift</title>
		<link>https://rhonabwy.com/2021/03/06/iterating-through-strings-in-swift/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 06 Mar 2021 18:00:55 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=6021</guid>

					<description><![CDATA[I recently decided to dive into a new bit of learning &#8211; creating my own software language interpreter. No, I&#8217;ve not gone stark raving mad due to COVID isolation, it is an interesting challenge that I wanted to understand better. Over a year ago, I remember Gus mentioning the process of creating an online book<a class="more-link" href="https://rhonabwy.com/2021/03/06/iterating-through-strings-in-swift/">Continue reading <span class="screen-reader-text">"Iterating through Strings in&#160;Swift"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I recently decided to dive into a new bit of learning &#8211; creating my own software language interpreter. No, I&#8217;ve not gone stark raving mad due to COVID isolation, it is an interesting challenge that I wanted to understand better. Over a year ago, I remember <a href="https://shapeof.com/archives/2020/4/crafting_crafting_interpreters.html">Gus mentioning the process of creating an online book</a> in his blog &#8211; that book was <a href="https://craftinginterpreters.com">Crafting Interpreters</a>, and I kept a reference to that site.</p>



<p>I&#8217;ve started working through the book &#8211; except that instead of doing the example code in Java, I&#8217;m converting the examples and content on the fly and implementing it in Swift. I just worked through implementing the scanner portion of the code. That code required me to read through a text file (or string) and get tokens from it. And as it turns out, this was super easy to do in swift, but quite non-obvious for me.</p>



<p>Since its inception the Swift language has changed, I think a couple of times, how it deals with strings. Because it supports the UTF8 strings, you can&#8217;t just iterate through it byte at a time and get what you want. A lot of early (pre-swift 3) examples did some of this, or variations on the theme, but that&#8217;s no longer valid. So a number of examples on StackOverflow tackling this kind of thing are dead code and very out-of-date.</p>



<p>The first, and most obvious way, to iterate through the characters in a string is to use it within a <code>for</code> loop. This is the pattern that you&#8217;ll see directly in <a href="https://docs.swift.org/swift-book/LanguageGuide/TheBasics.html">The Swift Programming Language Guide</a> on <a href="https://docs.swift.org/swift-book/LanguageGuide/StringsAndCharacters.html">Strings and Characters</a>:</p>



<pre class="wp-block-code"><code>for char in yourString {
    print(char)
}</code></pre>



<p>Works great, super efficient&#8230; except, you don&#8217;t have any reference to do some of the tricks that parsers and scanners want to do &#8211; which is peek to see what the next character might be. </p>



<p>The way you can interact with strings in this fashion involve a specific type called <a href="https://developer.apple.com/documentation/swift/string/index">String.Index</a>. The details are in a section just a bit further down: <a href="https://docs.swift.org/swift-book/LanguageGuide/StringsAndCharacters.html#ID494">Accessing and Modifying Strings</a> in that same guide.</p>



<p>Don&#8217;t make the mistake of thinking a <code>String.Index</code> is just a number that you can add and subtract to move around the index. Unicode makes it significantly more tricky than that, and the language represents <code>String.Index</code> as its own separate type, I think partially to  to keep me (and you) from making that mistake.  Fortunately, the standard library offers a couple of properties on each string to give us reference points: <code>startIndex</code> and <code>endIndex</code>. You can step forward to the next index with a methods on the index: <code>String.Index(after: _someIndex)</code>. There&#8217;s also a way to step back &#8211; use <code>String.Index(before: _someIndex)</code>. These allow you to step forward (or back) through a string, character by character, or shift the index location forward a step (or two) that you kind of need when you&#8217;re making a scanner for a language interpretter.</p>



<p>One last mechanism that&#8217;s helpful to know: you can retrieve a substring from the characters between two index locations, using a range expression made up of the two indices. A substring isn&#8217;t quite a string &#8211; it&#8217;s a different type in Swift. Swift plays some tricks with referencing the original string when you&#8217;re using this type, but you an easily make a full string to use as an argument for another function easily enough:</p>



<pre class="wp-block-code"><code>newString = String(firstString&#091;indexA...indexB])</code></pre>



<p>As I mentioned earlier, this is all in the <a href="https://docs.swift.org/swift-book/LanguageGuide/TheBasics.html">Swift language guide</a>. I hate to admit that it hasn&#8217;t been the first place I looked for help and guidance, but it probably should have been. It&#8217;s all there, and more.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Nested Observable Objects in SwiftUI</title>
		<link>https://rhonabwy.com/2021/02/13/nested-observable-objects-in-swiftui/</link>
					<comments>https://rhonabwy.com/2021/02/13/nested-observable-objects-in-swiftui/#comments</comments>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 13 Feb 2021 18:43:20 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[combine]]></category>
		<category><![CDATA[swiftui]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5997</guid>

					<description><![CDATA[This one often starts with the phrase: Hey, why isn&#8217;t my view updating? It shows the initial data, but it doesn&#8217;t update when that data gets changed! &#8230; more than one person, including me &#8230; When you get into seeing the code for the view, how it&#8217;s formed, and what the models look like, you<a class="more-link" href="https://rhonabwy.com/2021/02/13/nested-observable-objects-in-swiftui/">Continue reading <span class="screen-reader-text">"Nested Observable Objects in&#160;SwiftUI"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>This one often starts with the phrase:</p>



<figure class="wp-block-pullquote"><blockquote><p>Hey, why isn&#8217;t my view updating? It shows the initial data, but it doesn&#8217;t update when that data gets changed!</p><cite>&#8230; more than one person, including me &#8230;</cite></blockquote></figure>



<p>When you get into seeing the code for the view, how it&#8217;s formed, and what the models look like, you see a pattern appear: nested objects that conform to <code><a href="https://developer.apple.com/documentation/combine/observableobject">ObservableObject</a></code> with a reference from one to another, a top level object passed into the view, and view elements that follow the dot-notation chain to create the display.</p>



<p>Let me show you some code, a very simplified representation of this pattern:</p>



<pre class="wp-block-code"><code>class MainThing : ObservableObject {
    @Published var element : SomeElement
    init(element : SomeElement) {
        self.element = element
    }
}

class SomeElement : ObservableObject {
    @Published var value : String
    init(value : String) {
        self.value = value
    }
}</code></pre>



<p>And a view that displays it:</p>



<pre class="wp-block-code"><code>struct MainThingView: View {
    @ObservedObject var model : MainThing
    var body: some View {
        HStack {
            Text("Detail:")
            Text(model.element.value)
        }
    }
}</code></pre>



<p>At first blush, this looks fine &#8211; the view displays, and property within the nested view is shown as you&#8217;d expect; <em><strong>so what&#8217;s the problem?</strong></em> The issue is when you update that nested element&#8217;s property, even though it&#8217;s listed as <code>@Published</code>, the change doesn&#8217;t propagate to the view.</p>



<p>I&#8217;ve seen this pattern described as &#8220;nested observable objects&#8221;, and it&#8217;s a subtle quirk of SwiftUI and how the Combine <code>ObservableObject</code> protocol works that can be surprising. You can work around this, and get your view updating with some tweaks to the top level object, but I&#8217;m not sure that I&#8217;d suggest this as a good practice. When you hit this pattern, it&#8217;s a good time to step back and look at the bigger picture. What are you setting up with your views and models, and can you make them a more aligned to a direct representation. Let me explain what&#8217;s happening, how you can work around it, and you can judge for yourself.</p>



<p>This pattern is at the intersection of Combine and SwiftUI, and specific to classes. There are two parts to it, the first of which is the <a href="https://developer.apple.com/documentation/combine/observableobject"><code>ObservableObject</code> protocol</a>, and the second part is one or more properties on that object that have the <a href="https://developer.apple.com/documentation/combine/published"><code>@Published</code> property wrapper</a>. An object that conforms to the observable object protocol has a publisher on it with a specific name: <code>objectWillChange</code>. You can dig around a bit, and you&#8217;ll find what it publishes may not be what you expect: it isn&#8217;t publishing the values changing, just that something <em>will</em> change. The type aliases for this publisher point to a type signature of <code>Publisher&lt;Void, Never&gt;</code>. Not what I expected when I first uncovered it, and it made me scratch my head.</p>



<p>The idea, as I understand it, is that the publisher is specifically meant to provide a signal that something has changed &#8211; but not the details of what changed. From there, the code within the SwiftUI framework (which uses it), invalidates any relevant view, and looks up what it needs from the referenced object to display a new view.</p>



<p>When most folks use this protocol, they&#8217;re not creating the publisher &#8211; they&#8217;re letting the swift compiler do the heavy lifting, which synthesizes the code to create it, and with the <code>@Published</code> property wrapper, to hook up and watch the properties that should trigger it.</p>



<p>So here&#8217;s the kicker to what&#8217;s happening: The <code>@Published</code> property wrapper watches for the properties to have changed. We&#8217;re dealing with a class here, so we&#8217;re in the world of reference semantics. When you update something in a class, you&#8217;re not updating the reference to the class &#8211; the reference stays the same. That&#8217;s the benefit (and trouble) with reference semantics &#8211; it&#8217;s not entirely obvious that something down below that reference was updated, but as a benefit &#8211; you&#8217;re not having to copy around the world of what&#8217;s in there. If you had replaced the <code>element</code> property with a new instance of <code>SomeElement</code>, then it would trigger the publisher.</p>



<p>So what can you do to work around this? If you&#8217;re really tied to this nested class object structure, then you change the objects a bit to &#8220;manually&#8221; support notifying that publisher chain when the property within a nested object has updated, and you want that reflected in a SwiftUI view. One way to tackle this is to add your own connections from the synthesized publishers on internal <code>@Published</code> property wrappers to the synthesized subject that <code>ObservableObject</code> provides. </p>



<p>The <code>@Published</code> property wrapper synthesizes a publisher for you, referenced with a <code>$</code> preceding the property name. The compiler synthesizes a <code><a href="https://developer.apple.com/documentation/combine/subject">Subject</a></code> for <code>ObservableObject</code>. The subject has a <code>send()</code> method on it that you can invoke. Invoking <code>send()</code> doesn&#8217;t require any arguments – it&#8217;s not sending any specific data &#8211; instead it&#8217;s a trigger to say &#8220;publish the fact that something is changing and views should be invalidated and redisplayed&#8221;. The code below explicitly connects the publisher synthesized within <code>SomeElement</code> to the subject in <code>MainThing</code>.</p>



<pre class="wp-block-code"><code>class MainThing : ObservableObject {
    @Published var element : SomeElement
    var cancellable : AnyCancellable?
    init(element : SomeElement) {
        self.element = element
        self.cancellable = self.element.$value.sink(
            receiveValue: { &#091;weak self] _ in
                self?.objectWillChange.send()
            }
        )
    }
}</code></pre>



<p>There&#8217;s a whole world of problems with this setup: from breaking the idea of encapsulation across objects to the fact that it&#8217;s incredibly fragile. If you change the <code>element</code> property within <code>MainThing</code> to a new instance, you also need to re-establish the publisher chain to the <code>objectWillChange</code> subject. You can manually create your own subject, such as a <code><a href="https://developer.apple.com/documentation/combine/PassthroughSubject">PassthroughSubject</a>&lt;Void, Never&gt;</code> to your top-level class and manage the connections to invoke <code>send()</code> on it.</p>



<p>This whole pattern seems to be a &#8220;<a href="https://en.wikipedia.org/wiki/Code_smell">code smell</a>&#8220;. If you follow this primrose path, you&#8217;ll find yourself triggering the &#8220;invalidate and redisplay&#8221; at a high level, perhaps more often than you want. A large set of those additional connections, especially with a model changing regularly, pretty quickly leads to performance issues, as the effects invalidate larger swaths of view hierarchy for potentially small changes. One or two connections like this won&#8217;t hurt much, but more certainly can.</p>



<p>What seems to be better advice is to look closely at your views, and revise them to make more, and more targeted views. Structure your views so that each view displays a single level of the object structure, matching views to the classes that conform to <code>ObservableObject</code>. In the case above, you could make a view for displaying <code>SomeElement</code> (or even several views) that display&#8217;s the property from it that you want shown. Pass the property <code>element</code> to that view, and let it track the publisher chain for you.</p>



<pre class="wp-block-code"><code>struct FocusedView: View {
    @ObservedObject var element : SomeElement
    var body: some View {
        Text(element.value)
    }
}

struct MainThingView: View {
    @ObservedObject var model : MainThing
    var body: some View {
        HStack {
            Text("Detail:")
            FocusedView(element: model.element)
        }
    }
}</code></pre>



<p>This pattern implies making more, smaller, and focused views, and lets the engine inside SwiftUI do the relevant tracking. Then you don&#8217;t have to deal with the book keeping, and your views potentially get quite a bit simpler as well.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://rhonabwy.com/2021/02/13/nested-observable-objects-in-swiftui/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Integrating SwiftUI Bindings and Combine</title>
		<link>https://rhonabwy.com/2021/02/07/integrating-swiftui-bindings-and-combine/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 07 Feb 2021 19:27:37 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[binding]]></category>
		<category><![CDATA[combine]]></category>
		<category><![CDATA[swiftui]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5971</guid>

					<description><![CDATA[A misconception I had when first learning SwiftUI and Combine was that SwiftUI relied on Combine alone for updating data. There was a throw-away comment in one of the 2019 WWDC presentations (Data Flow through SwiftUI) relating the two, and I over-interpreted it to mean that SwiftUI solely used Combine. To be very clear &#8211;<a class="more-link" href="https://rhonabwy.com/2021/02/07/integrating-swiftui-bindings-and-combine/">Continue reading <span class="screen-reader-text">"Integrating SwiftUI Bindings and&#160;Combine"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>A misconception I had when first learning SwiftUI and Combine was that SwiftUI relied on Combine alone for updating data. There was a throw-away comment in one of the 2019 WWDC presentations (<a href="https://developer.apple.com/wwdc19/226">Data Flow through SwiftUI</a>) relating the two, and I over-interpreted it to mean that SwiftUI solely used Combine. To be very clear &#8211; it doesn&#8217;t. SwiftUI nicely integrates with Combine, and the components you use to expose external reference models into SwiftUI (such as <a href="https://developer.apple.com/documentation/swiftui/observedobject"><code>@ObservedObject</code></a>, <a href="https://developer.apple.com/documentation/swiftui/EnvironmentObject"><code>@EnvironmentObject</code></a>, <a href="https://developer.apple.com/documentation/swiftui/stateobject"><code>@StateObject</code></a>, and <a href="https://developer.apple.com/documentation/combine/published"><code>@Published</code></a>) use it. But quite a lot of the interaction with user interface elements, such as <a href="https://developer.apple.com/documentation/swiftui/text"><code>Text</code></a>, <a href="https://developer.apple.com/documentation/swiftui/toggle"><code>Toggle</code></a>, or the selection in <a href="https://developer.apple.com/documentation/swiftui/list"><code>List</code></a> operate using a different tool: <a href="https://developer.apple.com/documentation/swiftui/binding"><code>Bindings</code></a>.</p>



<p>I think the most important distinction between the two is the direction of data flow. With bindings, the data travels in <strong>BOTH</strong> directions, and in Combine it travels in a <strong>SINGLE</strong> direction. You could describe Combine pipelines as being a &#8220;one way street for data&#8221;, and Bindings as a &#8220;two way street&#8221;.</p>



<p>Bindings are also not set up to manipulate the timing or transform the data; and the operate on individual values &#8211; not streams of data or values over time. That is entirely by design. A core philosophy of the design of SwiftUI is having a single source of truth. A binding exposes that single source of truth, and allows user interface elements to both reflect it and change it. All the while, it keeps the source of truth clear and easily understood. It is designed to support high-speed, low-overheard updates of single values within a view.</p>



<p>So when you start to think something like &#8220;Hey, I&#8217;d really like to tweak the timing on this&#8230;&#8221;, then you&#8217;re in the realm of Combine. An example might be debouncing text field updates and using the result to trigger a more expensive computation, such as a network call to get information. Unfortunately, there&#8217;s not a direct path to create or get a publisher from an existing binding, such as a <code>@State</code> variable within a view.<br><br>So how do you get these things to work together? You can rearrange your code to pass in publishers, or otherwise externalize the data from your view with on of the <code><a href="https://developer.apple.com/documentation/combine/observableobject">ObservableObject</a></code> types. If you want to work on data that the view should own, such as view state, then the best option are a couple of view modifiers: <a href="https://developer.apple.com/documentation/swiftui/view/onreceive(_:perform:)"><code>onReceive(_:perform:)</code></a> and <a href="https://developer.apple.com/documentation/swiftui/view/onchange(of:perform:)"><code>onChange(:of:perform:)</code></a>.</p>



<h2>Publishing to a Binding</h2>



<p>Linking a publisher to a binding, such as a view&#8217;s state variable, is in my mind the easier path. How you arrange it depends on where you create the publisher. If the publisher exists outside the view, pass it into your view on initialization, and use it with the <a href="https://developer.apple.com/documentation/swiftui/view/onreceive(_:perform:)"><code>onReceive(_:perform:)</code></a> view modifier.</p>



<p>The view to which its attached becomes subscriber. Remember that in Combine, the subscriber &#8220;drives all the action&#8221;, so the view is now driving the publisher, for as long as it exists. When the <code><a href="https://developer.apple.com/documentation/swiftui/view/onreceive(_:perform:)">onReceive</a></code> modifier connects to the publisher, it requests <code><a href="https://developer.apple.com/documentation/combine/subscribers/demand/unlimited">unlimited</a></code> demand. When SwiftUI invalidates the view and recreates it, the pipeline will get set up again, and requests additional demand from the publisher. If your publisher does heavier work, such as a network request, you might find this a bit surprising.</p>



<p>If you do run into this, it&#8217;s a good idea to move that publisher, and the work, into an external object that isn&#8217;t owned by the view, but used by it. You can use the publisher from multiple views, and it&#8217;s a perfect place for the Combine <a href="https://developer.apple.com/documentation/combine/publishers/share"><code>share</code></a> operator to use a single publisher from multiple subscribers without needing to replicate the network requests for each one.</p>



<h2>Publishing from a Binding</h2>



<p>Prior to the SwiftUI updates in iOS 14, publishing from a binding was pretty much squashed. Local state declarations, such as a private state variable, don&#8217;t support tacking on your own custom getters and setters, where you could put in a closure to trigger a side effect.</p>



<p>Fortunately, with iOS 14, SwiftUI added the <a href="https://developer.apple.com/documentation/swiftui/view/onchange(of:perform:)">onChange(<code>of:perform:)</code></a> view modifier, perfectly suited for invoking your own closure when a state variable changes. Specify the binding you to which you want to watch, and do your work within a closure you provide. To get from a closure, an imperative style of code, to a Combine publisher, a declarative style of code, means you need to work with something that crosses that boundary. The most common way is using a Combine subject, designed for imperative code to publish values. I tend to use <a href="https://developer.apple.com/documentation/combine/passthroughsubject"><code>passthroughSubject</code></a>, especially within a view, since it&#8217;s lightweight and doesn&#8217;t require references to other values. This makes it easy to use in a declaration, outside of an initializer.</p>



<p>Again, remember that when using Combine, the subscriber is what drives the action. So be wary of thinking of this flow as <em>pushing</em> a value into a publisher chain; that can lead you to making some incorrect assumptions about how things will react. What&#8217;s typically happening is something, somewhere, is already subscribed to the publisher &#8211; and it did so with an unlimited demand &#8211; meaning any values added in get propagated almost immediately.</p>



<h2>Sample Code</h2>



<p>To illustrate how these work, I created a simple view, with both a binding sending a value into a publisher, and another getting updated from a publisher chain. The example includes two state variables: one that is the immediate binding for a <code><a href="https://developer.apple.com/documentation/SwiftUI/TextField">TextField</a></code> that updates as you type, the second is a holding spot for values out of a publisher. </p>



<p>A <code><a href="https://developer.apple.com/documentation/combine/passthroughsubject">PassthroughSubject</a></code> provides the imperative to declarative connection by using <code><a href="https://developer.apple.com/documentation/combine/passthroughsubject/send(_:)">send()</a></code> to publish values, called from within an <code><a href="https://developer.apple.com/documentation/swiftui/view/onchange(of:perform:)">onChange</a></code> view modifier on the <a href="https://developer.apple.com/documentation/SwiftUI/TextField"><code>TextField</code></a>. To debounce the input &#8211; in this case delay updating until you&#8217;ve stopped typing for a second &#8211; the publisher pipeline in the initializer pulls from the subject, uses the <code><a href="https://developer.apple.com/documentation/combine/fail/debounce(for:scheduler:options:)">debounce</a></code> operator, and connects to the state variable <code>delayed</code> using the <code><a href="https://developer.apple.com/documentation/swiftui/view/onreceive(_:perform:)">onReceive</a></code> view modifier.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
import SwiftUI
import Combine

struct PublisherBindingExampleView: View {
    
    @State private var filterText = &quot;&quot;
    @State private var delayed = &quot;&quot;
    
    private var relay = PassthroughSubject&lt;String,Never&gt;()
    private var debouncedPublisher: AnyPublisher&lt;String, Never&gt;
    
    init() {
        self.debouncedPublisher = relay
            .debounce(for: 1, scheduler: RunLoop.main)
            .eraseToAnyPublisher()
    }
    
    var body: some View {
        VStack {
            TextField(&quot;filter&quot;, text: $filterText)
                .onChange(of: filterText, perform: { value in
                    relay.send(value)
                })
            Text(&quot;Delayed result: \(delayed)&quot;)
                .onReceive(debouncedPublisher, perform: { value in
                    delayed = value
                })
        }
    }
}
</pre></div>


<p>You might be tempted to put the combine pipeline directly on the subject, but if you chained any operators, then you would be changing the type of the subject. That would effectively hide it and make it impossible to get a reference to the <code>send()</code> method.</p>



<p>Likewise, you might want to move the publisher chain up to be a declaration, but there&#8217;s a quirk here: initialization order. Since you need to reference another local variable (<code>relay</code> in this case), you need to set up that chain from inside a convenience initializer so that the other variables are already initialized and available to use. If you tried to just set it up in the declaration, the compiler would hand you an error such as:</p>



<pre class="wp-block-preformatted">Cannot use instance member 'relay' within property initializer; property initializers run before 'self' is available</pre>



<p>You might say, &#8220;Hey, but what about that neat trick with lazy initialization?&#8221;. In this case, it doesn&#8217;t conveniently work. When you use a variable declared with <code>lazy</code> within the <code><a href="https://developer.apple.com/documentation/swiftui/view/onreceive(_:perform:)">onReceive</a></code> view modifier, you get the error:</p>



<pre class="wp-block-preformatted">Cannot use mutating getter on immutable value: 'self' is immutable</pre>



<p>A lazy property mutates an object on its first invocation, so it&#8217;s always considered <em>mutating</em>, which doesn&#8217;t work with SwiftUI views, which are immutable.</p>



<p>As a final note, keep what you set up and process within a view&#8217;s initializer to an absolute minimum. SwiftUI views are intended to be light and ephemeral &#8211; and SwiftUI invalidates and recreates them, somethings quite often, so minimize any work you do within them. If, for example, you find yourself reaching to set up a publisher that makes a network request to get some data, seriously consider refactoring your code to externalize that request into a helper object that provides a publisher, rather than have the view repeatedly set it up each time.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>The evolution of &#8220;safe&#8221; and &#8220;unsafe&#8221; in the Swift programming language</title>
		<link>https://rhonabwy.com/2021/02/03/the-evolution-of-safe-and-unsafe-in-the-swift-programming-language/</link>
					<comments>https://rhonabwy.com/2021/02/03/the-evolution-of-safe-and-unsafe-in-the-swift-programming-language/#comments</comments>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Thu, 04 Feb 2021 02:59:02 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[concurrency]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5953</guid>

					<description><![CDATA[There&#8217;s been a lot of motion in the last four months of the evolution of the Swift programming language that I&#8217;ve been wanting, waiting, and hoping for. The language maintainers are tackling concurrency as a first-class construct in the language. I&#8217;m following along with the language evolution proposals in the forums, and so far have<a class="more-link" href="https://rhonabwy.com/2021/02/03/the-evolution-of-safe-and-unsafe-in-the-swift-programming-language/">Continue reading <span class="screen-reader-text">"The evolution of &#8220;safe&#8221; and &#8220;unsafe&#8221; in the Swift programming&#160;language"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>There&#8217;s been a lot of motion in the last four months of the evolution of the Swift programming language that I&#8217;ve been wanting, waiting, and hoping for. The language maintainers are tackling concurrency as a first-class construct in the language. I&#8217;m following along with <a href="https://forums.swift.org/c/evolution/">the language evolution proposals in the forums</a>, and so far have mostly been able to keep up with the details &#8211; although I&#8217;m sure I&#8217;m missing a lot of the fine-grained implications. Reading the forum pitches where people are talking them through has been fascinating.</p>



<p>One of the interesting take-aways is that the terms &#8220;safe&#8221; and &#8220;unsafe&#8221;, or at least the specific implications of when they&#8217;re used in the swift language, are broadening what they cover with the upcoming changes. You could start to see it as early as last October when the <a href="https://forums.swift.org/t/swift-concurrency-roadmap/">Swift Concurrency Roadmap</a> was published, but the wording wasn&#8217;t fully in place, more of just conceptual frameworks. The details of the broadening of the definition didn&#8217;t hit home for me until I caught up with the recent discussion on the pitch for <a href="https://forums.swift.org/t/concurrency-pitch-task-local-values/">task local values</a>.</p>



<p>Prior to these language extensions, &#8220;safe&#8221; implied something pretty narrow and specific: memory safety. It started with some swift-language specific guarantees about variables being guaranteed to be initialized before use. There’s a <a href="https://developer.apple.com/swift/blog/?id=28">Swift.org blog post from back in 2015</a> that calls this out, and far more detail in <a href="https://docs.swift.org/swift-book/LanguageGuide/MemorySafety.html">The Swift Programming Language book’s chapter on Memory Safety</a>. There&#8217;s an even better explanation and detail in the presentation <a href="https://developer.apple.com/videos/play/wwdc2020/10648/">Unsafe Swift</a> from WWDC 20. The heart of it being that the &#8220;safe&#8221; APIs have more preconditions and guarantees wrapped around them.</p>



<p>Across the recent pitches and proposals, some of the language terms that use safe are now being used to imply concurrency safety, somewhat independently of memory safety. The goal looks to be to provide APIs that have some guarantees about thread-safe access and updates. And along with the safe versions, there are some potential &#8220;unsafe&#8221; variants to use when you need the escape hatch and are willing to take on the thread safety guarantees yourself.</p>



<p>There is likely going to be some detail about what it means to be safe in the upcoming 3rd pitch for Structured Concurrency (both <a href="https://forums.swift.org/t/concurrency-pitch-task-local-values/42829/42">the task local values pitch</a> and the <a href="https://forums.swift.org/t/pitch-2-structured-concurrency/43452/132">2nd structured concurrency pitch</a> mention another round of updates and details for the <a href="https://github.com/DougGregor/swift-evolution/blob/structured-concurrency/proposals/nnnn-structured-concurrency.md">structured concurrency proposal</a>. As I&#8217;m writing this, it&#8217;s clear there&#8217;s still a lot of active and careful work going on there.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://rhonabwy.com/2021/02/03/the-evolution-of-safe-and-unsafe-in-the-swift-programming-language/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>M1 arm64 native OpenSSL with vcpkg</title>
		<link>https://rhonabwy.com/2021/01/18/m1-arm64-native-openssl-with-vcpkg/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Mon, 18 Jan 2021 23:31:34 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[vcpkg]]></category>
		<category><![CDATA[cmake]]></category>
		<category><![CDATA[openssl]]></category>
		<category><![CDATA[arm64]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5931</guid>

					<description><![CDATA[This article isn&#8217;t a how-to so much as a debugging/dev diary entry for future-me, and any other soul who stumbles into the same (or similar) issues. Let me provide the backdrop for this story: I&#8217;m working on a private C++ language based project, previously written to be cross platform (Windows, Linux, and Mac). It has<a class="more-link" href="https://rhonabwy.com/2021/01/18/m1-arm64-native-openssl-with-vcpkg/">Continue reading <span class="screen-reader-text">"M1 arm64 native OpenSSL with&#160;vcpkg"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>This article isn&#8217;t a how-to so much as a debugging/dev diary entry for future-me, and any other soul who stumbles into the same (or similar) issues.</p>



<p>Let me provide the backdrop for this story:</p>



<p>I&#8217;m working on a private C++ language based project, previously written to be cross platform (Windows, Linux, and Mac). It has a number of C++ library dependencies, which it&#8217;s managing with <a href="https://vcpkg.readthedocs.io/en/latest/">vcpkg</a>, a fairly nice library package manager solution for C++ projects. It happens to align well with this project, which uses <a href="https://cmake.org">CMake</a> as its build system. One of the dependencies that this project uses is <a href="https://grpc.io">grpc</a>, which in turn has a transitive dependency on <a href="https://www.openssl.org">OpenSSL</a>.</p>



<p>With the M1 series of laptops available from Apple, we wanted to compile and use this same code as an M1 <code>arm64</code> native binary. Sure &#8211; makes sense, should be easy. The good news is, it mostly has been. Both vcpkg and openssl recently had updates to resolve compilation issues with M1/arm based Macs, most of which revolved around a (common) built-in assumption that macOS meant you were building for an <code>x86_64</code> architecture, or maybe, just maybe, cross-compiling for iOS. The part that hasn&#8217;t been so smooth is there&#8217;s an odd complication with vcpkg, openssl, and the M1 Macs that ends up with a linker error when the build system tries to integrate and link the binaries created and managed by vcpkg. It boils down to this:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
ld: in /Users/heckj/bin/vcpkg/installed/arm64-osx/debug/lib/libcrypto.a(a_strex.o), building for macOS, but linking in object file built for iOS
</pre></div>


<p>For anyone else hitting this sort of thing, there&#8217;s two macOS specific command-line tools that you should know about to investigate this kind of thing: <code>lipo</code> and <code>otool</code>.</p>



<h2>lipo</h2>



<p><code>lipo</code> does a number of things &#8211; mostly around merging various archives into fat libraries, but the key part I&#8217;ve been using it for is to determine what architecture a library was built to support. The command <code>lipo -info /path/to/library.a</code>, tells you the architecture for that library. I stashed a copy of vcpkg in <code>~/bin</code> on my laptop, so using the command <code>lipo -info ~/bin/vcpkg/installed/arm64-osx/lib/libcrypto.a</code> reports the following:</p>



<pre class="wp-block-code"><code>Non-fat file: /Users/heckj/bin/vcpkg/installed/arm64-osx/lib/libcrypto.a is architecture: arm64</code></pre>



<p>Prior to OpenSSL version 1.1.1i, that reported an <code>x86_64</code> binary.</p>



<h2>otool</h2>



<p>As I&#8217;ve learned, architecture alone isn&#8217;t sufficient for C++ code when linking the library (at least on macOS). When the libraries are created (compiled), the libraries are also marked with information about what platform they were built for. This is a little harder to dig out, and where the command line tool <code>otool</code> comes into play. I found some great detail on Apple&#8217;s Developer forum in the thread at <a href="https://developer.apple.com/forums/thread/662611">https://developer.apple.com/forums/thread/662611</a>, which describes using <code>otool</code>, but not quite all the detail. Here&#8217;s the quick summary:</p>



<p>You can view the platform that is embedded into the library code directly using <code>otool -lv</code>. Now this generates a lot of output, and you&#8217;re looking for some specific patterns. For example, the command</p>



<pre class="wp-block-code"><code>otool -lv ~/bin/vcpkg/installed/arm64-osx/debug/lib/libcrypto.a&nbsp; | grep -A5 LC_</code></pre>



<p>includes this stanza in the (copious) output:</p>



<pre class="wp-block-code"><code>      cmd LC_VERSION_MIN_IPHONEOS
  cmdsize 16
  version 5.0
      sdk n/a
Load command 2</code></pre>



<p>And as far as I&#8217;ve been able to discern, if you see <code>LC_VERSION_MIN_IPHONEOS</code> in the output, it means the library was built for an iOS platform, and you&#8217;ll get the linker error I listed above when you try to link it to code built for macOS.</p>



<p>Another library which did get compiled &#8220;correctly&#8221; shows the following stanza within its output:</p>



<pre class="wp-block-code"><code>      cmd LC_BUILD_VERSION
  cmdsize 24
 platform MACOS
    minos 11.0
      sdk 11.1
   ntools 0</code></pre>



<p>Spotting <code>LC_BUILD_VERSION</code> and then the details following it shows the library can be linked against macOS code build for version 11.0 or later.</p>



<h2>Debugging</h2>



<p>After a number of false starts and deeper digging, I found that OpenSSL 1.1.1i included a patch that enabled arm64 macOS compilation. The patch <a href="https://github.com/openssl/openssl/pull/12369">https://github.com/openssl/openssl/pull/12369</a> specifically enables a new platform code: <code>darwin64-arm64-cc</code>.</p>



<p>The <a href="https://github.com/microsoft/vcpkg">vcpkg</a> codebase grabbed this update recently, with patch <a href="https://github.com/microsoft/vcpkg/pull/15298">https://github.com/microsoft/vcpkg/pull/15298</a>, but even with this patch in place, the build was failing with the error above.</p>



<p>I grabbed OpenSSL from <a href="http://github.com/openssl/openssl/issues">its source</a>, and started poking around. A lot of that poking and learning the innards of how OpenSSL does its builds wasn&#8217;t entirely useful, so I won&#8217;t detail all the dead ends I attempted to follow. What I did learn, in the end, was that the terminal &#8211; being native <code>arm64</code> or <a href="https://developer.apple.com/documentation/apple_silicon/about_the_rosetta_translation_environment">rosetta2</a> emulated <code>x86_64</code> &#8211; can make a huge difference. For convince, I made a copy of iTerm and ran it under rosetta so that I could easily install <a href="https://brew.sh">homebrew</a> and use all those various tools, even though it wasn&#8217;t arm64 native.</p>



<p>What I found is that if I want to make a macOS native version of OpenSSL, <strong><em>I need to run the compilation from a native <code>arm64</code> terminal session</em></strong>. Something is inferring the target platform – not sure what – from the shell in which it runs. I manually configured and compiled OpenSSL with an <code>arm64</code> native terminal, and was able to get an arm64 library, with the internal markers for macOS.</p>



<p>From there, I thought that perhaps this was an issue of how OpenSSL was configured and built. That&#8217;s something that vcpkg controls, with these little snippets of cmake under the covers. <a href="https://github.com/microsoft/vcpkg">vcpkg</a> does a nice job of keeping logs, so I went through the compilation for openssl (using <code>--triplet arm64-osx</code> in case you want to follow along), and grabbed all the configuration and compilation steps. I grabbed those logs and put them into their own text file, and edited them so I could run them step by step in my own terminal window and see the status of the output as it went. I then adapted the steps to reference a clean git repository checkout from openssl and the tag <code>OpenSSL_1_1_1i</code>, and updated the prefix to a separate directory (<code>/Users/heckj/openssl_arm64-osx/debug</code>) so I could inspect things without stepping into the vcpkg spaces.</p>



<pre class="wp-block-code"><code>cd /Users/heckj/src/openssl
git reset --hard OpenSSL_1_1_1i
export CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc
export AR=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ar
export LD=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ld
export RANLIB=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib
export MAKE=/usr/bin/make
export MAKEDEPPROG=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc
export PATH=$PATH:/Users/heckj/bin/vcpkg/downloads/tools/ninja-1.10.1-osx
/usr/bin/perl Configure no-shared enable-static-engine no-zlib no-ssl2 no-idea no-bf no-cast no-seed no-md2 no-tests darwin64-arm64-cc --prefix=/Users/heckj/openssl_arm64-osx/debug --openssldir=/etc/ssl -fPIC "--sysroot=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.1.sdk"
/usr/local/Cellar/cmake/3.19.3/bin/cmake -DDIR=/Users/heckj/src/openssl -P /Users/heckj/bin/vcpkg/ports/openssl/unix/remove-deps.cmake
# &#091;2/3]
cd /Users/heckj/src/openssl
export PATH=$PATH:/Users/heckj/bin/vcpkg/downloads/tools/ninja-1.10.1-osx
/usr/local/Cellar/cmake/3.19.3/bin/cmake -E touch /Users/heckj/src/openssl/krb5.h
/usr/bin/make build_libs</code></pre>



<blockquote class="wp-block-quote"><p>What I found was the if I ran this code under a terminal running under rosetta, I&#8217;d get the results that indicated the code was built against an iOS platform. And when I ran it under a native <code>arm64</code> terminal, it would correctly report macOS as the platform.</p><cite>This is a major insight, but I haven&#8217;t yet figured out how to apply it&#8230;</cite></blockquote>



<p>I originally installed and compiled vcpkg using the rosetta terminal, and it was running as an <code>x86_64</code> binary, so I thought perhaps that was the issue. Unfortunately, not. After I installed vcpkg with the <code>arm64</code> native (and verified the binary was <code>arm64</code> with the <code>lipo -info</code> command), I made another run at installing openssl, but ended up with the same iOS linked binary.</p>



<p>Prior to getting this far, I opened <a href="https://github.com/openssl/openssl/issues/13854">issue 13854 as a question on the OpenSSL repository</a>, which details some of this story. However, I now longer think that&#8217;s an issue, as I was able to get an arm64 native binary when I manually compiled things. There might be something OpenSSL could do to make this easier/better, but its build setup is incredibly complex and I get lost pretty darn quickly within it.</p>



<p>So to date, I&#8217;ve trailed this back to some interaction that <a href="https://github.com/microsoft/vcpkg">vcpkg</a>, and <code>x86_64</code> emulated binaries, are having on the build &#8211; but that&#8217;s it.</p>



<p>The story ends here, as I don&#8217;t have a solution. I have filed <a href="https://github.com/microsoft/vcpkg/issues/15741">issue 15741</a> with the vcpkg project, with a summary of these details.</p>



<p>For anyone reading until the bitter end, I&#8217;d love any suggestions on how to fully resolve this. I hope that someone stumbles across the issue at some point with more knowledge than I and has a solution in the future. In the meantime, this blog post will hopefully record the error and how you can diagnose the architecture that a library is compiled for, even if it doesn&#8217;t solve the end problem of getting you to a final resolution.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Priming and Pedagogy</title>
		<link>https://rhonabwy.com/2021/01/11/priming-and-pedagogy/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Mon, 11 Jan 2021 19:06:07 +0000</pubDate>
				<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[ML]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5928</guid>

					<description><![CDATA[A new year&#8217;s wander through a machine learning research paper led me down a really interesting rabbit role. Somewhere down the hole, I found a reference and have since been reading Thinking Fast and Slow, by Daniel Kahneman. The book was referenced by several AI/ML researchers, which is what got me started there. Chapter 4,<a class="more-link" href="https://rhonabwy.com/2021/01/11/priming-and-pedagogy/">Continue reading <span class="screen-reader-text">"Priming and Pedagogy"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>A new year&#8217;s wander through a machine learning research paper led me down a really interesting rabbit role. Somewhere down the hole, I found a reference and have since been reading <a href="https://a.co/3Fz1kke">Thinking Fast and Slow</a>, by <a href="https://scholar.princeton.edu/kahneman/home">Daniel Kahneman</a>. The book was referenced by several AI/ML researchers, which is what got me started there.</p>



<p> Chapter 4, titled <em>The Associated Machine, </em>is an interesting example of this book. It talks about the psychological effect of priming, and then goes on to illustrate it in the book with simple examples you can practically do while you&#8217;re reading the chapter. That kind of example brings the concept home, makes it super concrete and far easier to understand. That&#8217;s a delightful mechanism to find in a book where you&#8217;re reading to learn.</p>



<p>That chapter, both its content and how it presented the topic, spurred a wacky question in my head:</p>



<p>Are there priming techniques that could be used in technical documentation, either subtly or directly, that makes it easier for the reader to read and retain the content?</p>



<p>I don&#8217;t know if there is such a thing. From the examples in the book, I can see where there&#8217;s definitely priming that could be done that could work against taking up details, but I&#8217;m not coming up an inverse. The closest I could imagine was a relatively simple predictive puzzle of some sort, just something to get you in a puzzle-solving frame of mind, and then tackling the learning. Of course, if you&#8217;re one that hates puzzles that&#8217;s is likely going to &#8220;crash and burn&#8221; as a technique.</p>



<p>I&#8217;ve no grand conclusions, only more questions at this stage. Although I do highly recommend the book if you&#8217;re interested in the processes of cognition, associating, and how they relate to our everyday brain capabilities.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Creating Machine Learning Models with CreateML</title>
		<link>https://rhonabwy.com/2020/12/22/creating-machine-learning-models-with-createml/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Wed, 23 Dec 2020 03:15:52 +0000</pubDate>
				<category><![CDATA[AI]]></category>
		<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[CreateML]]></category>
		<category><![CDATA[ML]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5884</guid>

					<description><![CDATA[I have been following the bare outlines of building, and using, machine learning models in Apple&#8217;s software ecosystem for a while. Most of my learning and personal research has been with foundational technologies &#8211; following some of the frameworks (TensorFlow, PyTorch, SciKit-Learn) and some of the advances in models and their results. Until this holiday,<a class="more-link" href="https://rhonabwy.com/2020/12/22/creating-machine-learning-models-with-createml/">Continue reading <span class="screen-reader-text">"Creating Machine Learning Models with&#160;CreateML"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I have been following the bare outlines of building, and using, machine learning models in Apple&#8217;s software ecosystem for a while. Most of my learning and personal research has been with foundational technologies &#8211; following some of the frameworks (<a href="https://www.tensorflow.org">TensorFlow</a>, <a href="https://pytorch.org">PyTorch</a>, <a href="https://scikit-learn.org/stable/">SciKit-Learn</a>) and some of the advances in models and their results. Until this holiday, I had not applied myself to seeing the latest evolution of Apple&#8217;s machine learning frameworks and tooling. I scanned through websites last summer during Apple&#8217;s <a href="https://developer.apple.com/wwdc20/">WWDC</a>, but didn&#8217;t really clue in to the changes that were coming, and that are now available, with <a href="https://www.apple.com/macos/big-sur/">Big Sur</a> (macOS 11).</p>



<blockquote class="wp-block-quote"><p>Aside: The web site <a href="https://paperswithcode.com">Papers With Code</a> has been a great consolidated reference point for digging into the academic research behind quite a variety of machine learning advancements over the past couple of years, and includes both the research papers (often linked to <a href="https://arxiv.org">ArXiv</a>) and frequently references to the code matching the papers.</p></blockquote>



<p>First off, the tooling to create simple models with <a href="https://developer.apple.com/documentation/createml">CreateML</a> has received quite a boost. Matched by the documentation, there are a large number of new models that can be easily generated with you &#8220;just&#8221; providing the data. Models to classify sounds, motion data, and tabular data &#8211; including regressors (predictors) as well as classifiers. Prior to macOS 11, they had models for classifying and tagging words and sentences, as well as image classification. Those all still exist, and Apple provides some ready-to-use models built-in to frameworks such as <a href="https://developer.apple.com/documentation/naturallanguage">Natural Language</a>. </p>



<p>The goal I chose to explore with was focused on natural language processing &#8211; more on that later. Apple has long had a pretty good natural language library available, the earlier stuff being a bit more focused on Latent Semantic Mapping, but recently turning to &#8211; and leveraging &#8211; quite a bit of the natural language processing advances that have happened using more recent machine learning techniques.</p>



<p>The most interesting win that I immediately saw was the effectiveness of using transfer learning with CreateML. When you&#8217;re making a model (at least the word tagger model), you have the option of using a CRF (<a href="https://en.wikipedia.org/wiki/Conditional_random_field">conditional random field</a>) or applying the data with a transfer learning over existing models that Apple has in place. They don&#8217;t really tell you anything about how these models are built, or what goes into choosing the internals, but the bare results from a few simple experiments are positive, and quite obviously so.</p>



<p>I made sample models that mapped language <a href="https://universaldependencies.org/u/dep/">dependency mapping</a> from some publicly available datasets provided by <a href="https://universaldependencies.org">Universal Dependencies</a>. Dependency maps specify how the words relate to each other, and the part that I was specifically interested in was leveraging the ability to identify a few of those specific relationships: <a href="https://universaldependencies.org/u/dep/nsubj.html">nsubj:pass</a> and <a href="https://universaldependencies.org/u/dep/aux_.html">aux:pass</a>. These are the tell-tales for sentences that use passive voice. I did multiple passes at making and training models, and transfer learning was clearly more effective (for my models) in terms of the reported accuracy and evaluation.</p>



<h2>Training a Model</h2>



<p>One of my training runs used approximately 5000 labeled data points (from <a href="https://github.com/UniversalDependencies/UD_English-GUM">https://github.com/UniversalDependencies/UD_English-GUM</a>). About 4200 of those points I allocated for training, and another separate file with 800 data points for testing. In this case, a &#8220;data point&#8221; is a full or partial sentence, with each word mapped to an appropriate dependency relationship. The CRF model topped out at 81% accuracy, while the transfer learning model reached up to 89% accuracy.</p>



<figure class="wp-block-image size-large"><a href="https://josephheck.files.wordpress.com/2020/12/crf-1.png"><img data-attachment-id="5895" data-permalink="https://rhonabwy.com/crf-1/" data-orig-file="https://josephheck.files.wordpress.com/2020/12/crf-1.png" data-orig-size="1674,622" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="crf-1" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/12/crf-1.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2020/12/crf-1.png?w=900" src="https://josephheck.files.wordpress.com/2020/12/crf-1.png?w=1024" alt="" class="wp-image-5895" srcset="https://josephheck.files.wordpress.com/2020/12/crf-1.png?w=1024 1024w, https://josephheck.files.wordpress.com/2020/12/crf-1.png?w=150 150w, https://josephheck.files.wordpress.com/2020/12/crf-1.png?w=300 300w, https://josephheck.files.wordpress.com/2020/12/crf-1.png?w=768 768w, https://josephheck.files.wordpress.com/2020/12/crf-1.png 1674w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>Conditional Random Field Model: 11 iterations planned, converging &#8220;early&#8221; at 7 iterations.</figcaption></figure>



<figure class="wp-block-image size-large"><a href="https://josephheck.files.wordpress.com/2020/12/transfer.png"><img data-attachment-id="5893" data-permalink="https://rhonabwy.com/transfer/" data-orig-file="https://josephheck.files.wordpress.com/2020/12/transfer.png" data-orig-size="1650,614" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="transfer" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/12/transfer.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2020/12/transfer.png?w=900" src="https://josephheck.files.wordpress.com/2020/12/transfer.png?w=1024" alt="" class="wp-image-5893" srcset="https://josephheck.files.wordpress.com/2020/12/transfer.png?w=1024 1024w, https://josephheck.files.wordpress.com/2020/12/transfer.png?w=150 150w, https://josephheck.files.wordpress.com/2020/12/transfer.png?w=300 300w, https://josephheck.files.wordpress.com/2020/12/transfer.png?w=768 768w, https://josephheck.files.wordpress.com/2020/12/transfer.png 1650w" sizes="(max-width: 1024px) 100vw, 1024px" /></a><figcaption>Transfer Learning: Dynamic Embedding (25 iterations)</figcaption></figure>



<p>To put the accuracy in perspective, the latest &#8220;state of the art&#8221; parts-of-speech tagging models are running <a href="https://paperswithcode.com/sota/part-of-speech-tagging-on-penn-treebank">around 97 to 98% accuracy</a>, and <a href="https://paperswithcode.com/task/dependency-parsing">around 96% for dependency parsing</a>.</p>



<h2>Timing the Training</h2>



<p>I ran these experiments on one of the new M1 MacBooks with 16GB of memory. This training set took just over a minute to train the CRF model, and 7 minutes to train the transfer learning model. A similar run with more data (27,000 data points) took 20 minutes for the CRF model, and 35 minutes for the transfer learning model. Transfer learning takes longer, but &#8211; in my cases &#8211; resulted in better accuracy for predictions.</p>



<h2>Evaluating the Model</h2>



<p>Once trained, CreateML provides an evaluation panel that gives you precision and recall values for each value that in your tagging set. This information is ideal for understanding how your data actually played out versus what you were trying to achieve. Using it, you can spot weak points and consider how to resolve them. One way might be gathering more exemplar data for those specific classifications. For example, in the following data table, you can see that the recall for &#8220;csubj:pass&#8221; was 0%. This showed that I simply didn&#8217;t have any examples in the test data to validate it, and possibly only a few samples in my training data. If that was a tag I was interested in making sure I could predict from input text &#8211; I could find and improve the input and testing data to improve that accuracy.</p>



<figure class="wp-block-image size-large"><a href="https://josephheck.files.wordpress.com/2020/12/transfer-eval.png"><img data-attachment-id="5898" data-permalink="https://rhonabwy.com/transfer-eval/" data-orig-file="https://josephheck.files.wordpress.com/2020/12/transfer-eval.png" data-orig-size="1216,1370" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="transfer-eval" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/12/transfer-eval.png?w=266" data-large-file="https://josephheck.files.wordpress.com/2020/12/transfer-eval.png?w=900" src="https://josephheck.files.wordpress.com/2020/12/transfer-eval.png?w=909" alt="" class="wp-image-5898" srcset="https://josephheck.files.wordpress.com/2020/12/transfer-eval.png?w=909 909w, https://josephheck.files.wordpress.com/2020/12/transfer-eval.png?w=133 133w, https://josephheck.files.wordpress.com/2020/12/transfer-eval.png?w=266 266w, https://josephheck.files.wordpress.com/2020/12/transfer-eval.png?w=768 768w, https://josephheck.files.wordpress.com/2020/12/transfer-eval.png 1216w" sizes="(max-width: 909px) 100vw, 909px" /></a></figure>



<h2>Previewing Predictions from your Model</h2>



<p>Probably the most fun of using CreateML is the preview mode. Since I was making a word tagging model, it provided a text entry area and then applied the tagger against the data, showing me an example of the predictions against what-ever I typed.</p>



<figure class="wp-block-image size-large"><a href="https://josephheck.files.wordpress.com/2020/12/preview.png"><img data-attachment-id="5902" data-permalink="https://rhonabwy.com/preview/" data-orig-file="https://josephheck.files.wordpress.com/2020/12/preview.png" data-orig-size="1280,1192" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="preview" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/12/preview.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2020/12/preview.png?w=900" src="https://josephheck.files.wordpress.com/2020/12/preview.png?w=1024" alt="" class="wp-image-5902" srcset="https://josephheck.files.wordpress.com/2020/12/preview.png?w=1024 1024w, https://josephheck.files.wordpress.com/2020/12/preview.png?w=150 150w, https://josephheck.files.wordpress.com/2020/12/preview.png?w=300 300w, https://josephheck.files.wordpress.com/2020/12/preview.png?w=768 768w, https://josephheck.files.wordpress.com/2020/12/preview.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>



<p>Since I&#8217;d done two different models in the same project, I could flip back and forth between them to see how they fared against this sample, showing me the predictions from the model and the associated confidence values of those predictions. (and yes, I have considered locking my brother into a kitchen, he&#8217;s a damned fine chef!)</p>



<h2>Exporting a CoreML model from CreateML</h2>



<p>CreateML includes an Output tab that shows you the tags (also known as class labels) that you trained. It also gives you a preview of the metadata associated with the model, as well as kinds of inputs and outputs that the model supports. This makes it nicely clear on what you&#8217;ll need to send, and accept back, when you&#8217;re using the model in your own code.</p>



<figure class="wp-block-image size-large"><a href="https://josephheck.files.wordpress.com/2020/12/model-output.png"><img data-attachment-id="5905" data-permalink="https://rhonabwy.com/model-output/" data-orig-file="https://josephheck.files.wordpress.com/2020/12/model-output.png" data-orig-size="1730,1084" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="model-output" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/12/model-output.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2020/12/model-output.png?w=900" src="https://josephheck.files.wordpress.com/2020/12/model-output.png?w=1024" alt="" class="wp-image-5905" srcset="https://josephheck.files.wordpress.com/2020/12/model-output.png?w=1024 1024w, https://josephheck.files.wordpress.com/2020/12/model-output.png?w=150 150w, https://josephheck.files.wordpress.com/2020/12/model-output.png?w=300 300w, https://josephheck.files.wordpress.com/2020/12/model-output.png?w=768 768w, https://josephheck.files.wordpress.com/2020/12/model-output.png 1730w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></figure>



<p>One of the details that I particularly appreciated was including a metadata field explicitly for the license of the data. The data I used to create this model is public, but licensed to <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode">by-nc-sa-4.0</a> (An attribution, non-commercial, share-alike license). Sourcing data, both quality and licensed use, is a major element in making models. I think it&#8217;s super important to pass that kind of information along clearly and cleanly, so I&#8217;m glad it&#8217;s a default metadata attribute on models from CreateML.</p>



<h2>Notes on CreateML&#8217;s Rough Edges</h2>



<p>While CreateML was super easy to use and apply, it definitely has some issues. The first is that the user interface just doesn&#8217;t feel very &#8220;Mac&#8221; like &#8211; things I expected to be able to &#8220;click on and rename&#8221; didn&#8217;t smoothly operate as such (although I could control-click and choose rename), the window doesn&#8217;t easily or cleanly resize &#8211; so the app dominates your screen wether you want it to or not. On top of that, a queueing feature, and related window, for lining up a bunch of training was quite awkward to use and see the results as it was progressing, and after it was done. I had a couple training runs fail due to poor data, but the queuing window didn&#8217;t show any sort &#8220;well, that didn&#8217;t work&#8221; information &#8211; it just looked like it completed, but there was no training on those models.</p>



<p>The other awkward bit was dealing with messy data through the lens of CreateML. I can&#8217;t really say what it did was &#8220;wrong&#8221;, but it could have been so much better. In one of my experiments, the data I had chosen for training had issues within it: missing labels where the training system expected to see data. That&#8217;s cool &#8211; but the error was reported by a small bit of red text at the bottom of a screen saying &#8220;Entry # 5071 has a problem&#8221;. Finding entry #5071 of a structured JSON data set is, bluntly, a complete pain in the ass. When I&#8217;d parsed and assembled the data per the <a href="https://developer.apple.com/documentation/createml/creating_a_word_tagger_model">online documentation for making a word tagging model</a>, I&#8217;d dumped the data into a monster JSON, single-line data structure with no line breaks. That made finding a specific element using a text editor really rough. In the end, I re-did my JSON export to include pretty-printed JSON, and then also used VSCode&#8217;s &#8220;json outline&#8221; functionality (scaled up, since it defaults to 5000 items), to track down the specific item by position in a list. I found the offending data, and then looking around, noticed a bunch of other areas where the same &#8220;partially tagged data&#8221; existed. In the end I dealt with it by filtered it out if it wasn&#8217;t fully tagged up. It would have been much nicer, especially since CreateML clearly already had and could access the data, if it could have shown me the samples that were an issue &#8211; and notified me that it wasn&#8217;t just one line, but that a number were screwed up.</p>



<p>The evaluation details after you train a model aren&#8217;t readily exportable. As far as I can tell, if you want to share that detail with someone else, you&#8217;re either stuck making a screenshot or transcribing what&#8217;s in the windows. It seems you can&#8217;t export the evaluation data into CSV or or an HTML table format, or really even copy text by entry.</p>



<p>The details about the training process, its training and validation accuracy, number of iterations used, are likewise locked into non-copyable values. In fact, most of the text fields feel sort of &#8220;UIKit&#8221; rather than &#8220;AppKit&#8221; &#8211; in that you can&#8217;t select and copy the details, only get an image with a screenshot. This is a &#8220;not very Mac-like&#8221; experience in my opinion. Hopefully that will get a bit of product-feature-love to encourage sharing and collaboration of the details around CreateML models.</p>



<p>I filed a few feedback notices with Apple for the truly egregious flaws, but I also expect they&#8217;re known, given how easy they were to spot with basic usage. They didn&#8217;t stop the app from being effective or useful, just unfortunate and kind of awkward against normal &#8220;Mac&#8221; expectations.</p>



<p>I do wish the details of what CreateML was doing behind the scenes was more transparent &#8211; a lot of what I&#8217;ve read about in prior research is starting with descriptions of ML models in PyTorch, Keras, or Tensorflow. If I wanted to use those, I&#8217;d need to re-create the models myself with the relevant frameworks, and then use the <a href="https://coremltools.readme.io/docs">CoreML tools</a> library to convert the trained model into a <a href="https://developer.apple.com/documentation/coreml">CoreML</a> model that I could use. By their very nature, the models and details are available if you take that path. It&#8217;s hard to know, by comparison, how that compares to the models created with CreateML. </p>



<h2>Creating a Model with CreateML Versus a Machine Learning Framework</h2>



<p>My early take-away (I&#8217;m very definitely still learning) is that CreateML seems to offer a quick path to making models that are very direct, and one-stage only. If you want to make models that flow data through multiple transforms, combine multiple models, or provide more directed feedback to the models, you&#8217;ll need to step into the world of PyTorch, Keras, and Tensorflow to build and train your models. Then in the end convert the trained models back to CoreML models for use within Apple platform applications.</p>



<p>Where the raw frameworks expose (require you to define) all the details, they also inflict the &#8220;joy&#8221; of <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyper-parameter tuning</a> to train them effectively. That same tuning/choosing process is (I think) happening auto-magically when you build models with CreateML. CreateML chooses the iterations, while paying attention to convergence, iterations, and epochs of applying data. It also appears to do a good job of segmenting data for training, evaluation, and testing &#8211; all of which you&#8217;d need to wrangle yourself (and ideally not screw up) while making a raw machine learning model. That&#8217;s a pretty darned big win, even if it does end up being more of a &#8220;trust me, I know what I&#8217;m doing&#8221; rather than a &#8220;see, here&#8217;s what I did for you&#8221; kind of interaction.</p>



<h2>Final Thoughts on Learning CreateML</h2>



<p>I&#8217;m still very much in &#8220;the early days&#8221; of learning how to build and apply machine learning models, but CreateML has already been an immense help in learning both what&#8217;s possible, and providing hints as to how I might structure my own thinking about using and applying models within apps. </p>



<p>The first is simply that they provide a lot of good, basic models that are directly available to use and experiment with &#8211; assuming you can source the data, and that&#8217;s a <strong><em>big</em></strong> assumption. Data management is not an easy undertaking: including correctly managing the diversity of the sourcing, data licensing, and understanding the bias&#8217; inherent within the data. But assuming you get all that, you can get good &#8211; mostly interactive &#8211; feedback from CreateML. It gives you a strong hint to answer the question &#8220;Will this concept work, or not?&#8221; pretty quickly.</p>



<p>The second is that showing you the outputs from the model makes the inputs you provide, and what you expect to get out, more clear. I think of it as providing a clear API for the model. Models expose a &#8220;here&#8217;s what the model thinks it&#8217;s likely to be &#8211; and how likely&#8221; kind of answer rather than a black-and-white answer with complete assurance. Not that the assurance is warranted with any other system, just that exposing the confidence is an important and significant thing.</p>



<p>If I were tackling a significant machine learning project, I&#8217;d definitely expect to need to include some data management tooling as a part of that project, either browser/web based or app based. It&#8217;s clear that viewing, managing, and diagnosing the data used to train machine learning models is critical.</p>



<p>I&#8217;m also looking forward to see what Apple releases in the future, especially considering the more complex ways machine learning is being used within Apple&#8217;s existing products. I imagine it to include more tooling, advances to existing tooling, and maybe some interesting visualization assistance to understand model efficacy. I would love for something related to data management tooling &#8211; although I suspect it&#8217;s nearly impossible to provide since everyone&#8217;s data is rather specific and quite different.</p>



<p> </p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2020/12/crf-1.png?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2020/12/transfer.png?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2020/12/transfer-eval.png?w=909" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2020/12/preview.png?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2020/12/model-output.png?w=1024" medium="image" />
	</item>
		<item>
		<title>Apple&#8217;s M1 Chip Changes&#8230; Lots of Things</title>
		<link>https://rhonabwy.com/2020/12/20/apples-m1-chip-changes-lots-of-things/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 20 Dec 2020 22:11:12 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5868</guid>

					<description><![CDATA[The new Apple Macs with an M1 chip in them is finishing a job that started a few years ago: changing my assumption that commodity hardware would always win. Having worked in the technology/computing field for over 30 years, you&#8217;d think I know better by now not to make such a broad assumption, even internally.<a class="more-link" href="https://rhonabwy.com/2020/12/20/apples-m1-chip-changes-lots-of-things/">Continue reading <span class="screen-reader-text">"Apple&#8217;s M1 Chip Changes&#8230; Lots of&#160;Things"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>The new Apple <a href="https://www.apple.com/mac/m1/">Macs with an M1 chip</a> in them is finishing a job that started a few years ago: changing my assumption that commodity hardware would always win. Having worked in the technology/computing field for over 30 years, you&#8217;d think I know better by now not to make such a broad assumption, even internally. For years, I thought the juggernaut of Intel/x86 was unstoppable, but now? Now I&#8217;m questioning that.</p>



<p>Apple has always prided itself on its deep integration of hardware and software. And sometimes they&#8217;ve even made good on it. The iOS (and iPadOS) integration has been impressive for the last fifteen years, and now they brought it to their laptop lineup, spectacularly so. It&#8217;s not just Apple doing this &#8211; Samsung and some other manufacturers have been down this road for a while, merging in computing silicon with sensors at a deep foundational level, changing what we think of as components within computing systems. Some of the deeply integrated cameras are effectively stand-alone systems in their own right. Lots of phones and digital cameras use that very-not-commodity component.</p>



<p>I still think there&#8217;s a notable benefit to leveraging commodity hardware &#8211; shoot, that&#8217;s what this whole &#8220;cloud&#8221; computing market is all about. But it&#8217;s also pretty clear that the guarantee that the win that commodity gives you won&#8217;t necessarily outweighs the commercial benefits of deep hardware/software integration.</p>



<p>One of the interesting things about the M1 system-on-a-chip isn&#8217;t the chip itself, but the philosophy that Apple&#8217;s embracing in making the chip. That pattern of behavior and thought goes way beyond what you can do with commodity stuff. The vertical integration allows seriously advanced capabilities. Commodity, on the other hand, tends to be sort of &#8220;locked down&#8221; and very resistant to change, even improvements. Then pile on top of that the tendency for these chip designs to be far more modular. They&#8217;re aggressively using coprocessors and investing in the infrastructure of how to make them work together. I think that&#8217;s the core behind the unified memory architecture. What Apple has done, or started to do, is invest in the ways to go even more parallel and make it easier for those co-processors to work together. In a commodity (Intel) system, the rough equivalent is the PCIe bus and the motherboard socket you plug cards into. Only that doesn&#8217;t solve the &#8220;how you share memory&#8221; or &#8220;who talks to who, and when&#8221; problems. In fact, it kind of makes it worse as you get more cards, sockets, or components.</p>



<p>I may be giving Apple&#8217;s hardware engineering team more credit than they&#8217;re due &#8211; but I don&#8217;t think so. I think the reason they talked about &#8220;Unified Memory Architecture&#8221; so much with this chip is that it IS a solution to the &#8220;how to get lots of co-processors to work together&#8221;, while not exploding the amount of power that a system consumes while doing so. (If you&#8217;re not familiar, memory is a &#8220;sunuvabitch&#8221; when it comes to power consumption &#8211; one of the biggest sinks for power in a modern PC. The only thing that&#8217;s worse than memory are ethernet network ports, which was a serious eye-opener for me back in the day.)</p>



<p>There are other computing changes that aren&#8217;t trumpeted around so much that are going to contribute equally to the future of computing. I was introduced to the world of NVMe (Non-volatile memory) a few years back, when it was just hitting it&#8217;s first iteration of commercial introduction. The &#8220;holy crap&#8221; moment was realizing that the speed at which it operated was equivalent to those power hungry memory chips. Add that into the mix, and you&#8217;ve got lots of compute, persistent memory, and far lower power requirements in the future for a heck of lot of computing. It&#8217;s the opposite direction that Intel, and nVidia, are charging into &#8211; assuming they&#8217;ll have power to spare, and can happily burn the watts to provide the benefits. Truth be told, only individual consumers were still really oriented that way &#8211; the cloud providers, over a decade ago, had already clued in that the two most expensive things in running cloud services were 1) power and 2) people.</p>



<p>Bringing this back to the M1 chip, it&#8217;s tackling the power component of this all very impressively. I&#8217;m writing this on an M1 MacBook, and loving the speed and responsiveness. Honestly, I haven&#8217;t felt a &#8220;jump&#8221; in perceived speed and responsiveness to a computer with a generational gap like this in over 15 years. And that&#8217;s WITH the massive power reduction while providing it. </p>



<p>I do wish Apple was a little more open and better about providing tooling and controls to help solve the &#8220;cost of people&#8221; equation of this situation. Yes, I know there&#8217;s <a href="https://support.apple.com/guide/mdm/welcome/web">MDM</a> and such, but comparatively it&#8217;s a pain in the butt to use, and that&#8217;s the problem. It needs to be simpler, more straightforward, and easier &#8211; but I suspect that Apple doesn&#8217;t really give much of a crap about that. Maybe I&#8217;m wrong here, but the tooling to make it really, really easy to combine sets of systems together hasn&#8217;t been their strong point. At the same time, groupings and clusters of compute is just the larger reflection of what&#8217;s happening at the chip level with the M1 SOCs (For what it&#8217;s worth: SOC stands for &#8220;system on a chip&#8221;).</p>



<p>I think you can see some hints they might be considering more support here &#8211; talking about their AWS business interactions a bit more, and on the software side the focus on &#8220;swift on the server&#8221; and the number of infrastructural open-source pieces they help lay out over the past year that are specific to capturing logging, metrics, tracing, or helping to dynamically create clusters of systems.</p>



<p>I think there&#8217;a s lot to look forward to, and I&#8217;m super interested to see how this current pattern plays over of this coming year (and the next few). Like many others, I&#8217;ve raised my expectations for what&#8217;s coming. And I think those expectations will be met with future technology building on top of these M1 chips.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Thanksgiving 2020</title>
		<link>https://rhonabwy.com/2020/11/23/thanksgiving-2020/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Mon, 23 Nov 2020 18:31:37 +0000</pubDate>
				<category><![CDATA[Ranting and Reflections]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5858</guid>

					<description><![CDATA[This has been a right 🤬 of a year, and that probably applies to most anyone else on this globe. In the US, the stress of this presidential election was extreme, and acerbated by COVID pandemic that we pretty much failed to get any sort of handle on. I&#8217;ve managed to stay tucked down and<a class="more-link" href="https://rhonabwy.com/2020/11/23/thanksgiving-2020/">Continue reading <span class="screen-reader-text">"Thanksgiving 2020"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>This has been a right <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f92c.png" alt="🤬" class="wp-smiley" style="height: 1em; max-height: 1em;" /> of a year, and that probably applies to most anyone else on this globe. In the US, the stress of this presidential election was extreme, and acerbated by COVID pandemic that we pretty much failed to get any sort of handle on. I&#8217;ve managed to stay tucked down and safe, but my day to day life from a year ago is completely different now. My favorite &#8220;office&#8221; coffeeshop, El Diablo, is now gone &#8211; 20 years after it started. The loss has an oversized impact on my because it was also my social hub connecting me to friends in my community. Like a lot of others, my world is currently collapsed down to a pretty tiny bubble.</p>



<p>The downsides of this year are undeniable and clear, but there have been a number of silver linings, and since we&#8217;re heading into Thanksgiving, it seemed a good time to reflect and celebrate the positives for the year. It doesn&#8217;t remove the horror and pain, but to me &#8211; it helps offset it.</p>



<p>Right as the pandemic was sweeping up, I managed to get a longer term contract gig that&#8217;s been really useful for me, working on technical writing. I&#8217;ve been programming, and managing programmers, devops, QA, and the whole software lifecycle kit for multiple decades, and one of the skills that I&#8217;ve been trying to cultivate has been communication &#8211; specifically writing. Last year around this time, I self-published what was primarily a labor of love &#8211; <a href="http://gumroad.co/usingcombine">Using Combine</a> &#8211; a book/reference doc combination on Apple&#8217;s <a href="http://developer.apple.com/documentation/combine/">Combine</a> framework. The year before I&#8217;d published a different book through Packt, <a href="https://www.packtpub.com/product/kubernetes-for-developers/9781788834759">Kubernetes for Developers</a> (A google search on that phrase now directs you to more of offline training and certifications, but there&#8217;s a book in there too!). I&#8217;d been searching for editors to work with, that really dug into the work to help me structure it &#8211; not just the fluffy top level stuff that so a number of publishers stick to.</p>



<p>It&#8217;s the combination of these things that ends up being what tops my give-thanks list this year. In the past eight months, I&#8217;ve had a chance to work closely with a number of truly amazing editors, helping to refine and improve my writing skills. While I&#8217;m still crappy at spotting passive voice in my own writing, the feedback I&#8217;ve gotten from Chuck, Joni, Susan, Colleen, Mary Kate, and Paul has been amazing. Everything from basic grammar (that I had just never really did well) to structure, narrative flow, complexity. Top that off with some wonderful writers, Ben, Liz, Dave, and Joanna, willing to answer the odd question or just chat about the latest garden recipes on a video call, and really these past months of contracting have been a terrific experience.</p>



<p>I was super fortunate to find a gig when everyone else seemed to losing them. I&#8217;m sure the troubles aren&#8217;t even close to over &#8211; there&#8217;s so much to rebuild &#8211; but it&#8217;s a bit of light against the backdrop of this year.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Combine and Swift Concurrency</title>
		<link>https://rhonabwy.com/2020/11/08/combine-and-swift-concurrency/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 08 Nov 2020 19:35:43 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[combine]]></category>
		<category><![CDATA[concurrency]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5846</guid>

					<description><![CDATA[Just before last weekend, the folks on the Swift Core Team provided what I took to be a truly wonderful gift: a roadmap and series of proposals that outline a future of embedding concurrency primitives deeper into the swift language itself. If you&#8217;re interested in the details of programming language concurrency, it&#8217;s a good read.<a class="more-link" href="https://rhonabwy.com/2020/11/08/combine-and-swift-concurrency/">Continue reading <span class="screen-reader-text">"Combine and Swift&#160;Concurrency"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Just before last weekend, the folks on the Swift Core Team provided what I took to be a truly wonderful gift:<a href="https://forums.swift.org/t/swift-concurrency-roadmap/41611"> a roadmap</a> and series of proposals that outline a future of embedding concurrency primitives deeper into the swift language itself. If you&#8217;re interested in the details of programming language concurrency, it&#8217;s a good read. The series of pitches and proposals:</p>



<ul><li><a href="https://forums.swift.org/t/concurrency-asynchronous-functions/41619">async/await pitch</a>, and <a href="https://github.com/DougGregor/swift-evolution/blob/async-await/proposals/nnnn-async-await.md">proposal</a></li><li><a href="https://forums.swift.org/t/concurrency-structured-concurrency/41622">task API and structured concurrency pitch</a>, and <a href="https://github.com/DougGregor/swift-evolution/blob/structured-concurrency/proposals/nnnn-structured-concurrency.md">proposal</a></li><li><a href="https://forums.swift.org/t/concurrency-actors-actor-isolation/41613">actor and actor isolation pitch</a> and <a href="https://github.com/DougGregor/swift-evolution/blob/actors/proposals/nnnn-actors.md">proposal</a></li><li><a href="https://forums.swift.org/t/concurrency-interoperability-with-objective-c/41616">concurrency interop with Objective-C pitch</a> and <a href="https://github.com/DougGregor/swift-evolution/blob/concurrency-objc/proposals/NNNN-concurrency-objc.md">proposal</a> </li></ul>



<p>If you have questions, scan through each of the pitches (which all link into the forums), and you&#8217;ll see a great deal of conversation there &#8211; some of which may have your answer (other parts of which, at least if you&#8217;re like me, may just leave you more confused), but most importantly the core team is clearly willing to answer questions and explore the options and choices.</p>



<p>When I first got involved with the swift programming language, I was already using some of these kinds of concurrency constructs in other languages &#8211; and it seemed to be a glaring lack of the language that they didn&#8217;t specify, instead relying on the Objective-C runtime and in particular the dispatch libraries in that runtime. The dispatch stuff, however, is darned solid &#8211; battle honed as it were. You can still abuse it into poor performance, but it worked solidly, so while it kind of rankled, it made sense with the thinking of &#8220;do the things you need to do now, and pick your fights carefully&#8221; in order to make progress. Since then time (and the language) has advanced significantly, refined out quite a bit, and I&#8217;m very pleased to see formal concurrency concepts getting added to the language.</p>



<p>A lot of folks using Combine have reached for it, looking for the closest thing they can find to Futures and the pattern of linking futures together, explicitly managing the flow of asynchronous updates. While it is something Combine does, Combine is quite a bit more &#8211; and a much higher level library, than the low level concurrency constructs that are being proposed.</p>



<blockquote class="wp-block-quote"><p>For those of you unfamiliar, Combine provides a <a href="https://developer.apple.com/documentation/combine/Future">Future publisher</a>, and <a href="https://heckj.github.io/swiftui-notes/#reference-future">Using Combine details how to use it a bit</a>, and <a href="https://www.donnywals.com/using-promises-and-futures-in-combine/">Donny Wals has a nice article</a> detailing it that&#8217;s way more &#8220;tutorial like&#8221;.</p></blockquote>



<h3><em>What does this imply for Combine?</em></h3>



<p>First up, pitches and proposals such as these are often made when there&#8217;s something at least partially working, that an individual or three have been experimenting with. But they aren&#8217;t fully baked, nor are they going to magically appear in the language tomorrow. Whatever comes of the proposals, you should expect it&#8217;ll be six months minimum before they appear in any seriously usable form, and quite possibly longer. This is tricky, detailed stuff &#8211; and the team is excellent with it, but there&#8217;s still a lot of moving parts to manage with this.</p>



<p>Second, where there&#8217;s some high level conceptual overlap, they&#8217;re very different things. Combine, being a higher level library and abstraction, I expect will take advantage of the the lower-level constructs with updates, very likely ones that we&#8217;ll never see exposed in their API, to make the operators more efficient. The capabilities that are being pitched for language-level actors (don&#8217;t confuse that with a higher level actor or distributed-actor library &#8211; such as <a href="https://akka.io">Akka</a> or <a href="https://dotnet.github.io/orleans/">Orleans</a>) may offer some really interesting capabilities for Combine to deal with it&#8217;s queue/runloop hopping mechanisms more securely and clearly.</p>



<p>Finally, I think when this <em>does</em> come into full existence, I hope the existing promise libraries that are used in Swift (<a href="https://github.com/mxcl/PromiseKit">PromiseKit</a>, Google&#8217;s <a href="https://github.com/google/promises">promises</a>, or Khanlou&#8217;s <a href="https://github.com/khanlou/Promise">promise</a>) start to leverage the async constructs into their API structure &#8211; giving a clear path to use for people wanting a single result processed through a series of asynchronous functions. You can use Combine for that, but it is really aimed at being a library that deals with a whole series or stream of values, rather than a single value, transformed over time.</p>



<h2>tl;dr</h2>



<p>The async and concurrency proposals are goodness, not replacement, for Combine &#8211; likely to provide new layers that Combine can integrate and build upon to make itself more efficient, and easier to use.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>The range operator and SwiftUI&#8217;s layout engine</title>
		<link>https://rhonabwy.com/2020/11/08/the-range-operator-and-swiftuis-layout-engine/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 08 Nov 2020 18:45:00 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[swiftui]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5808</guid>

					<description><![CDATA[This post is specific to Swift the programming language and SwiftUI, Apple&#8217;s newest multi-platform UI framework. If you&#8217;re not interested in both, probably best to skip past this&#8230; I was working on visualization code that leverages SwiftUI to see how that might work, and ran into a few interesting tidbits: playgrounds with SwiftUI works brilliantly,<a class="more-link" href="https://rhonabwy.com/2020/11/08/the-range-operator-and-swiftuis-layout-engine/">Continue reading <span class="screen-reader-text">"The range operator and SwiftUI&#8217;s layout&#160;engine"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>This post is specific to Swift the programming language and SwiftUI, Apple&#8217;s newest multi-platform UI framework. If you&#8217;re not interested in both, probably best to skip past this&#8230;</p>



<p>I was working on visualization code that leverages <a href="http://developer.apple.com/documentation/swiftui">SwiftUI</a> to see how that might work, and ran into a few interesting tidbits: playgrounds with SwiftUI works brilliantly, right up until it doesn&#8217;t and then you&#8217;re in a deep, dark hole of <strong><em>WTF?!</em></strong>, and the SwiftUI layout engine, especially with nested and semi-complex container views, do some sort of iterative solver technique. The less interesting tidbit is a classic aphorism: <em>It&#8217;s the things you think you know that aren&#8217;t true that bite you</em>.</p>



<p>When you&#8217;re fiddling with adding in your own constraints in SwiftUI, you might be tempted to use the <a href="https://developer.apple.com/documentation/swift/range">range operator</a> &#8211; at least my thinking was &#8220;Oh hey, this looks like a super convenient way to check to make sure this value is within an expected range&#8221;. It works stunningly well for me, as long as I&#8217;m careful about creating it. I started creating ranges on the fly from variable values, and that&#8217;s where playgrounds, and my bad assumptions, bit me.</p>



<p>If you create a range that&#8217;s ludicrous, Swift can throw an exception at runtime. So if you&#8217;re working with ranges, you&#8217;ve got the possibility that passing in a blatantly incorrect value will give you a non-sensical range, and that will result in a crashing exception. When you stumble into this using Playgrounds, you get a crash that doesn&#8217;t really tell you much of anything. When you kick that same thing up in Xcode, it still crashes (of course), but at least the debugger will drop into place and show you what you did that was wrong. I love using <a href="https://developer.apple.com/videos/play/wwdc2020/10643/">SwiftUI with Playgrounds</a>, but the lack of runtime feedback when I hit an exception – about what I screwed up – makes it significantly less useful to me.</p>



<p>And debugging this in Xcode was where I learned that closures you provide within SwiftUI layout, such as <a href="https://developer.apple.com/documentation/swiftui/view/alignmentguide(_:computevalue:)-330fz">alignmentGuide</a> or a method of your own creation working with a <a href="https://developer.apple.com/documentation/swiftui/geometryreader">GeometryReader</a> don&#8217;t get called just once. Sometimes they&#8217;re called one, but other times they are called repeatedly, and with pretty strange values for the view&#8217;s dimension. I think underneath the covers, there&#8217;s an iterative layout solver that&#8217;s trying out a variety of layout options for the view that&#8217;s being created. Sometimes those closures would be invoked once, other times repeatedly &#8211; and in some cases repeatedly with the same values. Interestingly, sometimes those values included a <a href="https://developer.apple.com/documentation/swiftui/viewdimensions">ViewDimension</a> or <a href="https://developer.apple.com/documentation/swiftui/geometryproxy">GeometryProxy</a> with a <code>size.width</code> of <code>0</code>. The bad assumption I made was that it would be sized quite a bit larger, never zero. Because of that, I attempted to build an incorrect range – effectively <code>ClosedRange(x ... x-1)</code> – which caused the exception.</p>



<p>Even with my own assumptions biting me, I like the use of <code>range</code> and I&#8217;m trying to use it in an experimental API surface. Lord knows what&#8217;ll come of the experiment, but the basics are bearing some fruit. I have a bit of code where I&#8217;ve been porting some of the concepts, such as <a href="https://observablehq.com/@d3/d3-scalelinear">scale</a> and <a href="https://observablehq.com/@d3/axis-ticks">tick</a>, from <a href="https://d3js.org">D3</a> to use within SwiftUI.</p>



<p>The current experimental code looks like:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; title: ; notranslate">
// axis view w/ linear scale - simple/short
HStack {
    VerticalTickDisplayView(
        scale: LinearScale(
            domain: 0 ... 5.0,
            isClamped: false)
    )
    VerticalAxisView(
        scale: LinearScale(
            domain: 0 ... 5.0,
            isClamped: false)
    )
}
.frame(width: 60, height: 200, alignment: .center)
.padding()

// axis view w/ log scale variant - manual ticks
HStack {
    VerticalTickDisplayView(
        scale: LogScale(
            domain: 0.1 ... 100.0,
            isClamped: false),
        values: &#91;0.1, 1.0, 10.0, 100.0]
    )
    VerticalAxisView(
        scale: LogScale(
            domain: 0.1 ... 100.0,
            isClamped: false)
    )
}
.frame(width: 60, height: 200, alignment: .center)
.padding()
</pre></div>


<p>And results in fairly nice horizontal and vertical tick axis that I can use around a chart area:</p>



<figure class="wp-block-image size-large"><img data-attachment-id="5819" data-permalink="https://rhonabwy.com/verticalscales/" data-orig-file="https://josephheck.files.wordpress.com/2020/08/verticalscales.png" data-orig-size="666,1328" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="verticalscales" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/08/verticalscales.png?w=150" data-large-file="https://josephheck.files.wordpress.com/2020/08/verticalscales.png?w=514" src="https://josephheck.files.wordpress.com/2020/08/verticalscales.png?w=514" alt="" class="wp-image-5819" srcset="https://josephheck.files.wordpress.com/2020/08/verticalscales.png?w=514 514w, https://josephheck.files.wordpress.com/2020/08/verticalscales.png?w=75 75w, https://josephheck.files.wordpress.com/2020/08/verticalscales.png?w=150 150w, https://josephheck.files.wordpress.com/2020/08/verticalscales.png 666w" sizes="(max-width: 514px) 100vw, 514px" /></figure>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2020/08/verticalscales.png?w=514" medium="image" />
	</item>
		<item>
		<title>NaNoWriMo Beat Sheet Scrivener Template</title>
		<link>https://rhonabwy.com/2020/10/17/nanowrimo-beat-sheet-scrivener-template/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 17 Oct 2020 20:00:20 +0000</pubDate>
				<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[nanowrimo]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5835</guid>

					<description><![CDATA[I participated in NaNoWriMo once before, in 2016 &#8211; having watched my partner get more deeply involved in previous years. I haven&#8217;t really been back, but this year with all the &#8230; yeah, I decided to give it a shot again. I&#8217;ve got my profile set up, at least the basics, and now I&#8217;m stumbling<a class="more-link" href="https://rhonabwy.com/2020/10/17/nanowrimo-beat-sheet-scrivener-template/">Continue reading <span class="screen-reader-text">"NaNoWriMo Beat Sheet Scrivener&#160;Template"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I participated in <a href="https://nanowrimo.org">NaNoWriMo</a> once before, in 2016 &#8211; having watched my partner get more deeply involved in previous years. I haven&#8217;t really been back, but this year with all the &#8230; yeah, I decided to give it a shot again. I&#8217;ve got <a href="https://nanowrimo.org/participants/heckj">my profile</a> set up, at least the basics, and now I&#8217;m stumbling around trying to figure out what my story is going to encompass. </p>



<p>Last time through, I made a great start, but didn&#8217;t have much of a plan, so it fizzled out pretty hard about half-way through. So this time, I thought I&#8217;d try something a bit different and do a little planning. Last weekend, the local NaNoWriMo group hosted a session for planning and plotting, which I&#8217;d not tried previously. I&#8217;m going to try out using a beat sheet this time, to provide some constraints and structure for the overall story arc.</p>



<p>I grabbed a basic beat sheet from the <a href="https://nanowrimo.org/nano-prep-101">NaNoWriMo writer&#8217;s prep resources</a>, and since I have a copy the amazing writing app <a href="https://www.literatureandlatte.com/scrivener/overview">Scrivener</a>, I decided to go ahead and make a NaNoWriMo template based on that.</p>



<p>If you are so inclined, you&#8217;re absolutely welcome to use it &#8211; it&#8217;s based on 50,000 words, deadline of 11/30, and the <a href="https://docs.google.com/document/d/1f_Uzr21Mt6t5mjq7Z2ih2AvtGqXPVnvoFLmCb27QMms/edit">beat sheet google doc linked from the resources</a>.</p>



<div class="wp-block-file"><a href="https://josephheck.files.wordpress.com/2020/10/nanowrimo2020-beat-sheet-template.scrivtemplate.zip">nanowrimo2020-beat-sheet-template.scrivtemplate</a><a href="https://josephheck.files.wordpress.com/2020/10/nanowrimo2020-beat-sheet-template.scrivtemplate.zip" class="wp-block-file__button" download>Download</a></div>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Open apps with SwiftUI</title>
		<link>https://rhonabwy.com/2020/08/29/open-apps-with-swiftui/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 29 Aug 2020 19:26:55 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[swiftui]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5824</guid>

					<description><![CDATA[Earlier this week, James Dempsey asked on twitter about who else was actively trying to build macOS apps using SwiftUI. I&#8217;m super interested in SwiftUI. A year ago, it spawned my own side-project into writing my own reference docs on Combine. Originally I had a vision of writing about Combine as well as SwiftUI. Combine<a class="more-link" href="https://rhonabwy.com/2020/08/29/open-apps-with-swiftui/">Continue reading <span class="screen-reader-text">"Open apps with&#160;SwiftUI"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Earlier this week, <a href="https://twitter.com/jamesdempsey/status/1297945662948950019">James Dempsey asked on twitter</a> about who else was actively trying to build macOS apps using <a href="http://developer.apple.com/swiftui/">SwiftUI</a>. I&#8217;m super interested in SwiftUI. A year ago, it spawned my own side-project into writing <a href="http://heckj.github.io/swiftui-notes/">my own reference docs on Combine</a>. Originally I had a vision of writing about Combine as well as SwiftUI. Combine alone was hugely, so I stopped there with the notes, especially as SwiftUI is still massively maturing. Some pretty amazing updates came just earlier this year. While clearly not finished, likely not even close to finished, it&#8217;s now far enough along in it&#8217;s maturity that you can at least consider using it for full apps. Or parts of your app if you like &#8211; macOS, iOS, watchOS, or tvOS.</p>



<p>While I&#8217;m been keeping track of the framework, I&#8217;ve also been keeping track of people who are using it, writing about it, struggling with it, etc. There&#8217;s two implementations of full applications, all open source (and hence completely visible), that I&#8217;ve been following as super interesting examples of using SwiftUI: NetNewsWire and ControlRoom.</p>



<h3>NetNewsWire</h3>



<p>I&#8217;ve contributed a bit to <a href="https://ranchero.com/netnewswire/">NetNewsWire</a>, only a tiny amount (and mostly around continuous integration), but I&#8217;ve been using it since the earliest days and from it&#8217;s original inception and through multiple owners to it&#8217;s current state as an open-source project that <a href="https://twitter.com/brentsimmons">Brent Simmons</a> is leading. The code is available online, ever evolving, at <a href="https://github.com/Ranchero-Software/NetNewsWire">https://github.com/Ranchero-Software/NetNewsWire</a>. The recent work to embrace SwiftUI is on it&#8217;s <code>main</code> branch with a lot of the SwiftUI code under the directory <code><a href="https://github.com/Ranchero-Software/NetNewsWire/tree/main/Multiplatform/Shared">multiplatform/shared</a></code>. Take a deep dive and dig around &#8211; there&#8217;s some gems and interesting questions, and you can see some really fascinating examples of integrating SwiftUI and UIKit or AppKit where SwiftUI isn&#8217;t quite up to some of the tasks desired by the project.</p>



<h3>ControlRoom</h3>



<p>The other app I&#8217;ve been watching is ControlRoom, an app that <a href="https://twitter.com/twostraws">Paul Hudson</a> referenced in a <a href="https://twitter.com/twostraws/status/1227619436187803648">demo capture on twitter</a>. ControlRoom&#8217;s code is on Github at <a href="https://github.com/twostraws/ControlRoom">https://github.com/twostraws/ControlRoom</a>, released earlier in SwiftUI&#8217;s lifecycle, and showing an integration not of the new SwiftUI app architecture pieces, but more of &#8220;classic&#8221; macOS AppKit integration. Like NetNewsWire, I found a number of really great gems within the code, often having &#8220;light-bulb&#8221; moments when I understood how the app accomplished some of its goals.</p>



<h4>Others&#8230;</h4>



<p>There are easily other apps out there, that I&#8217;m unaware of &#8211; but not too many folks are openly sharing their development like the two projects above. I did find a list of <a href="https://github.com/dkhamsing/open-source-ios-apps">open-source IOS apps</a> on GitHub that includes a <a href="https://github.com/dkhamsing/open-source-ios-apps#swiftui">subset listing SwiftUI</a>, that might be interesting.</p>



<p>I have a few of my own experiments, but nothing as polished and effective as these two, and I don&#8217;t think I solve any problems in novel ways that they haven&#8217;t. In a bit of <a href="http://github.com/heckj/MPCF-TestBench">test and benchmarking code</a>, I was creating SwiftUI interfaces across macOS, iOS, and tvOS &#8211; which turns out to be a right pain in the butt, even for the simplest displays.</p>



<p>I hope, but don&#8217;t expect, more apps to become available &#8211; or to be more visible down the road. Having the open sharing of how they solved problems is invaluable to me for learning, and even more so for sharing. Apple has their sample code, well &#8211; some of it anyway &#8211; but seeing folks outside of Apple use the framework &#8220;in the wild&#8221; really shows it&#8217;s working (or where it isn&#8217;t).</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Learning to Write, Writing to Learn</title>
		<link>https://rhonabwy.com/2020/08/09/learning-to-write-writing-to-learn/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 09 Aug 2020 21:42:30 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5798</guid>

					<description><![CDATA[Writing, while I love it, doesn&#8217;t come naturally to me. I suspect it doesn&#8217;t come naturally to any writer. The process of getting something written, really tightly focused and right on target, is a heroic act of understanding, simplification and embracing constraints. It&#8217;s a skill for which I don&#8217;t have a good analogue in the<a class="more-link" href="https://rhonabwy.com/2020/08/09/learning-to-write-writing-to-learn/">Continue reading <span class="screen-reader-text">"Learning to Write, Writing to&#160;Learn"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Writing, while I love it, doesn&#8217;t come naturally to me. I suspect it doesn&#8217;t come naturally to any writer. The process of getting something written, really tightly focused and right on target, is a heroic act of understanding, simplification and embracing constraints. It&#8217;s a skill for which I don&#8217;t have a good analogue in the other kinds of work I&#8217;ve done. Maybe the closest is the  finish work (sanding and polishing) with jewelry making or metal-work, but that doesn&#8217;t quite fit. I suspect there are good analogues, I&#8217;m just not spotting them.</p>



<p>A few weeks ago I wrote about my challenges with the learning to write process, complicated by COVID and the overall lockdown. While I broke through that plateau, there are always more challenges to work through. This last week, I hit another slog-swamp &#8211; this time it&#8217;s more about mental framing than my actual writing and feedback loops.</p>



<p>Part of why I&#8217;m doing technical writing is that writing is an act of sharing that has an incredibly long reach. It&#8217;s a really big-damn-lever in the <a href="https://en.wikiquote.org/wiki/Archimedes"><em>move the world</em></a> kind of metaphor. That makes it, to me, a supremely worthy endeavor. </p>



<p>To really do it well, you need to know the subject you&#8217;re writing about: backwards and forwards, from a couple different angles, maybe even coming in from a different dimension or two. I embraced the idea of &#8220;If you can explain something simply, then you may understand it.&#8221; That&#8217;s the &#8220;writing to learn&#8221; part of this &#8211; what I&#8217;m doing with the writing is forcing myself to learn, to think about the subject matter from different angles. </p>



<p>The hardest part of that learning is figuring out the angles that don&#8217;t come naturally, or from which I don&#8217;t have a lot of background. I&#8217;m generally fairly empathetic, so I&#8217;ve got at least a slight leg up; I can often at least visualize things from another point of view, even if I don&#8217;t fully understand it.</p>



<p>The flip side of it, learning to write, happened this week. I completed a draft revision, and when I reviewed with some folks, I realized it fell way off the mark. There were a few things that I thought were true (that weren&#8217;t) that knocked it awry. More than that the feedback I got was about taking things to a more concrete example to help reinforce what I was trying to say. Really, it was super-positive feedback, but the kind of feedback that has me thinking I might need to gut the structure pretty heavily and rebuild it back up. As the weekend closes out, I suspect an act of creative destruction might be coming in order to reset this and get it closer to where it needs to be.</p>



<p>I&#8217;ve been noodling on that feedback most of this weekend &#8211; it has been percolating in my rear-brain for quite a while. Aside from the &#8220;things you thought were true that weren&#8217;t&#8221; (the evil bits that get you, no matter what the topic area) that I got straight pretty quickly, the key from this feedback is that while I was correct in what I was touching on in the writing, it was too abstracted and too easily mis-undertstood. Especially in the world of technical writing and programming topics, it&#8217;s SUPER easy to get &#8220;too abstract&#8221;. And then there&#8217;s the death knell of what should be good technical writing &#8211; too abstract AND indirect.</p>



<p>Embracing the constraint of &#8220;making it more concrete&#8221; is some next-level thinking. It&#8217;s forcing me to be less abstract (a good thing) and it&#8217;s taking a while to really see how to make that work for the topic I&#8217;m writing about. I mean, really, I&#8217;m still working on the &#8220;nuke all the passive voice&#8221; thing. While I&#8217;m getting better, it takes me something like 3 or 4 passes, reading sentence by sentence, to spot them in my writing.</p>



<p>For what it&#8217;s worth, I&#8217;m loving the &#8220;nuke passive voice&#8221; constraint. I love that it forces the writing to be specific about what takes action and what the results are &#8211; so much hidden stuff becomes clear, and it&#8217;s a great forcing function to see if you also really understand how it&#8217;s working.</p>



<p>For now I&#8217;ll continue to noodle on how to make my example more concrete, and get back to my baking and kitchen cleaning for the afternoon. Come tomorrow, I&#8217;ll take another pass at that article and see what I can do for the concrete-ness aspect.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Feeling alone and outside of your comfort zone</title>
		<link>https://rhonabwy.com/2020/07/25/feeling-alone-and-outside-of-your-comfort-zone/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 25 Jul 2020 18:13:17 +0000</pubDate>
				<category><![CDATA[Ranting and Reflections]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/2020/07/25/feeling-alone-and-outside-of-your-comfort-zone/</guid>

					<description><![CDATA[Month five of the COVID lockdown, and when it started I picked up a new bit of work. It is something I&#8217;d wanted to do and from which I get enjoyment: technical writing. I am definitely stepping outside of my comfort zone. Although I&#8217;ve written extensively and am a published author with several titles, the<a class="more-link" href="https://rhonabwy.com/2020/07/25/feeling-alone-and-outside-of-your-comfort-zone/">Continue reading <span class="screen-reader-text">"Feeling alone and outside of your comfort&#160;zone"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Month five of the COVID lockdown, and when it started I picked up a new bit of work. It is something I&#8217;d wanted to do and from which I get enjoyment: technical writing. I am definitely stepping outside of my comfort zone. Although I&#8217;ve written extensively and am a published author with several titles, the skills of grammar, spelling, and word choice aren&#8217;t what I&#8217;d describe as my super powers.</p>



<p>The past few weeks have been a bit harder that usual, as I&#8217;ve been pushing past the basics, and breaking through the plateau where I felt comfortable and knew what I was doing. It is doubly-hard with COVID lockdown and remote-only work, as none of the avenues I&#8217;ve used in the past to vet ideas or check thinking and progress has been easily available to me. At it&#8217;s heart, writing is about communicating &#8211; and the technical writing I&#8217;m doing is aimed at being precise, accurate, concise, and easy to understand. Sometimes I&#8217;ve got some gems and it works well. Other times I stare at a single paragraph for the better part of 3 hours, tearing apart the sentences word by word and setting them into the form that I think may work &#8211; because I certainly don&#8217;t think or speak with that level of simple, direct, and accurate conciseness.</p>



<p>I&#8217;m confident I&#8217;ll get this, eventually. It will take longer than I&#8217;d like to get through the &#8220;Man, I suck at this&#8221; stage &#8211; as it always does when you&#8217;re learning something. It feels terrible at the moment. I often feel lost, sometimes confused, and &#8211; not surprisingly &#8211; frustrated. The constraints are a gift, but I rail at them just the same. It&#8217;s awkward and painful, and the assistance I have been able to find from coworkers or my fellow coffee-house peers in bouncing ideas around is gone or greatly reduced.</p>



<p>I&#8217;m determined to make <em>something</em> better in this whole mess. One of the few things I can control is what I&#8217;m working on, and I have that luxury &#8211; so I&#8217;m using the current time and constraints to improve myself.</p>



<p>If you&#8217;re doing the same, remember that it&#8217;s worth acknowledging that <em>this shit ain&#8217;t easy</em>, whatever your skill or task may be. If it&#8217;s worth improving, then it won&#8217;t be &#8211; almost by the very nature of it. Doesn&#8217;t matter if it&#8217;s hand-eye timing coordination and mastery for a video game, learning to paint in a new style, strategic puzzle solving of board games, or learning to set a perfect weld. Keep at it and keep moving, even if it doesn&#8217;t always feel like forward motion.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>post-WWDC &#8211; more device collaboration?</title>
		<link>https://rhonabwy.com/2020/07/10/post-wwdc-more-device-collaboration/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Fri, 10 Jul 2020 19:28:33 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[swiftui]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5788</guid>

					<description><![CDATA[It&#8217;s been two weeks since WWDC as I&#8217;m writing this. I certainly haven&#8217;t caught all the content from this year&#8217;s event, or even fully processed what I have learned. I see several patterns evolving, hear and read the various rumors, and can&#8217;t help but wonder. Maybe it&#8217;s wishful thinking, or I&#8217;m reading tea leaves incorrectly,<a class="more-link" href="https://rhonabwy.com/2020/07/10/post-wwdc-more-device-collaboration/">Continue reading <span class="screen-reader-text">"post-WWDC &#8211; more device&#160;collaboration?"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>It&#8217;s been two weeks since WWDC as I&#8217;m writing this. I certainly haven&#8217;t caught all the content from this year&#8217;s event, or even fully processed what I have learned. I see several patterns evolving, hear and read the various rumors, and can&#8217;t help but wonder. Maybe it&#8217;s wishful thinking, or I&#8217;m reading tea leaves incorrectly, but a few ideas keep popping into my head. One of them is that I think we would benefit from — and have the technology seeds for — more and better device collaboration between Apple devices.</p>



<p>One of the features that SwiftUI released this year was leveraging a pattern of pre-computed visualizations for use as complications on the Apple Watch, and more generally as <a href="https://developer.apple.com/design/human-interface-guidelines/ios/system-capabilities/widgets">Widgets</a>. The notion of a timeline is ingenious and a great way to solve the constrained compute capability of compact devices. This feature heavily leverages SwiftUI&#8217;s declarative views; that is, views derived from data.</p>



<p>Apple devices have frequently worked with each other &#8211; Phone and Watch pairing, the <a href="https://support.apple.com/en-us/HT210380">Sidecar mechanism</a> that&#8217;s available with a Mac laptop and an iPad, and of course <a href="https://www.apple.com/airplay/">AirPlay</a>. They offload data, and in some cases they offload computation. There&#8217;s creative caching involved, but even the new <a href="https://developer.apple.com/app-clips/">App Clips</a> feature seems like it&#8217;s a variation on this theme &#8211; with code that can be <em>trusted</em> and run for small, very focused purposes.</p>



<p>Apple has made some really extraordinary wearable computing devices &#8211; the watch and AirPods, leveraging Siri &#8211; in a very different take than the smart speakers of Google Home and Amazon&#8217;s Alexa. <a href="https://developer.apple.com/videos/play/wwdc2020/10068/">This year&#8217;s update to Siri</a>, for example, supports for on-device translation as well as dictation.</p>



<p>Now extrapolate out just a bit farther&#8230;</p>



<p>My house has a lot of Apple devices in it &#8211; laptop, watch, AirPods, several iPads, and that&#8217;s just the stuff I use. My wife has a similar set. The wearable bits are far more constrained and with me all the time &#8211; but also not always able to do everything <em>themselves</em>. And sometimes, they just conflict with each other &#8211; especially when it comes to Siri. (Go ahead &#8211; say &#8220;Hey Siri&#8221; in my house, and hear the chorus of responses)</p>



<p>So what about collaboration and communication between these devices? It would be fantastic if they could share sufficient context to support making the interactions even more seamless. A way to leverage the capabilities of a remote device (my phone, tablet, or even laptop) from the AirPods and a Siri request. They could potentially even hand-off background tasks (like tracking a timer) or knowing which device has been used most recently to better infer context for a request. For example, I want a timer while cooking often on my watch, not my phone &#8211; but &#8220;hey Siri&#8221; is not at all guaranteed to get it there.</p>



<p>That they could also know about the various devices capabilities and share those capabilities would make the whole set even smarter and more effective, and depending on which rumors you are excited by &#8211; they may be able to do some heavier computation off the devices that are more power constrained (wearables) by near-by but not physically connected (and power efficient) microprocessors. That could be generating visuals like Widgets, or perhaps the inverse &#8211; running a machine learning model against transmitted Lidar updates to identify independent objects and their traits from a point cloud or computed 3D mesh. </p>



<p>It&#8217;ll be interesting to see where this goes &#8211; I hope that distributed coordination, a means of allowing it (as a user), and a means developing for it is somewhere in the near future.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Exploring MultipeerConnectivity</title>
		<link>https://rhonabwy.com/2020/05/17/exploring-multipeerconnectivity/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 17 May 2020 18:20:55 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[ios]]></category>
		<category><![CDATA[macos]]></category>
		<category><![CDATA[multipeerconnectivity]]></category>
		<category><![CDATA[swiftui]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5782</guid>

					<description><![CDATA[A few weeks ago, I got curious about the MultipeerConnectivity framework available across Apple&#8217;s platforms. It&#8217;s a neat framework, and there are community-based libraries that layer over it to make it easier to use for some use cases: MultipeerKit (src) being the one that stood out to me. The promise of what this framework does<a class="more-link" href="https://rhonabwy.com/2020/05/17/exploring-multipeerconnectivity/">Continue reading <span class="screen-reader-text">"Exploring MultipeerConnectivity"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>A few weeks ago, I got curious about the <a href="https://github.com/insidegui/MultipeerKit">MultipeerConnectivity</a> framework available across Apple&#8217;s platforms. It&#8217;s a neat framework, and there are community-based libraries that layer over it to make it easier to use for some use cases: <a href="https://multipeerkit.rambo.codes">MultipeerKit</a> (<a href="https://github.com/insidegui/MultipeerKit">src</a>) being the one that stood out to me.</p>



<p>The promise of what this framework does is compelling, which is to seamlessly enable peer to peer networking, layering over any local transport available (bluetooth, ethernet if available, a local wifi connection, or a common wifi infrastructure). There&#8217;s a lot of &#8220;magic&#8221; in that capability, layering over underlying technologies and dealing with the advertise and connect mechanisms. Some of it uses <a href="https://developer.apple.com/bonjour/">Bonjour</a> (aka <a href="http://www.zeroconf.org">zeroconf</a>), and I suspect other mechanisms as well. </p>



<p>One of the &#8220;quirks&#8221; of this technology is that you don&#8217;t direct what transport is used, nor do you get information about the transport. You do get a nicely designed cascade of objects, all of which leverage the delegate/protocol structure to do their thing. Unfortunately, the documentation doesn&#8217;t make how to use them and what to expect entirely clear.</p>



<p>The structure starts with an advertiser, which is paired with an advertising browser. It wasn&#8217;t completely obvious to me at first, but you don&#8217;t need both sides of the peer to peer conversation doing advertising in order to make a connection. One side can advertise, the other browse, and you can establish a connection on that basis. It doesn&#8217;t need to be set up bi-directionally, although it can.</p>



<blockquote class="wp-block-quote"><p>When I first started in, having glanced through the developer docs, I thought you needed to have both sides actively advertising for this to work. Nope, bad assumption on my part. Both can advertise, and it&#8217;s makes for interesting viewing of &#8220;who&#8217;s out there&#8221; &#8211; but the heart that enables data transfer is another layer down: MCSession.</p></blockquote>



<p>You use the browser to &#8220;invite&#8221; a found peer to a <a href="https://developer.apple.com/documentation/multipeerconnectivity/mcsession">session</a>, and the corresponding <a href="https://developer.apple.com/documentation/multipeerconnectivity/mcnearbyserviceadvertiserdelegate">advertiser has a delegate</a> you use to accept invites. </p>



<p>The session (<a href="https://developer.apple.com/documentation/multipeerconnectivity/mcsession">MCSession</a>) is the heart of the communications from here. Session has an active side and a reactive side to it&#8217;s API &#8211; methods like <a href="https://developer.apple.com/documentation/multipeerconnectivity/mcsession">send(_:toPeers:with:)</a> pair to a <a href="https://developer.apple.com/documentation/multipeerconnectivity/mcsessiondelegate/">session delegate</a> responding using the method <a href="https://developer.apple.com/documentation/multipeerconnectivity/mcsessiondelegate/1406934-session">session(_:didReceive:fromPeer:)</a>. Before you get into sending data, however, you need to be connected.</p>



<p>While the browser allows you to invite, and the advertiser to accept, it is the session that gives you detail on what&#8217;s happening. Session is designed to do this through delegate method callbacks to <a href="https://developer.apple.com/documentation/multipeerconnectivity/mcsessiondelegate/1406958-session">session(_:peer:didChange:)</a> which is how you get state changes on for connections changes, and information on to whom you are connected. The <a href="https://developer.apple.com/documentation/multipeerconnectivity/mcsessiondelegate/1406958-session">session state</a> is a tri-state thing: notConnected, connecting, or connected. In my experience so far, you don&#8217;t spend very long in the connecting state, and state updates propagate pretty quickly (within a second or two) when the infrastructure changes. For example, when your iOS device goes to sleep, or you set the device into airplane mode. I haven&#8217;t measured exactly how fast, or how consistently, these updates propagate.</p>



<p>Session is a bi-directional communications channel, and once you are connected, either side can send data for the other to receive. Session also has the concept of not just sending Data, but of explicitly transferring (presumably larger) resources that are URL based. I haven&#8217;t experimented with this layer, but I&#8217;m guessing it&#8217;s a bit more optimized for reading a large file and streaming it out. The session delegate has callbacks for when it starts transfering and when it completes.</p>



<p>There&#8217;s a third transport mechanism, which uses open ended streams, that I haven&#8217;t yet touched. When I started looking, I did find some older github projects that tested using the streams capability &#8211; and how many simultaneous streams could be triggered and used effectively &#8211; but no published results. Alas those projects were written with swift 3, so while I poked at them out of curiosity, I mostly left them alone.</p>



<p>To explore MultipeerConnectivity, I created a project (available on github) called <a href="https://github.com/heckj/MPCF-TestBench">MPCF-TestBench</a>. The project is open source and available for anyone to compile and use themselves &#8211; but no promises on it all working correctly or looking anything close to good. (contributions welcome, but certainly not expected). </p>



<blockquote class="wp-block-quote"><p>Fair warning: when I think of &#8220;open source&#8221;, it&#8217;s the messy sausage-making process, not the completed and pretty, cleaned, ready-to-use-no-work-involved library or application that is the desired end goal. Feel free to dig in to the source, ask questions if you like, improve it and share the changes, or use it to your own explorations &#8211; but demand anything and you&#8217;ll just get a snicker.</p></blockquote>



<p>The project is an excuse to do something &#8220;heavier&#8221; in SwiftUI and see how things work &#8211; like how to get updates from more of these delegate heavy structures into the UI, and to see how SwiftUI translates across platforms. In short, it&#8217;s an excuse to learn.</p>



<p>All of this started several weeks ago when I poked <a href="https://twitter.com/_inside">Guiherme Rambo</a> about MultiPeerKit to see how fast it actually worked. He hadn&#8217;t made any explicit measurements, so I thought it might be interesting to do just that. To that end, the MPCF-TestBench has (crudely) cobbled a reflector and a test-runner with multiple targets (iOS, tvOS, and mac). This is also an excuse to see how SwiftUI translates across the platforms, but more on that in another (later) post. If you go to use this, I&#8217;d recommend sticking with the iOS targets for now, as it&#8217;s where I&#8217;m actively doing my development and using it.</p>



<figure class="wp-block-image size-large"><img data-attachment-id="5781" data-permalink="https://rhonabwy.com/2020/05/17/exploring-multipeerconnectivity/img_3203/" data-orig-file="https://josephheck.files.wordpress.com/2020/05/img_3203.jpg" data-orig-size="1125,2436" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_3203" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=139" data-large-file="https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=473" src="https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=473" alt="" class="wp-image-5781" srcset="https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=473 473w, https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=946 946w, https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=69 69w, https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=139 139w, https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=768 768w" sizes="(max-width: 473px) 100vw, 473px" /><figcaption>MPCF-TestBench work in progress</figcaption></figure>



<p>I have yet to do the work of trying out the transmissions at various sizes, but you can get basic information for yourself with the code as it stands. The screenshot above was transmitted from my iPhone (iPhone X) to a 10&#8243; iPad Pro, leveraging the my local wifi network, to which both were connected. The test sent 100 data packets that were about 1K in size, and a corresponding reflector on the iPad echoed the data back. No delay, just shoving them down a pipe as fast as I could &#8211; using the &#8220;reliable&#8221; transport mode.</p>



<p>I used a <a href="https://github.com/heckj/MPCF-TestBench/blob/master/MPCF-Reflector/Common/RoundTripXmitReport.swift">simple Codable structure</a> that exports to JSON to <a href="https://github.com/heckj/MPCF-TestBench/blob/master/results/wifi-connected.mpcftestjson">dump out the data</a> (although my export mechanism is only half-working to be honest). Still, it&#8217;s far enough to get a sample available if you&#8217;re curious. Feel free to dig apart the list of JSON objects for your own purposes, I&#8217;ll be adding more and making a more thorough result set over time.</p>



<p>I haven&#8217;t yet been able to establish a bluetooth only connection &#8211; the peering mechanism isn&#8217;t making a connection, but it could easily be something stupid I&#8217;ve done as well. </p>



<p>So there you have it &#8211; my initial answer to &#8220;how fast is it&#8221;: I&#8217;m seeing about 3 Kbytes/sec transferred using a &#8220;reliable&#8221; transport mode, over wifi, and using more recent iOS devices. The transmissions appear to be reasonably stable as well &#8211; not a terrible amount of standard deviation in my simple tests.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2020/05/img_3203.jpg?w=473" medium="image" />
	</item>
		<item>
		<title>Continuous Integration with Github Actions for macOS and iOS projects</title>
		<link>https://rhonabwy.com/2020/05/09/continuous-integration-with-github-actions-for-macos-and-ios-projects/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 09 May 2020 23:42:54 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[ci]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5766</guid>

					<description><![CDATA[GitHub Actions released in August 2019 &#8211; I&#8217;ve been trying them out for nearly a full year, using beta access available the adventurous before it was generally available. It was a long time in coming, and I saw this feature as GitHub&#8217;s missing piece. Some great companies stepped into that early gap and provide excellent<a class="more-link" href="https://rhonabwy.com/2020/05/09/continuous-integration-with-github-actions-for-macos-and-ios-projects/">Continue reading <span class="screen-reader-text">"Continuous Integration with Github Actions for macOS and iOS&#160;projects"</span></a>]]></description>
										<content:encoded><![CDATA[
<p><a href="https://github.com/features/actions">GitHub Actions</a> released in August 2019 &#8211; I&#8217;ve been trying them out for nearly a full year, using beta access available the adventurous before it was generally available. It was a long time in coming, and I saw this feature as GitHub&#8217;s missing piece. Some great companies stepped into that early gap and provide excellent services: <a href="https://travis-ci.org">TravisCI</a>, <a href="https://circleci.com">CircleCI</a>, <a href="https://codeship.com">codeship</a>, <a href="https://semaphoreci.com">SemaphoreCI</a>, <a href="https://www.bitrise.io">Bitrise</a>, and many others. I&#8217;ve used most of these, predominantly TravisCI because it was available before the rest, and I got started with it. When GitHub finally did circle back and make actions available, I was there trying it out and seeing how it worked.</p>



<p>Setting up CI for macOS and iOS project has always been a little odd, but doable. For many people who are making apps, the goal is to build the code, run any unit tests, maybe run some UI or integration tests, sign the resulting elements, and ship the whole out via <a href="https://testflight.apple.com">testflight</a>. Tools like <a href="https://fastlane.tools">fastlane</a> do a spectacular job of helping to automate into these services where Apple hasn&#8217;t provided a lot of support, or connected the dots.</p>



<p>I&#8217;m going to focus a bit more narrowly in this post &#8211; looking at how to leverage <a href="https://swift.org/package-manager/">swift package manager</a> and <a href="https://developer.apple.com/library/archive/technotes/tn2339/_index.html">xcodebuild</a>, both command line tools for building swift projects or mac and iOS applications respectively. I&#8217;ll leave the whole &#8220;setting up fastlane&#8221;, dealing with the complexities of signing code, and submitting builds from CI systems to others.</p>



<h2>Building swift packages with github actions</h2>



<p>If you want to build a swift package, then reach for <a href="https://github.com/apple/swift-package-manager/blob/master/Documentation/Usage.md">swiftpm</a>. You can&#8217;t build macOS or iOS applications with swiftpm, but you can create command-line executables or compile swift libraries. Most interestingly, you can compile swift on other platforms &#8211; linux is supported, and other operating systems (Windows, Android, and more) are being worked on by the swift open source community. Swiftpm is also the go-to tooling if you want to use the burgeoning <a href="https://forums.swift.org/c/server/43">server-side swift</a> capabilities.</p>



<p>While there are some complicated corners to using the swift package manager, especially when it comes to integrating existing C or C++ libraries,  the basics for how to use it are gloriously simple:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: bash; title: ; notranslate">
swift test --enable-test-discovery
</pre></div>


<p>To use that tooling, we need to define how it&#8217;s invoked &#8211; and that whole accumulation of detail is what goes into a GitHub Action declarative workflow.</p>



<p>To set up an action, create a YAML file in the directory <code>.github/workflows</code> at the root of your repository. GitHub will look in this directory for YAML files and they&#8217;ll become the actions enabled for your repository. The documentation for github actions is available at <a href="https://help.github.com/en/actions">https://help.github.com/en/actions</a>, but it isn&#8217;t exactly easy deciphering unless you&#8217;re already familiar with CI and some github specific terms. </p>



<p>One the simplest CI definitions I&#8217;ve seen is the CI running on <a href="https://swiftdoc.org">SwiftDocOrg</a>&#8216;s <a href="https://github.com/SwiftDocOrg/DocTest">DocTest repository</a>, which builds its executables for both swift on macOS and swift on Linux:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
name: CI
on:
  push:
    branches: &#91;master]
  pull_request:
    branches: &#91;master]
jobs:
  macos:
    runs-on: macOS-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v1
      - name: Build and Test
        run: swift test
        env:
          DEVELOPER_DIR: /Applications/Xcode_11.4.app/Contents/Developer
  linux:
    runs-on: ubuntu-latest
    container:
      image: swift:5.2
      options: --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --security-opt apparmor=unconfined
    steps:
      - name: Checkout
        uses: actions/checkout@v1
      - name: Build and Test
        run: swift test --enable-test-discovery
</pre></div>


<p>To explain the setup, let&#8217;s look at it in pieces. The very first piece is the name of the action: <code>CI</code>. For all practical purposes the name effects nothing, but knowing the name can be helpful. GitHub indexes actions by name. What I find most useful is that GitHub provides an easy-to-use badge that you can drop into a README file, so that people viewing the rendered markdown will have a quick look as to the current build status.</p>



<p>The badge uses the repository name and the workflow name together in a URL. The pattern for this URL is:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
https://github.com/USERNAME/REPOSITORY_NAME/workflows/WORKFLOW_NAME/badge.svg
</pre></div>


<p>Make an image link in markdown to display this badge in your README. For example, a badge for DocTest&#8217;s repository could be:<a href="https://github.com/heckj/MPCF-TestBench/actions"></a></p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
&#91;!&#91;Actions Status](https://github.com/SwiftDocOrg/DocTest/workflows/CI/badge.svg)](https://github.com/SwiftDocOrg/DocTest/actions)
</pre></div>


<p>The next segment is <code>on</code>, that defines when the action will be applied. DocTest&#8217;s repository has the action triggering when the <code>master</code> branch (but no other branches) changes via push, or when a pull request is opened against the <code>master</code> branch. <a href="https://github.com/heckj/MPCF-TestBench/actions"></a></p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
on:
  push:
    branches: &#91;master]
  pull_request:
    branches: &#91;master]
</pre></div>


<p>The next segment is <code>jobs</code>, which has two elements defined underneath it. Each job is run separately, and you may declare dependencies between jobs if you want or need. Each job defines where it runs &#8211; more specifically what operating system is used, and DocTest&#8217;s example has a job for building on macOS and another for building on Linux.</p>



<p>The first is the <code>macos</code> job, which runs within a macOS virtual machine:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
jobs:
  macos:
    runs-on: macOS-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v1
      - name: Build and Test
        run: swift test
        env:
          DEVELOPER_DIR: /Applications/Xcode_11.4.app/Contents/Developer
</pre></div>


<p>The steps are run linearly, each having to complete without a failure or error response before the next. This example shows the common practice for leveraging <code><a href="https://github.com/marketplace/actions/checkout">actions/checkout</a></code>, a pre-defined action in the GitHub &#8220;<a href="https://github.com/marketplace">Marketplace</a>&#8220;. </p>



<blockquote class="wp-block-quote"><p>Marketplace gets quotes because I think marketplace is poor name choice &#8211; you&#8217;re not required to buy anything, which I originally thought was the intention. And to be clear, I&#8217;m glad it&#8217;s not. GitHub&#8217;s mechanism allows anyone to host their own actions, and the marketplace is the place to find them.</p></blockquote>



<p>The second step is simply invoking <code>swift test</code>, just like you might on your own laptop with macOS installed. The environment variable <code>DEVELOPER_DIR</code> is being defined here, which Xcode uses as a means to indicate which version of Xcode to use when multiple are installed. The alternative way to do this is by explicitly selecting the version of Xcode with another command <code>xcode-select</code>.</p>



<p>The GitHub actions runners have been maintained impressively well over the past year, and even beta releases of Xcode are frequently available within weeks of when they are released. The VM image has an impressive array of commonly used tools, libraries, and languages pre-installed &#8211; and that&#8217;s maintained publicly in a list at <a href="https://github.com/actions/virtual-environments/blob/master/images/macos/macos-10.15-Readme.md">https://github.com/actions/virtual-environments/blob/master/images/macos/macos-10.15-Readme.md</a>.</p>



<p>By selecting the version of Xcode with the environment variable declaration, this also implies the version of swift that&#8217;s being used, swift version 5.2 in this case.</p>



<p>The last segment of this CI declaration is the version that builds the swift package on Linux.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
  linux:
    runs-on: ubuntu-latest
    container:
      image: swift:5.2
      options: --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --security-opt apparmor=unconfined
    steps:
      - name: Checkout
        uses: actions/checkout@v1
      - name: Build and Test
        run: swift test --enable-test-discovery
</pre></div>


<p>In this case, it&#8217;s using Ubuntu 18.04 (the latest supported by GitHub as I&#8217;m writing this post) &#8211; which has a corresponding README of everything that includes at <a href="https://github.com/actions/virtual-environments/blob/master/images/linux/Ubuntu1804-README.md">https://github.com/actions/virtual-environments/blob/master/images/linux/Ubuntu1804-README.md</a>.</p>



<p>The container declaration defines a docker contain that&#8217;s used to run these steps on top of that base linux image, in this case the <a href="https://hub.docker.com/layers/swift/library/swift/5.2/images/sha256-11c01323295bdda05ed0fcea10dd75b97f085f4ede3c3be44f6f08ca9d7e857e?context=explore">swift:5.2</a> image. The additional options listed are to open up specific security mechanisms otherwise locked down within a container &#8211; in this case, it&#8217;s enabling the <a href="http://man7.org/linux/man-pages/man2/ptrace.2.html">ptrace system call</a>, which <a href="https://forums.swift.org/t/rfc-removing-the-integrated-repl/35441/17">is critical to allowing swift to run an integrated REPL or use the LLDB debugger when run within a container</a>. </p>



<p>The last bit that you might have noticed is the option <code>--enable-test-discovery</code>. This is an <a href="https://github.com/apple/swift-package-manager/pull/2513">option available from the swift package manager that only recently released</a>. Where the Xcode leverages the objective-C runtime to dynamically inspect and identify test classes to run, the same didn&#8217;t exist (and was a right pain in the butt) for swift on Linux until this option was made available in swift 5.2. The build system creates an index while it&#8217;s building the code on Linux, and then uses this index to identify functions that should be invoked based on their name (the ones prefixed with <code>test</code> and that are within subclasses of <code>XCTest</code>). The end result is <code>swift test</code> &#8220;finding the tests&#8221; as most other unit testing libraries do.</p>



<h2>Building macOS or iOS applications using xcodebuild with github actions</h2>



<p>If you want to build a macOS, tvOS, iOS, or even watchOS application, use <code>xcodebuild</code>. <code>xcodebuild</code> is the command-line invocation that uses the build toolchain built into xcode, leveraging all the built in mechanisms with targets, schemes, build settings, and overlays interactions with the simulators. To use <code>xcodebuild</code>, you&#8217;ll need to have xcode installed &#8211; and with github actions, that&#8217;s available through virtualized instances of macOS with Xcode (and <a href="https://github.com/actions/virtual-environments/blob/master/images/macos/macos-10.15-Readme.md">a lot of other tools</a>) pre-installed.</p>



<p>The example repository I&#8217;m using is one of my own (<a href="https://github.com/heckj/MPCF-TestBench/blob/master/.github/workflows/build.yml">https://github.com/heckj/MPCF-TestBench/blob/master/.github/workflows/build.yml</a>), although it was a hard choice &#8211; as I really like <a href="https://github.com/Ranchero-Software/NetNewsWire/blob/master/.github/workflows/build.yml">NetNewsWire&#8217;s CI setup</a> as a good example. Definitely take a look at <a href="https://github.com/Ranchero-Software/NetNewsWire/blob/master/.github/workflows/build.yml">https://github.com/Ranchero-Software/NetNewsWire/blob/master/.github/workflows/build.yml</a> and the corresponding <a href="https://github.com/Ranchero-Software/NetNewsWire/blob/master/Technotes/ContinuousIntegration.md">CI Tech Note</a> for some excellent detail and to see how another project enabled CI on GitHub.</p>



<p>The whole CI file, from <a href="https://github.com/heckj/MPCF-TestBench/blob/master/.github/workflows/build.yml">https://github.com/heckj/MPCF-TestBench/blob/master/.github/workflows/build.yml</a>:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
name: CI
on: &#91;push]
jobs:
  build:
    runs-on: macos-latest
    strategy:
      matrix:
        run-config:
          - { scheme: 'MPCF-Reflector-mac', destination: 'platform=macOS' }
          - { scheme: 'MPCF-TestRunner-mac', destination: 'platform=macOS' }
          - { scheme: 'MPCF-Reflector-ios', destination: 'platform=iOS Simulator,OS=13.4.1,name=iPhone 8' }
          - { scheme: 'MPCF-TestRunner-ios', destination: 'platform=iOS Simulator,OS=13.4.1,name=iPhone 8' }
          - { scheme: 'MPCF-Reflector-tvOS', destination: 'platform=tvOS Simulator,OS=13.4,name=Apple TV' }
    steps:
    - name: Checkout Project
      uses: actions/checkout@v1
    - name: Homebrew build helpers install
      run: brew bundle
    - name: Show the currently detailed version of Xcode for CLI
      run: xcode-select -p
    - name: Show Build Settings
      run: xcodebuild -workspace MPCF-TestBench.xcworkspace -scheme '${{ matrix.run-config&#91;'scheme'] }}' -showBuildSettings
    - name: Show Build SDK
      run: xcodebuild -workspace MPCF-TestBench.xcworkspace -scheme '${{ matrix.run-config&#91;'scheme'] }}' -showsdks
    - name: Show Available Destinations
      run: xcodebuild -workspace MPCF-TestBench.xcworkspace -scheme '${{ matrix.run-config&#91;'scheme'] }}' -showdestinations
    - name: lint
      run: swift format lint --configuration .swift-format-config -r .
    - name: build and test
      run: xcodebuild clean test -scheme '${{ matrix.run-config&#91;'scheme'] }}' -destination '${{ matrix.run-config&#91;'destination'] }}' -showBuildTimingSummary
</pre></div>


<p>Both this example and NetNewsWire&#8217;s CI use the technique of a matrix build. This is immensely useful when you have multiple targets in the same Xcode project and want to verify that they&#8217;re all building and testing correctly. The matrix is defined right at the top of this file as a <code>strategy</code>:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
jobs:
  build:
    runs-on: macos-latest
    strategy:
      matrix:
        run-config:
          - { scheme: 'MPCF-Reflector-mac', destination: 'platform=macOS' }
          - { scheme: 'MPCF-TestRunner-mac', destination: 'platform=macOS' }
          - { scheme: 'MPCF-Reflector-ios', destination: 'platform=iOS Simulator,OS=13.4.1,name=iPhone 8' }
          - { scheme: 'MPCF-TestRunner-ios', destination: 'platform=iOS Simulator,OS=13.4.1,name=iPhone 8' }
          - { scheme: 'MPCF-Reflector-tvOS', destination: 'platform=tvOS Simulator,OS=13.4,name=Apple TV' }
</pre></div>


<p>This is a single job &#8211; meaning a single operating system to run the build &#8211; but when you use a matrix is replicates the job by the size of the matrix. In this case, there are 5 matrix definitions &#8211; 2 for macOS targets, 2 for iOS targets, and 1 target for tvOS. When this is run, it runs 5 parallel instances, each with its own matrix definition and applies those values to further steps. This example defines two properties, <code>scheme</code> and <code>destination</code>, to be filled out with different values for each matrix run, and which are used later in the steps. The <code>scheme</code> definition corresponds to the names of schemes that are in the Xcode workspace, and <code>destination</code> maps to parameters used to xcodebuild&#8217;s destination argument which is a combination of target platform, name, and version of the operating system to use.</p>



<blockquote class="wp-block-quote"><p>Something to be aware of &#8211; the specific values that are used in the <code>destinations</code> of the matrix will vary with the version of Xcode, and the latest version of Xcode will always be used unless you explicitly override it. In this example, I am intentionally setting it to latest to keep tracking any updates, but if you&#8217;re building a CI system for verifying stability over time, that&#8217;s probably something you want to explicitly declare and lock down in your CI configuration.</p></blockquote>



<p>Checkout is pretty self explanatory, and right after that step you&#8217;ll see the step that installs helpers.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
    - name: Homebrew build helpers install
      run: brew bundle
</pre></div>


<p>This uses a feature of <a href="https://brew.sh">Homebrew</a> called <code>bundle</code>, which reads a file named <code><a href="https://github.com/heckj/MPCF-TestBench/blob/master/Brewfile">Brewfile</a></code>, allowing you to define a single place to say &#8220;install all these additional tools for my later use&#8221;.</p>



<p>The next steps are purely informational, and aren&#8217;t actually needed for the build, but are handy to have for debugging:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
    - name: Show the currently detailed version of Xcode for CLI
      run: xcode-select -p
    - name: Show Build Settings
      run: xcodebuild -workspace MPCF-TestBench.xcworkspace -scheme '${{ matrix.run-config&#91;'scheme'] }}' -showBuildSettings
    - name: Show Build SDK
      run: xcodebuild -workspace MPCF-TestBench.xcworkspace -scheme '${{ matrix.run-config&#91;'scheme'] }}' -showsdks
    - name: Show Available Destinations
      run: xcodebuild -workspace MPCF-TestBench.xcworkspace -scheme '${{ matrix.run-config&#91;'scheme'] }}' -showdestinations

</pre></div>


<p>These all invoke xcodebuild with the various options to show the parameters available. These parameters vary with the version of Xcode that is running, and the very first command (<code>xcode-select -p</code>) prints that version.</p>



<p>The next command, named <em>lint</em>, uses one of the helpers that is defined in that <a href="https://github.com/heckj/MPCF-TestBench/blob/master/Brewfile">Brewfile</a> &#8211; <a href="https://github.com/apple/swift-format">swift format</a>. In this example, I&#8217;m using Apple&#8217;s swift-format command (the other common example is <a href="https://github.com/nicklockwood/SwiftFormat">Nick Lockwood&#8217;s swiftformat</a>). In other projects I&#8217;ve used Nick&#8217;s <a href="https://github.com/nicklockwood/SwiftFormat">swiftformat</a> (and the often paired tool: <a href="https://github.com/realm/SwiftLint">swiftlint</a>), both of which I like quite a lot. This project was an excuse to try out the tooling from the Apple&#8217;s open source version and see how it worked, and how I liked working with it. My final opinion is still pending, but it mostly does the right thing.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
    - name: lint
      run: swift format lint --configuration .swift-format-config -r .
</pre></div>


<p>The lint step is really applying a verification of formatting, so arguably might not be relevant to be included in a build. I rely on linters (in several languages) to yell at me, as otherwise I&#8217;m lousy about consistency in how I format code. In practice, I also try and use the same tools to auto-format the code to just keep it up to whatever standard seems predominantly acceptable.</p>



<p>The final step is where it compiles and run the tests:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
    - name: build and test
      run: xcodebuild clean test -scheme '${{ matrix.run-config&#91;'scheme'] }}' -destination '${{ matrix.run-config&#91;'destination'] }}' -showBuildTimingSummary
</pre></div>


<p>And you can see the references to the matrix values that are applied directly as parameters to <code>xcodebuild</code>. The text <code>${{ matrix.run-config['scheme'] }}</code> is a replacement definition, indicating that the value of scheme for the running  matrix build should be dropped into that position for the command line argument.</p>



<p>The NetNewsWire project uses the exact same technique, although the developers run <code>xcodebuild</code> from within a shell script so that they can arrange for signing keys and other parameters to be set up. It&#8217;s a thoughtful and ingenious system, but quite a bit harder to explain or show the matrix being used directly.</p>



<h2>The downsides of GitHub Actions</h2>



<p>While I am very pleased with how GitHub actions works, I am completely leveraging the &#8220;no cost to run&#8221; options. The cost of the pay-for GitHub Actions is notable, and in some cases, injuriously expensive.</p>



<p>If you&#8217;re running GitHub actions in a private repository (and paying for the privilege)  you may find it just too expensive. The billing information for github actions shows that running macOS virtual images is 10 times the price of running Linux images ($0.08 per minute vs. $0.008 per minute). If you build on every pull request you&#8217;re going to wrack up an impressive number of minutes, very quickly. On top of that, techniques like the matrix build add an additional multiplier &#8211; 5x in my case. GitHub does offer the option of allowing you to create your own &#8220;GitHub Action runners&#8221;, and I&#8217;d recommend seriously looking at that option &#8211; just using GitHub as the coordinator &#8211; from the cost perspective alone. It&#8217;s more &#8220;stuff you have to maintain&#8221;, but in the end &#8211; likely quite a bit cheaper than paying the GitHub Actions hosting fees.</p>



<p>If you are building something purely open source, then you&#8217;re allowed to take advantage of the free services. That&#8217;s pretty darned nice.</p>



<h2>Where GitHub is not yet supporting the swift ecosystem</h2>



<p>This isn&#8217;t directly related to GitHub Actions, but more a related note on how GitHub is (or isn&#8217;t) supporting the swift ecosystem. There are a tremendous number of support options in their site for some great features, but really all of them are pretty anemic when it comes to supporting swift, either via Apple or through the open source ecosystem:</p>



<ul><li>There&#8217;s no current availability for swift language syntax highlighting. It does offer Objective-C, which is a pretty good fallback, but swift specific highlighting would be really lovely to have when reviewing pull requests or just reading code.</li><li>GitHub&#8217;s security mechanisms, which host security advisories, track dependency flow, and the recently announced <a href="https://github.blog/changelog/2020-05-06-github-advanced-security-code-scanning-now-available-in-limited-public-beta/">security code scanning</a> &#8211; don&#8217;t support swift:<ul><li>Dependencies through Package.swift, although parsable, aren&#8217;t tracked and aren&#8217;t reflected. </li><li>While you can draft a security advisory and host it at Github, it won&#8217;t propagate to any dependencies because it&#8217;s not tracked.</li></ul></li><li>A year ago GitHub announced that it would be supporting &#8220;Swift Packages&#8221; &#8211; I&#8217;m not aware of much that came of that effort, if anything. </li><li>The same constraint of not parsing and tracking dependencies is highlighted in their Insights tab and the &#8220;Dependency graph&#8221;. On swift packages, there&#8217;s nothing. Same with Xcode projects. Nada, zilch, zip.</li><li>The code scanning, which uses a really great technology called <a href="https://securitylab.github.com/tools/codeql">CodeQL</a>, <a href="https://help.semmle.com/codeql/supported-languages-and-frameworks.html">doesn&#8217;t support Swift, or even Objective-C</a>.</li></ul>



<p>At a bare minimum, I would have hoped that GitHub would have enabled parsing and showing dependencies for Package.swift. I can understand not wanting to reverse engineer the <code>.pbproj</code> XML plist structure to get to dependencies, but swift itself is open source and the package manifest is completely open. </p>



<p>In addition <a href="https://github.com/apple/swift-syntax">Swift Syntax</a>, the AST parsing mechanisms that swift enables through open source, are being used for swift language server support. These, I think, would be a perfect complement to leverage within CodeQL.</p>



<p>I do hope we&#8217;ll so more movement on these fronts from GitHub, but if not &#8211; well, I suppose it&#8217;s a good differentiating opportunity for competitive sites such as <a href="https://bitbucket.org/">BitBucket</a> or <a href="https://gitlab.com/">GitLab</a>.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>How to make a SwiftUI component that draws a Shape with light</title>
		<link>https://rhonabwy.com/2020/04/05/how-to-make-a-swiftui-component-that-draws-a-shape-with-light/</link>
					<comments>https://rhonabwy.com/2020/04/05/how-to-make-a-swiftui-component-that-draws-a-shape-with-light/#comments</comments>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 05 Apr 2020 19:00:00 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[swiftui]]></category>
		<category><![CDATA[viewbuilder]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5738</guid>

					<description><![CDATA[While I was experimenting with SwiftUI, one of the effects I wanted to re-create was taking a shape and making &#8220;stroke effects&#8221; on it. The goal was to create a SwiftUI component that could take an arbitrary path, apply this effect to the shape, and render it within the context of a larger SwiftUI view.<a class="more-link" href="https://rhonabwy.com/2020/04/05/how-to-make-a-swiftui-component-that-draws-a-shape-with-light/">Continue reading <span class="screen-reader-text">"How to make a SwiftUI component that draws a Shape with&#160;light"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>While I was experimenting with SwiftUI, one of the effects I wanted to re-create was taking a shape and making &#8220;stroke effects&#8221; on it. The goal was to create a SwiftUI component that could take an arbitrary path, apply this effect to the shape, and render it within the context of a larger SwiftUI view. </p>



<p>The effect I wanted to create was a &#8220;laser light&#8221; like drawing. You see this a lot in science fiction film user interfaces or backgrounds, and it is just kind of fun and neat. And yeah, I want it to be decent in both light and dark modes, although &#8220;dark mode&#8221; is where it will shine the most.</p>



<figure class="wp-block-image size-large"><img data-attachment-id="5758" data-permalink="https://rhonabwy.com/laserlightshape/" data-orig-file="https://josephheck.files.wordpress.com/2020/04/laserlightshape.png" data-orig-size="709,685" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="laserlightshape" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/04/laserlightshape.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2020/04/laserlightshape.png?w=709" src="https://josephheck.files.wordpress.com/2020/04/laserlightshape.png?w=709" alt="" class="wp-image-5758" srcset="https://josephheck.files.wordpress.com/2020/04/laserlightshape.png 709w, https://josephheck.files.wordpress.com/2020/04/laserlightshape.png?w=150 150w, https://josephheck.files.wordpress.com/2020/04/laserlightshape.png?w=300 300w" sizes="(max-width: 709px) 100vw, 709px" /></figure>



<p>And the code to represent this is:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; title: ; notranslate">
VStack {
    LaserLightShape(color: Color.orange, lineWidth: 1) {
        Rectangle()
    }

    LaserLightShape(color: Color.red, lineWidth: 2) {
        Circle()
    }

    LaserLightShape(color: Color.blue, lineWidth: 0.5) {
        Path { path in
            path.move(to: CGPoint(x: 0, y: 0))
            path.addLine(to: CGPoint(x: 50, y: 50))
        }
    }
}
</pre></div>


<p>Solving this challenge underscored the change in mindset from imperative code to declarative code. When I was looking through the SwiftUI methods, I kept looking for a methods with a means to &#8220;add on&#8221; to the existing view &#8211; or perhaps to replace an element within the view. In both of those cases, this highlighted my pattern of thinking &#8211; me telling the framework (or library) what to do. SwiftUI&#8217;s biggest win (and challenge) is inverting that thinking. </p>



<p>The methods to work with SwiftUI Views don&#8217;t &#8220;take a view, modify it, and hand it back&#8221;. Instead they take in some information, make a whole new View, and return it. There&#8217;s no &#8220;tweaking&#8221; or &#8220;changing&#8221; &#8211; the closest you get to that (imperative) paradigm is wholesale replacement. My natural instinct was to reach for something that had an explicit side effect, I suspect because that&#8217;s how a lot of languages I&#8217;ve used for years got something done. It&#8217;s a pattern I&#8217;m familiar with, and the first tool I reach towards.</p>



<p>This change in mindset is also why you&#8217;ll see a lot of the same people who &#8220;get&#8221; the new paradigm talking about how it overlaps with functional programming, using phrases like &#8220;pure functions&#8221;, and perhaps even &#8220;functors&#8221; and &#8220;monads&#8221;. I quickly get lost in many of these abstract concepts. For me, the biggest similarity is like when I understand the inversion of control that was the mental leap in moving from using a library to writing for a framework. This feels very much akin to that &#8216;Ah ha&#8217; moment. And for the record, I&#8217;m not asserting that I fully understand it &#8211; only that I recognize I need to change how I&#8217;m framing the problem in my head in order to use these new tools.</p>



<p>To solve this particular challenge, I originally started looking at <a href="https://developer.apple.com/documentation/swiftui/viewmodifier">SwiftUI ViewModifiers</a>. I&#8217;d been reading about them and thought <em>maybe that&#8217;s what I want</em>. Unfortunately, that didn&#8217;t work &#8211; ViewModifiers are great when you want to layer additional effects on a <a href="https://developer.apple.com/documentation/swiftui/view">View</a> &#8211; which means constraining what you do to the methods available and defined on the View protocol &#8211; but I wanted to work on a <a href="https://developer.apple.com/documentation/swiftui/shape">Shape</a>, specifically leveraging the <a href="https://developer.apple.com/documentation/swiftui/shape/3365367-stroke">stroke</a> method, which is a different critter.</p>



<p>The solution that I came up with uses a <a href="https://developer.apple.com/documentation/swiftui/viewbuilder">ViewBuilder</a>. I wrote a bit about these before talking about making <a href="https://rhonabwy.com/2020/03/17/introducing-and-explaining-the-previewbackground-package/">PreviewBackground</a>. The mental framing that helped provide this solution was thinking about what I wanted to achieve as <em>taking some information</em> (a <a href="https://developer.apple.com/documentation/swiftui/shape">Shape</a>) and <em>returning some kind of View</em>. </p>



<p>A ViewBuilder is a fairly significantly generic-heavy function. So to make it accept something conforming to the Shape protocol, I specified that the generic type it accepted was constrained to the protocol. I am still stumbling through the specifics of how to effectively use generics and protocols with swift, and thought this was a pretty nice way to show some of its strength.</p>



<p>Without further ado, here&#8217;s the code that produces the effect:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; title: ; notranslate">
struct LaserLightShape&lt;Content&gt;: View where Content: Shape {
    let content: () -&gt; Content
    let color: Color
    let lineWidth: CGFloat

    @Environment(\.colorScheme) var colorSchemeMode
    var blendMode: BlendMode {
        if colorSchemeMode == .dark {
            // lightens content within a dark
            // color scheme
            return BlendMode.colorDodge
        } else {
            // darkens content within a light
            // color scheme
            return BlendMode.colorBurn
        }
    }

    init(color: Color, lineWidth: CGFloat, @ViewBuilder content: @escaping () -&gt; Content) {
        self.content = content
        self.color = color
        self.lineWidth = lineWidth
    }

    var body: some View {
        ZStack {
            // top layer, intended only to reinforce the color
            // narrowest, and not blurred or blended
            if colorSchemeMode == .dark {
                content()
                    .stroke(Color.primary, lineWidth: lineWidth / 4)
            } else {
                content()
                    .stroke(color, lineWidth: lineWidth / 4)
            }

            if colorSchemeMode == .dark {
                // pushes in a bit of additional lightness 
                // when in dark mode
                content()
                    .stroke(Color.primary, lineWidth: lineWidth)
                    .blendMode(.softLight)
            }
            // middle layer, half-width of the stroke and blended
            // with reduced opacity. re-inforces the underlying
            // color - blended to impact the color, but not blurred
            content()
                .stroke(color, lineWidth: lineWidth / 2)
                .blendMode(blendMode)

            // bottom layer - broad, blurred out, semi-transparent
            // this is the &quot;glow&quot; around the shape
            if colorSchemeMode == .dark {
                content()
                    .stroke(color, lineWidth: lineWidth)
                    .blur(radius: lineWidth)
                    .opacity(0.9)
            } else {
                // knock back the blur/background effects on
                // light mode vs. dark mode
                content()
                    .stroke(color, lineWidth: lineWidth / 2)
                    .blur(radius: lineWidth / 1.5)
                    .opacity(0.8)
            }
        }
    }
}
</pre></div>


<p>The instructions for what to do are embedded in body of the view we are returning. The pattern is one I looked up from online references of how to make this same effect in photoshop. The gist is:</p>



<ul><li>You take the base shape, make a wide stroke of it, and blur it out a bit. This will end up being the &#8220;widest&#8221; portion of the effect.</li><li>Over that, you put another layer &#8211; roughly half the width of the bottom layer, and stroke it with the color you want to show.</li><li>And then you add a final top layer, narrowest of the set, intended to put the &#8220;highlight&#8221; or shine onto the result.</li></ul>



<p>You&#8217;ll notice in the code that it&#8217;s also dynamic in regards to light and dark backgrounds &#8211; in a light background, I tried to reinforce the color without making the effect look like an unholy darkness shadow trying to swallow the result, and in the dark background I wanted to have a &#8220;laser light&#8221; light shine show through. I also found through trial-and-error experiments that it helped to have a fourth layer sandwiched in there in dark mode specifically to brighten up the stack, adding in more &#8220;white&#8221; to the end effect.</p>



<p>A lot of this ends up leveraging the <a href="https://developer.apple.com/documentation/swiftui/blendmode">blend modes</a> from SwiftUI that composite layers. I&#8217;m far from a master of blend modes, and had to look up a number of primers to figure out what I wanted from the fairly large set of possibilities.</p>



<p>I suspect there is a lot more that can be accomplished by leveraging the ViewBuilder pattern.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://rhonabwy.com/2020/04/05/how-to-make-a-swiftui-component-that-draws-a-shape-with-light/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2020/04/laserlightshape.png?w=709" medium="image" />
	</item>
		<item>
		<title>Introducing and explaining the PreviewBackground package</title>
		<link>https://rhonabwy.com/2020/03/17/introducing-and-explaining-the-previewbackground-package/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Tue, 17 Mar 2020 17:33:00 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[PreviewBackground]]></category>
		<category><![CDATA[swiftui]]></category>
		<category><![CDATA[viewbuilder]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5742</guid>

					<description><![CDATA[While learning and experimenting with SwiftUI, I use the canvas assistant editor to preview SwiftUI views extensively. It is an amazing feature of Xcode 11 and I love it. There is a quirk that gets difficult for me though &#8211; the default behavior of the preview provider uses a gray background. I frequently use multiple<a class="more-link" href="https://rhonabwy.com/2020/03/17/introducing-and-explaining-the-previewbackground-package/">Continue reading <span class="screen-reader-text">"Introducing and explaining the PreviewBackground package"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>While learning and experimenting with SwiftUI, I use the canvas assistant editor to preview SwiftUI views extensively. It is an amazing feature of Xcode 11 and I love it. There is a quirk that gets difficult for me though &#8211; the default behavior of the preview provider uses a gray background. I frequently use multiple previews while making SwiftUI elements, wanting to see my creation on a background supporting both light and dark modes.</p>



<p>The following little stanza is a lovely way to iterate through the modes and displaying them as previews:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; title: ; notranslate">
#if DEBUG
struct ExampleView_Previews: PreviewProvider {
    static var previews: some View {
        Group {
            ForEach(ColorScheme.allCases,
                    id: \.self) { scheme in

                Text("preview")
                    .environment(\.colorScheme, scheme)
                    .frame(width: 100,
                           height: 100,
                           alignment: .center)
                    .previewDisplayName("\(scheme)")
            }
        }
    }
}
#endif
</pre></div>


<p>Results in the following preview:</p>



<figure class="wp-block-image size-medium"><img data-attachment-id="5745" data-permalink="https://rhonabwy.com/gray_preview/" data-orig-file="https://josephheck.files.wordpress.com/2020/03/gray_preview.png" data-orig-size="536,824" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gray_preview" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/03/gray_preview.png?w=195" data-large-file="https://josephheck.files.wordpress.com/2020/03/gray_preview.png?w=536" src="https://josephheck.files.wordpress.com/2020/03/gray_preview.png?w=195" alt="" class="wp-image-5745" srcset="https://josephheck.files.wordpress.com/2020/03/gray_preview.png?w=195 195w, https://josephheck.files.wordpress.com/2020/03/gray_preview.png?w=390 390w, https://josephheck.files.wordpress.com/2020/03/gray_preview.png?w=98 98w" sizes="(max-width: 195px) 100vw, 195px" /></figure>



<p>The gray background doesn&#8217;t help all that much here. It is perfect when you are viewing a fairly composed element set, as you are often working over an existing background. When you are creating an element to stand alone, or moving an element. In those cases, I really want a background for the element.</p>



<p>And this is exactly what <a href="https://github.com/heckj/PreviewBackground">PreviewBackground</a> provides. I made PreviewBackground into a SwiftPM package. While I could have created this effect with a <code><a href="https://developer.apple.com/documentation/swiftui/viewmodifier">ViewModifier</a></code>, I tried it out as a <code><a href="https://developer.apple.com/documentation/swiftui/viewbuilder">ViewBuilder</a></code> instead, thinking it would be nice to wrap the elements I want to preview explicitly.</p>



<p>The same example, using PreviewBackground:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; title: ; notranslate">
import PreviewBackground

#if DEBUG
struct ExampleView_Previews: PreviewProvider {
    static var previews: some View {
        Group {
            ForEach(ColorScheme.allCases,
                    id: \.self) { scheme in
                PreviewBackground {
                    Text("preview")
                }
                .environment(\.colorScheme, scheme)
                .frame(width: 100,
                       height: 100,
                       alignment: .center)
                .previewDisplayName("\(scheme)")
            }
        }
    }
}
#endif
</pre></div>


<figure class="wp-block-image size-medium"><img data-attachment-id="5746" data-permalink="https://rhonabwy.com/background_preview/" data-orig-file="https://josephheck.files.wordpress.com/2020/03/background_preview.png" data-orig-size="506,740" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="background_preview" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/03/background_preview.png?w=205" data-large-file="https://josephheck.files.wordpress.com/2020/03/background_preview.png?w=506" src="https://josephheck.files.wordpress.com/2020/03/background_preview.png?w=205" alt="" class="wp-image-5746" srcset="https://josephheck.files.wordpress.com/2020/03/background_preview.png?w=205 205w, https://josephheck.files.wordpress.com/2020/03/background_preview.png?w=410 410w, https://josephheck.files.wordpress.com/2020/03/background_preview.png?w=103 103w" sizes="(max-width: 205px) 100vw, 205px" /></figure>



<p>The code is available on Github, and you may include it within your own projects by adding a swift package with the URL: <code><a href="https://github.com/heckj/PreviewBackground">https://github.com/heckj/PreviewBackground</a></code></p>



<p>Remember to <code>import PreviewBackground</code> in the views where you want to use it, and work away!</p>



<h2>Explaining the code</h2>



<p>There are not many examples of using ViewBuilder to construct a view, and this is a simple use case. Here is how it works:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; title: ; notranslate">
import SwiftUI

public struct PreviewBackground&lt;Content&gt;: View where Content: View {
    @Environment(\.colorScheme) public var colorSchemeMode

    public let content: () -&gt; Content

    public init(@ViewBuilder content: @escaping () -&gt; Content) {
        self.content = content
    }

    public var body: some View {
        ZStack {
            if colorSchemeMode == .dark {
                Color.black
            } else {
                Color.white
            }
            content()
        }
    }
}

</pre></div>


<blockquote class="wp-block-quote"><p>The heart of using <code><a href="https://developer.apple.com/documentation/swiftui/viewbuilder">ViewBuilder</a></code> is using it within a View initializer to return a (specific but) generic instance of View, and using the returned closure as a property that you execute when composing a view. </p></blockquote>



<p>There is a lot of complexity in that statement. Allow me to try and explain it:</p>



<p>Normally when creating a SwiftUI view, you create a struct that conforms to the View protocol. This is written in code as <code>struct SomeView: View</code>. You may use the default initializer that swift creates for you, or you can write your own &#8211; often to set properties on your view. <a href="https://developer.apple.com/documentation/swiftui/viewbuilder">ViewBuilder</a> allows you to take a function in that initializer that returns an arbitrary View. But since the kind of view is arbitrary, we need to make the struct generic &#8211; since we can&#8217;t assert exactly what type it will be until the closure is compiled. To tell the compiler it&#8217;ll need to do the work to figure out the types, we label the struct as a being generic, using the <code>&lt;SomeType&gt;</code> syntax:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; gutter: false; title: ; notranslate">
struct SomeView&lt;Content&gt;: View where Content: View
</pre></div>


<p>This says there is a generic type that we&#8217;re calling <code>Content</code>, and that generic type is expected to conform to the <code>View</code> protocol. There is a more compact way to represent this that you may prefer:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; gutter: false; title: ; notranslate">
struct SomeView&lt;Content: View&gt;: View
</pre></div>


<p>Within the view itself, we have a property &#8211; which we name <code>content</code>. The type of this content isn&#8217;t known up front &#8211; it is the arbitrary type that the compiler gets to infer from the closure that will provided in the future. This declaration is saying the <code>content</code> property will be a closure &#8211; taking no parameters &#8211; that returns some an arbitrary type we are calling <code>Content</code>.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; gutter: false; title: ; notranslate">
public let content: () -&gt; Content
</pre></div>


<p>Then in the initializer, we use ViewBuilder:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; gutter: false; title: ; notranslate">
public init(@ViewBuilder content: @escaping () -&gt; Content) {
    self.content = content
}
</pre></div>


<p>In case it wasn&#8217;t obvious, <a href="https://developer.apple.com/documentation/swiftui/viewbuilder">ViewBuilder</a> is a function builder, the swift feature that is enabling this declarative structure with SwiftUI. This is what allows us to ultimately use it with in that declarative syntax form. </p>



<p>The final bit of code to describe is using the <code><a href="https://developer.apple.com/documentation/swiftui/environment">@Environment</a></code> property wrapper. </p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">
@Environment(\.colorScheme) public var colorSchemeMode
</pre></div>


<p>The property wrapper is not in common use, but perfect for this need. The property wrapper uses exposes a specific part of the existing environment as a local property for this view. This is what enables <a href="http://github.com/heckj/PreviewBackground">PreviewBackground</a> to choose the color for the background appropriate to the mode. By reading the environment it chooses an appropriately colored background. It then uses that property to assemble a view by invoking the property named <code>content</code> (which was provided by the function builder) within a <code><a href="https://developer.apple.com/documentation/swiftui/zstack">ZStack</a></code>.</p>



<p>By using ViewBuilder, we can use the PreviewBackground struct like any other composed view within SwiftUI:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; gutter: false; title: ; notranslate">
var body: some View {
    PreviewBackground {
        Text("Hello there!")
    }
}
</pre></div>


<p>If we had created this code as a <a href="https://developer.apple.com/documentation/swiftui/viewmodifier">ViewModifier</a>, then using it would look  different &#8211; instead of the curly-bracket syntax, we would be chaining on a method. The default set up for something like that looks like:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; gutter: false; title: ; notranslate">
var body: some View {
    Text("Hello there!")
    .modify(PreviewBackground())
}
</pre></div>


<p>I wanted to enable the curly-bracket syntax for this, hence the choice of using a ViewBuilder.</p>



<h2><em>A side note about moving code into a Swift package</em></h2>



<p>When I created this code, I did so within the context of another project. I wanted to use it across a second project, and the code was simple enough (a single file) to copy/paste &#8211; but instead I went ahead and made it a Swift package. Partially to make it easier for anyone else to use, but also just to get a bit more experience with what it takes to set up and use this kind of thing.</p>



<p>The mistake that I made immediately on moving the code was <strong><em>not</em></strong> explicitly making all the structs and properties public. It moved over, compiled fine, and everything was looking great as a package, but then when I went to use it &#8211; I got some really odd errors:</p>



<pre class="wp-block-preformatted">Cannot call value of non-function type 'module&lt;PreviewBackground&gt;'</pre>



<p>In other instances (yes, I admit this wasn&#8217;t the first time I made this mistake &#8211; and it likely won&#8217;t be the last) the swift compiler would complain about the scope of a function, letting me know that it was using the default internal scope, and was not available. But SwiftUI and this lovely function builder mechanism is making the compiler work quite a bit more, and it is not nearly as good at identifying why this mistake might have happened, only that it was failing.</p>



<blockquote class="wp-block-quote is-style-large"><p>If you hit the error <code>Cannot call value of non-function type</code> when moving code into a package, you may have forgotten to make the struct (and relevant properties) explicitly public.</p></blockquote>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2020/03/gray_preview.png?w=195" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2020/03/background_preview.png?w=205" medium="image" />
	</item>
		<item>
		<title>Four strategies to use while developing SwiftUI components</title>
		<link>https://rhonabwy.com/2020/03/12/four-strategies-to-use-while-developing-swiftui-components/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Thu, 12 Mar 2020 17:05:00 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[swiftui]]></category>
		<category><![CDATA[xcode]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5716</guid>

					<description><![CDATA[Lets start out with the (possibly) obvious: when I code, I frequently make mistakes (and fix them); but while I am going through that process function builders are frequently kicking my butt. When you are are creating SwiftUI views, you use function builders intensely &#8211; and the compiler is often at a loss to explain<a class="more-link" href="https://rhonabwy.com/2020/03/12/four-strategies-to-use-while-developing-swiftui-components/">Continue reading <span class="screen-reader-text">"Four strategies to use while developing SwiftUI&#160;components"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Lets start out with the (possibly) obvious: when I code, I frequently make mistakes (and fix them); but while I am going through that process function builders are frequently kicking my butt. When you are are creating SwiftUI views, you use function builders intensely &#8211; and the compiler is often at a loss to explain how I screwed up. And yeah, even with the amazing new updates into the <a href="https://swift.org/blog/new-diagnostic-arch-overview/">Diagnostic Engine</a> alongside Swift 5.2, which I am loving.</p>



<p>What is a function builder? It is the thing that looks like a normal &#8220;do some work&#8221; code closure in swift that you use as the declarative structure when you are creating a SwiftUI view. When you see code such as:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; highlight: [5]; title: ; notranslate">
import SwiftUI

struct ASimpleExampleView: View {
    var body: some View {
        Text("Hello, World!")
    }
}
</pre></div>


<p>The bit after <code>some View</code> is the function builder closure, which includes the single line <code>Text("Hello, World!")</code>.</p>



<p>The first mistake I make is assuming all closures are normal &#8220;workin&#8217; on the code&#8221; closures. I immediately start trying to put every day code inside of function builders. When I do, the compiler &#8211; often immediately and somewhat understandably &#8211; freaks out. The error message that appears in Xcode:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; auto-links: false; gutter: false; title: ; notranslate">
Function declares an opaque return type, but has no return statements in its body from which to infer an underlying type
</pre></div>


<p>And some times there are other errors as well. It really depending on what I stacked together and how I grouped and composed the various underlying elements in that top level view, and ultimately what I messed up deep inside all that.</p>



<p>I want to do some calculations in <em>some</em> of what I am creating, but doing them inline in the function builder closures is definitely not happening, so my first recommended strategy:</p>



<blockquote class="wp-block-quote is-style-large"><p>Strategy #1: Move calculations into a function on the view</p></blockquote>



<p>Most of the reasons I&#8217;m doing a calculation is because I want to determine a value to hand in to a SwiftUI view modifier. Fiddling with the opacity, position, or perhaps line width. If you are really careful, you can do some of that work &#8211; often simple &#8211; inline. But when I do that work, I invariably screw it up &#8211; make a mistake in matching a type, dealing with an optional, or something. At those times when the code is inline in a function builder closure, the compiler is having a hell of a hard time figuring out what to tell me about how I screwed it up. By putting the relevant calculation/code into a function that returns an explicit type, the compiler gets a far more constrained place to provide feedback about what I screwed up.</p>



<p>As an example:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; title: ; notranslate">
struct ASimpleExampleView: View {
    func determineOpacity() -&gt; Double {
        1
    }

    var body: some View {
        ZStack {
            Text(&quot;Hello World&quot;).opacity(determineOpacity())
        }
    }
}
</pre></div>


<p>Some times you aren&#8217;t even doing calculations, and the compiler gets into a tizzy about the inferred type being returned. I have barked my shins on that particular edge repeatedly while experimenting with all the various options, seeing what I like in a visualization. The canvas assistant editor that is available in Xcode is a god-send for fast visual feedback, but I get carried away in assembling lots of blocks with <code>ZStacks</code>, <code>HStacks</code>, and <code>VStacks</code> to see what I can do. This directly leads to my second biggest win:</p>



<blockquote class="wp-block-quote is-style-large"><p>Strategy #2: Ruthlessly refactor your views into subcomponents.</p></blockquote>



<p>I am beginning to think that seeing repeated, multiple kinds of stacks together in a single view is possibly a <a href="https://en.wikipedia.org/wiki/Code_smell">code smell</a>. But more than anything else, keeping the code within a single SwiftUI view as brutally simple as possible gives the compiler a better than odds chance of being able to tell me what I screwed up, rather than throwing up it&#8217;s proverbial hands with an inference failure.</p>



<p>There are a number of lovely mechanisms with <code>Binding</code> that make it easy to compose and link to the relevant data that you want to use. When I am making a subcomponent that provides some visual information that I expect the enclosing view to be tracking, I have started using the <a href="https://developer.apple.com/documentation/swiftui/binding"><code>@Binding</code> property wrapper</a> to pass it in, which works nicely in the enclosing view.</p>



<blockquote class="wp-block-quote"><p><strong>TIP:</strong></p><p>When you&#8217;re using <code>@Binding</code>, remember that you can make a constant binding in the <code><a href="https://developer.apple.com/documentation/swiftui/previewprovider">PreviewProvider</a></code> in that same file:</p><p>YourView(someValue: .constant(5.0))</p></blockquote>



<p>While I was writing this, John Sundell has recently published a very in-depth look at exactly this topic. His article <a href="https://www.swiftbysundell.com/articles/avoiding-massive-swiftui-views/">Avoiding Massive SwiftUI Views</a> covers another angle of how and why to ruthlessly refactor your views.</p>



<p>On the topic of the mechanics of that refactoring, when we lean what to do, it leads to leveraging Xcode&#8217;s canvas assistant editor with<code><a href="https://developer.apple.com/documentation/swiftui/previewprovider">PreviewProvider</a></code> &#8211; and my next strategy:</p>



<blockquote class="wp-block-quote is-style-large"><p>Strategy #3: use Group and multiple view instances to see common visual options quickly</p></blockquote>



<p>This strategy is more or less obvious, and was highlighted in a number of the <a href="https://developer.apple.com/videos/play/wwdc2019/216/">SwiftUI WWDC presentations</a> that are online. The technique is immensely useful when you have a couple of variations of your view that you want to keep operational. It allows you to visually make sure they are working as desired while you continue development. In my growing example code, this looks like:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: objc; title: ; notranslate">
import SwiftUI

struct ASimpleExampleView: View {
    let opacity: Double
    @Binding var makeHeavy: Bool

    func determineOpacity() -&gt; Double {
        // maybe do some calculation here
        // mixing the incoming data
        opacity
    }

    func determineFontWeight() -&gt; Font.Weight {
        if makeHeavy {
            return .heavy
        }
        return .regular
    }

    var body: some View {
        ZStack {
            Text(&quot;Hello World&quot;)
                .fontWeight(determineFontWeight())
                .opacity(determineOpacity())
        }
    }
}

struct ASimpleExampleView_Previews: PreviewProvider {
    static var previews: some View {
        Group {
            ASimpleExampleView(opacity: 0.8, 
                makeHeavy: .constant(true))

            ASimpleExampleView(opacity: 0.8, 
                makeHeavy: .constant(false))
        }
    }
}
</pre></div>


<p>And the resulting canvas assistant editor view:</p>



<figure class="wp-block-image size-large"><img data-attachment-id="5725" data-permalink="https://rhonabwy.com/canvas_assistant_example/" data-orig-file="https://josephheck.files.wordpress.com/2020/03/canvas_assistant_example.png" data-orig-size="376,524" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="canvas_assistant_example" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2020/03/canvas_assistant_example.png?w=215" data-large-file="https://josephheck.files.wordpress.com/2020/03/canvas_assistant_example.png?w=376" src="https://josephheck.files.wordpress.com/2020/03/canvas_assistant_example.png?w=376" alt="" class="wp-image-5725" srcset="https://josephheck.files.wordpress.com/2020/03/canvas_assistant_example.png 376w, https://josephheck.files.wordpress.com/2020/03/canvas_assistant_example.png?w=108 108w, https://josephheck.files.wordpress.com/2020/03/canvas_assistant_example.png?w=215 215w" sizes="(max-width: 376px) 100vw, 376px" /></figure>



<p>This does not always help you experiment with what your views look like in all variations. For sets of pre-defined options, or data that influences your view, it can make a huge difference. A good variation that I recommend anyone use is setting and checking the accessibility environment settings to make sure everything renders as you expect. Another that I have heard is in relatively more frequent use: verifying localization rendering.</p>



<p>The whole rapid experimentation and feedback capability is what is so compelling about using SwiftUI. Which leads pretty directly to my next strategy:</p>



<blockquote class="wp-block-quote is-style-large"><p>Strategy #4: Consider making throw-away control views to tweak your visualization effects</p></blockquote>



<p>I am not fortunate enough to constantly work closely with a designer. Additionally, I often do not have the foggiest idea of how some variations will feel in terms of a final design. When the time comes, seeing the results on a device (or on multiple devices) makes a huge difference.</p>



<p><strong><em>You do not want to do this for every possible variation.</em></strong> That is where mocks fit into the design and development process &#8211; take the time to make them and see what you think. Once you have narrowed down your options to a few, then this strategy can really kick in and be effective.</p>



<p>In the cases when I have a few number of variations to try out, I encapsulate those options into values that I can control. Then I make a throw-away view that will never be shown in the final code that allows me to tweak the view within the context of a running application. Then the whole thing goes into whatever application I am working on &#8211; macOS, iOS, etc &#8211; and I see how it looks</p>



<p>When I am making a throw-away control view, I often also make (another throw-away) SwiftUI view that composes the control and controlled view together, as I intend to display it in the application. This is primarily to see the combined effect in a single Preview within Xcode.<em> The live controls are not active in the Xcode canvas assistant editor</em>, but it helps to see how having the controls influences the rest of the view structure.</p>



<blockquote class="wp-block-quote is-style-large"><p>A final note: Do not be too aggressive about moving code in a SwiftPM package</p></blockquote>



<p>You may (like me) be tempted to move your code into a library, especially with the <a href="https://developer.apple.com/documentation/xcode/adding_package_dependencies_to_your_app">lovely SwiftPM capabilities</a> that now exist within Xcode 11. This does work, and it functionally works quite well, from my initial experiments. But there is a significant downside, at least with the current versions (including Xcode 11.4 beta 3) &#8211; while you are still doing active development on the library:</p>



<p>If you open the package to edit or tweak it with Xcode, and load and build from <em>only</em> <code>Package.swift</code> without an associated Xcode project, the SwiftUI canvas assistant preview <strong><em>will</em></strong> <strong><em>not</em></strong> be functioning. If you use an Xcode project file, it works fine &#8211; so if you do go down this route, just be cautious about removing the Xcode project file for now. I have filed feedback to Apple to report the issue &#8211; both with Xcode 11.3 (FB7619098) and Xcode 11.4 beta 3 (FB7615026).</p>



<p>I would not recommend moving anything into a library until you used had it stable in case. There are also still some awkward quirks about developing code and a dependent library at the same time with Xcode. It can be done, but it plays merry havoc with Xcode&#8217;s automatic build mechanisms and CI.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2020/03/canvas_assistant_example.png?w=376" medium="image" />
	</item>
		<item>
		<title>Using Combine v1.1 is available</title>
		<link>https://rhonabwy.com/2020/03/07/using-combine-v1-1-is-available/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 08 Mar 2020 00:45:05 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[combine]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5713</guid>

					<description><![CDATA[After getting the major edits for the existing content done, I called the result the first release. As with any creative product, I wasn&#8217;t happy with some of the corners that still had rough edges. Over the past two weeks I fleshed those in, wrote a bunch of unit tests, figured out some of the<a class="more-link" href="https://rhonabwy.com/2020/03/07/using-combine-v1-1-is-available/">Continue reading <span class="screen-reader-text">"Using Combine v1.1 is&#160;available"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>After getting the major edits for the existing content done, I called the result the <a href="https://rhonabwy.com/2020/02/28/using-combine-first-edition-available/">first release</a>. As with any creative product, I wasn&#8217;t happy with some of the corners that still had rough edges. Over the past two weeks I fleshed those in, wrote a bunch of unit tests, figured out some of the darker corners that I&#8217;d previously ignored, and generally worked to improve on the overall consistency.</p>



<p>The results have been flowing into the online version as I merged them. And now the updated version, <a href="https://gum.co/usingcombine">available on gumroad in PDF and ePub format</a>, is updated as well. Anyone who&#8217;s previously purchased the content gets the updates for free &#8211; just log in and they are available for you.</p>



<p>The rough bits that were fleshed out include several focuses of content:</p>



<ul><li>Tests created and content written (and updated) for the <code><a href="https://heckj.github.io/swiftui-notes/#reference-multicast">multicast</a></code> and <code><a href="https://heckj.github.io/swiftui-notes/#reference-share">share</a></code> operators. The focus was primarily how they work and how to use them.</li><li>Worked through what the <code><a href="https://heckj.github.io/swiftui-notes/#reference-record">Record</a></code> publisher offers (and doesn&#8217;t offer), including how to serialize &amp; deserialize recorded streams (while this sounds cool, its not ultimately as useful as I hoped it might be).</li><li>Added the missing note that swift&#8217;s <code><a href="https://heckj.github.io/swiftui-notes/#reference-result">Result</a></code> type could also be used as a publisher, courtesy of a little extension that was added in Combine.</li><li>Updated some of the details of <code><a href="https://heckj.github.io/swiftui-notes/#reference-throttle">throttle</a></code> and <code><a href="https://heckj.github.io/swiftui-notes/#reference-debounce">debounce</a></code> with the specifics of delays that are incurred in timing, after having validated the theories with some unit tests (spoiler: debounce always delays the events by a short bit, but throttle doesn&#8217;t have much of an impact). I had previously written about <a href="https://rhonabwy.com/2019/12/15/combine-throttle-and-debounce/">throttle and debounce on this blog</a> as well.</li></ul>



<p>The new version is 1.1, <a href="https://github.com/heckj/swiftui-notes/tree/v1.1">tagged in the underlying repository</a> if you are so inclined to review/poke at the unit tests beyond the narrative details I shared in the book itself.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Using Combine &#8211; first edition available</title>
		<link>https://rhonabwy.com/2020/02/28/using-combine-first-edition-available/</link>
					<comments>https://rhonabwy.com/2020/02/28/using-combine-first-edition-available/#comments</comments>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Fri, 28 Feb 2020 21:14:34 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[combine]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5707</guid>

					<description><![CDATA[I just finished my first edit pass of the content of Using Combine, and am incredibly pleased. Sufficiently pleased, in fact, that I am going to call this version the &#8220;first edition&#8221;. It is certainly not perfect, nor even as complete as I would like, but a significant enough improvement that I wanted to put<a class="more-link" href="https://rhonabwy.com/2020/02/28/using-combine-first-edition-available/">Continue reading <span class="screen-reader-text">"Using Combine &#8211; first edition&#160;available"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I just finished my first edit pass of the content of <a href="https://gum.co/usingcombine">Using Combine</a>, and am incredibly pleased. Sufficiently pleased, in fact, that I am going to call this version the &#8220;first edition&#8221;.</p>



<p>It is certainly not perfect, nor even as complete as I would like, but a significant enough improvement that I wanted to put a stake in the ground and get it out there.</p>



<p>I&#8217;m not planning on stopping the development work, there are more examples, more details, and useful tidbits that will be developed. I have continued to receive wonderful feedback, and plan to continue to accept updates, as all the content, example code, and sample project pieces are available in the github repository <a href="https://github.com/heckj/swiftui-notes/">swiftui-notes</a>.</p>



<p>The content will continue to remain available for free in a single-page HTML format, hosted at <a href="http://heckj.github.io/swiftui-notes/">http://heckj.github.io/swiftui-notes/</a>. The ePub and PDF version is <a href="https://gum.co/usingcombine">available through gumroad</a>.</p>



<p></p>
]]></content:encoded>
					
					<wfw:commentRss>https://rhonabwy.com/2020/02/28/using-combine-first-edition-available/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>SwiftUI and Combine &#8211; Binding, State, and notification of changes</title>
		<link>https://rhonabwy.com/2020/02/26/swiftui-and-combine-binding-state-and-notification-of-changes/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Wed, 26 Feb 2020 17:17:53 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[combine]]></category>
		<category><![CDATA[swiftui]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5697</guid>

					<description><![CDATA[When I started the project that became Using Combine, it was right after WWDC; I watched streamed WWDC sessions online, captivated like so many others about SwiftUI. I picked up this idea that SwiftUI was &#8220;built using the new framework: Combine&#8221;. In my head, I thought that meant Combine managed all the data &#8211; notifications<a class="more-link" href="https://rhonabwy.com/2020/02/26/swiftui-and-combine-binding-state-and-notification-of-changes/">Continue reading <span class="screen-reader-text">"SwiftUI and Combine &#8211; Binding, State, and notification of&#160;changes"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>When I started the project that became <a href="http://gum.co/usingcombine">Using Combine</a>, it was right after <a href="https://developer.apple.com/wwdc19/">WWDC</a>; I watched streamed WWDC sessions online, captivated like so many others about SwiftUI. I picked up this idea that SwiftUI was &#8220;built using the new framework: Combine&#8221;. In my head, I thought that meant Combine managed all the data &#8211; notifications and content &#8211; for SwiftUI. And well, that ain&#8217;t so. While the original &#8220;built using Combine&#8221; is accurate, it misses a lot of detail, and the truth is a bit more complex.</p>



<p>After I finished my first run through drafting the content for Using Combine, I took some time to dig back into SwiftUI. I originally intended to write (and learn) more about that. In fact, SwiftUI is what started the whole segue into Combine. I hadn&#8217;t really tried to use SwiftUI seriously, or get into the details until just recently. I realized after all the work on examples for Combine and UIKit, I had completely short shifted the SwiftUI examples.</p>



<p>Mirroring a common web technology pattern, SwiftUI works as a declarative structure of what gets shown with the detail being completely derived from some source of truth &#8211; derived from state stored or managed <em>somewhere</em>. The introductory docs made is clear that <code><a href="https://developer.apple.com/documentation/swiftui/state">@State</a></code> was how this declarative mechanism could represent a bit of local state within a View, and with the benefit of <a href="https://dimsumthinking.com">Daniel</a> and <a href="https://www.hackingwithswift.com">Paul&#8217;s</a> writing (<a href="https://gumroad.com/l/swiftuikickstart">SwiftUI Kickstart</a> and <a href="https://www.hackingwithswift.com/quick-start/swiftui">SwiftUI by Example</a>), it was also quickly clear that <code><a href="https://developer.apple.com/documentation/swiftui/environmentobject">@EnvironmentObject</a></code> and <a href="https://developer.apple.com/documentation/swiftui/observedobject"><code>@ObservedObject</code></a> played a role there too.</p>



<p>The Combine link to SwiftUI, as it turns out, is really only about notifying the SwiftUI components that a model <em>had changed</em>, not at all <em>what changed</em>. The key is the protocol from Combine: <a href="https://heckj.github.io/swiftui-notes/#reference-observableobject">ObservableObject</a> (<a href="https://developer.apple.com/documentation/combine/observableobject">Apple&#8217;s docs</a>). This protocol, along with the <code><a href="https://developer.apple.com/documentation/combine/published">@Published</a></code> property wrapper, does the wonderful work of generating a combine publisher &#8211; the default type of which is represented by the class <a href="https://developer.apple.com/documentation/combine/observableobjectpublisher">ObservableObjectPublisher</a>. In the world of Combine, it has a defined output and failure type: <code>&lt;Void, Never&gt;</code>. The heart of that <code>Void</code> output type is that the data that is changing doesn&#8217;t matter &#8211; only that a change was happening.</p>



<p>So how does SwiftUI go and get the data it needs?<code><a href="https://developer.apple.com/documentation/swiftui/binding">Binding</a></code> is the SwiftUI generic structure that is used to do this linkage. The documentation at Apple asserts:</p>



<blockquote class="wp-block-quote"><p>Use a binding to create a two-way connection between a view and its underlying model. For example, you can create a binding between a&nbsp;<a href="https://developer.apple.com/documentation/swiftui/toggle"><code>Toggle</code></a>&nbsp;and a&nbsp;<code>Bool</code>&nbsp;property of a&nbsp;<a href="https://developer.apple.com/documentation/swiftui/state"><code>State</code></a>. Interacting with the toggle control changes the value of the&nbsp;<code>Bool</code>, and mutating the value of the&nbsp;<code>Bool</code>&nbsp;causes the toggle to update its presented state.</p><p>You can get a binding from a&nbsp;<code>State</code>&nbsp;by accessing its&nbsp;<em>binding</em>&nbsp;property. You can also use the&nbsp;<code>$</code>prefix operator with any property of a&nbsp;<code>State</code>&nbsp;to create a binding.</p><cite><a href="https://developer.apple.com/documentation/swiftui/binding" rel="nofollow">https://developer.apple.com/documentation/swiftui/binding</a></cite></blockquote>



<p>Looking around a bit more while creating some examples, and it becomes clear that some handy form elements (such as <code><a href="https://developer.apple.com/documentation/swiftui/textfield">TextField</a></code>) expect a parameter of type binding when they are declared. <code>Binding</code> itself works by leveraging swift&#8217;s property getters and setters. You can even manually create a <code>Binding</code> if you&#8217;re so inclined, defining the closures for get and set to whatever you like. Property wrappers such as <code>@State</code>, <code>@ObservedObject</code>, and <code>@EnvironmentObject</code> either create and expose a Binding, or create a wrapper that in turn passes back a <code>Binding</code>.</p>



<p>My take away is the flow with Combine and SwiftUI has a generally expected pattern: A model to be represented by a reference object, which sends updates when the data is about to change (by conforming to the <code>ObservableObject</code> protocol). SwiftUI goes and gets the data that it needs based on what was declared in the <code>View</code> using <code>Binding</code> to get to the underlying data (and potentially allowing the SwiftUI view to update it in turn if that&#8217;s relevant). </p>



<p>Given that SwiftUI views are also designed to be composed, I am leaning towards expecting a pattern that state will need to be defined for pretty much any variation of a view &#8211; and potentially externalized. The property wrappers for representing, and externalizing, state within SwiftUI are:</p>



<ul><li>@State</li><li>@ObservedObject and @Published</li><li>@EnvironmentObject</li></ul>



<p><code>@State</code> is all about local representation, and the simplest mechanism, simply providing a link to a property and the Binding. </p>



<p><code>@ObservedObject</code> (along with <code>@Published</code>) adds a notification mechanism on change, as well as a way to get a typed <code>Binding</code> to properties on the model. SwiftUI&#8217;s mechanism expects this always to be a reference type (aka a &#8216;class&#8217;), which ends up being pretty easy to define in code.</p>



<p><code>@EnvironmentObject</code> takes that a step further and exposes a reference model not just to a single view, but allows it to be used by any number of views in their own hierarchy.</p>



<ul><li>Drive most of the visual design choices entirely by the current state</li></ul>



<p>But  that&#8217;s not the only mechanism that is available: SwiftUI is also set up to react to a Combine publisher &#8211; although not in a heavily predetermined fashion. An interesting aspect is that all of the SwiftUI views also support a Combine subscriber: <code><a href="https://heckj.github.io/swiftui-notes/#reference-onreceive">onReceive</a></code>. So you can bring the publisher, and then write code within a <code>View</code> (or View component) to react to what it sends.</p>



<p>The onReceive subscriber acts very similarly to Combine&#8217;s <code><a href="https://heckj.github.io/swiftui-notes/#reference-sink">sink</a></code> subscriber &#8211; the single-closure version of <code>sink</code> (implying a Combine pipeline failure type of <code>Never</code>). You to define a closure within your SwiftUI view element that accepts that data and does whatever needs doing. This could be using the data, transforming and storing it into local <code>@State</code>, or just reacting to the fact that data was sent and updating the view based on that.</p>



<p>From a &#8220;What is a best practice&#8221; point of view, it seems the more you represent what you want to display within a reference model, the easier it will be to use. While you can expose a publisher right into a SwiftUI view, it tightly couples the combine publisher to the view and all links all those relevant types. You could (likely just as easily) have the model object encapsulate that detail &#8211; in which case the declaration of how you handle event changes over time are separated from how you present the view. This is likely a better separation of concerns.</p>



<p>The project (SwiftUI-Notes) linked to <a href="https://gum.co/usingcombine">Using Combine</a> now has two examples with Combine and SwiftUI. The first is a simple form validation (the view <a href="https://github.com/heckj/swiftui-notes/blob/master/SwiftUI-Notes/ReactiveForm.swift">ReactiveForm.swift</a> and model <a href="https://github.com/heckj/swiftui-notes/blob/master/SwiftUI-Notes/ReactiveFormModel.swift">ReactiveFormModel.swift</a>). This uses both the pattern of encapsulating the state within the model, and exposing a publisher to the SwiftUI View to show what <em>can</em> be done. I&#8217;m not espousing that the publisher mechanism is a good way to solve that particular problem, but it <em>illustrates what can be done</em> nicely.</p>



<p>The second example is a view (<a href="https://github.com/heckj/swiftui-notes/blob/master/SwiftUI-Notes/HeadingView.swift">HeadingView.swift</a>) that uses a model and publisher I created to use the built-in <a href="https://developer.apple.com/documentation/corelocation/">CoreLocation</a> framework. The model (<a href="https://github.com/heckj/swiftui-notes/blob/master/SwiftUI-Notes/LocationModelProxy.swift">LocationModelProxy.swift</a>) exposes the authorization as a published property, as well as the location updates through a publisher. Within the built-in Cocoa framework, those are normally exposed through a delegate callback. A large number of the existing Cocoa frameworks are convertible into a publisher-based mechanism to work with Combine using this pattern. The interesting bit was linking this up to SwiftUI, which was fun &#8211; although this example only taps the barest possibility of what could done.</p>



<p>It will be interesting to see what Apple might provide in terms of adopting Combine as alternative interfaces to its existing frameworks. CoreLocation is such a natural choice with its streaming updates, but there are a lot of others that could be used as well. And of course I&#8217;m looking forward to seeing how they expand on SwiftUI &#8211; and if they bring in more Combine based mechanisms into it or not.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>programming tools &#8211; start with sketching and build to fully specified</title>
		<link>https://rhonabwy.com/2020/02/25/programming-tools-start-with-sketching-and-build-to-fully-specified/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Tue, 25 Feb 2020 21:25:23 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5689</guid>

					<description><![CDATA[A couple of years ago I was managing a very distributed team &#8211; or really set of teams: Shanghai, Minneapolis, Seattle, Santa Clara, and Boston. Everyone worked for the same company, but culturally the teams were hugely divergent. In one region, the developers in the team preferred things to be as loosely defined as possible.<a class="more-link" href="https://rhonabwy.com/2020/02/25/programming-tools-start-with-sketching-and-build-to-fully-specified/">Continue reading <span class="screen-reader-text">"programming tools &#8211; start with sketching and build to fully&#160;specified"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>A couple of years ago I was managing a very distributed team &#8211; or really set of teams: Shanghai, Minneapolis, Seattle, Santa Clara, and Boston. Everyone worked for the same company, but culturally the teams were hugely divergent.</p>



<p>In one region, the developers in the team preferred things to be as loosely defined as possible. Their ideal state was &#8220;tell me how you&#8217;re going to rate me&#8221; and then you got of their way as they min-max&#8217;d based on whatever measurement rules you just set in place. When they were in charge of a specific element of the solution, there was a great deal of confidence it&#8217;d get done and effectively. But when they needed to be a bit more of active coordinator with a growing solution, they struggled quite a bit more &#8211; communication overhead really took a steep hit.</p>



<p>The opposite end of the spectrum was a region where the developers preferred for everything to be fully specified. Every detail defined, every option laid out and determined, how things work, and how things fail. Lots of detailed definition, to a level that extremely exhaustive &#8211; and would frankly annoy nearly all the developers from the first region.</p>



<p>I would love to make a hasty generalization and tell you that this was a &#8220;web developer&#8221; on one side and an ex-firmware developer on the other, but these folks were *all* firmware-level developers, stepping into a new team we had merged from a variety of pieces and places. Everyone was dealing with new development patterns, a new language &#8211; quite different from what they were used to historically, and a different business reality with goals and intentions that were in a high degree of flux.</p>



<p>What is surprising to me is that while I started out sympathizing with the first region&#8217;s culture far more, the influence of the spectrum &#8211; and the far side that preferred the specificity is what has stuck with me. I had previously left a lot more unspecified and open to interpretation &#8211; and not surprisingly that bit back upon occasion. Now with programming, regardless of the language, I want the tools to help me support that far more detailed, specified version of what I expect.</p>



<p>In the last year, I&#8217;ve actively programmed in C, C++, Javascript, TypeScript, Swift, Python, and Objective-C. When I programmed in dynamic languages (javascript, python) I just *expected* to need to write a lot of tests, including ones that a type system would otherwise do some overlapping validation with. A lot of time it was tracking null values, or explicitly invalid inputs to functions and systems to make sure the responses were as expected (errors and failures). As I followed this path, I gained a huge amount of respect for the optional concept that&#8217;s been embedded into Swift. It has been an amazing eye opener for me, and I find I struggle to go back to languages without it now.</p>



<p>Last fall I diverted heavily into a C++ project, where we explicitly pulled in and used an optional extension through a library: <a href="http://foonathan.net/type_safe">type_safe</a>. Back in December, <a href="https://developercommunity.visualstudio.com/content/problem/811155/ice-trying-to-compile-type-safecompact-optional.html">Visual Studio went and exposed a fatal compiler error bug in the Microsoft compiler update</a> that just exploded when attempting to use this library, much to my annoyance. But even with the type system of C++ and this library working with me, I can&#8217;t even *imagine* trying to write stable C++ code without doing a ton of tests to validate what&#8217;s happening and how it&#8217;s working &#8211; and failing.</p>



<blockquote class="wp-block-quote"><p>I&#8217;m still tempted to delve into <a href="https://www.rust-lang.org">Rust</a>, as I keep hearing and seeing references to it as a language with very similar safety/structural goals to swift (quite possibly with stronger guarantees &#8211; I don&#8217;t have a great way to judge the specifics there though). Bryan Cantrill keeps pimping it in the background of his conversations at <a href="https://oxide.computer/blog/categories/on-the-metal/">Oxide Computing&#8217;s On The Metal podcast</a>. If you&#8217;re into some fascinating history/interviews about the software-hardawre interface, it&#8217;s amazing!</p></blockquote>



<p>The end result is that I&#8217;ll start out sketching in ideas, but want my tools and languages to help me drive to a point where I&#8217;m far more &#8220;fully specified&#8221; as I go through the development process and iterate on the code. In my ideal world, which I don&#8217;t really expect, the language and software tooling helps me have confidence in the correctness and functionality of the code I&#8217;ve written, while allowing me the expressiveness to explore ideas as quickly as possible. </p>



<p>Just like the cultures and regions I navigated with that global team, it is a matter of sliding around on the spectrum of possibilities to optimize for the software challenge at hand.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>CRDTs and lockless data structures</title>
		<link>https://rhonabwy.com/2019/12/31/crdts-and-lockless-data-structures/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Tue, 31 Dec 2019 22:29:17 +0000</pubDate>
				<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[crdt]]></category>
		<category><![CDATA[lock-free-algorithm]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5681</guid>

					<description><![CDATA[A good five or so years ago Shevek, a friend I met while building cloud-generating appliances at the (now defunct) Nebula, spent an afternoon and evening describing the benefits of lock-free data structures to me. He went deep into the theoretical aspects of it, summarizing a lot of the research thinking at the time. While<a class="more-link" href="https://rhonabwy.com/2019/12/31/crdts-and-lockless-data-structures/">Continue reading <span class="screen-reader-text">"CRDTs and lockless data&#160;structures"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>A good five or so years ago Shevek, a friend I met while building cloud-generating appliances at the (now defunct) <a href="https://en.wikipedia.org/wiki/Nebula_(company)">Nebula</a>, spent an afternoon and evening describing the benefits of <a href="https://en.wikipedia.org/wiki/Non-blocking_algorithm">lock-free data structures</a> to me. He went deep into the theoretical aspects of it, summarizing a lot of the research thinking at the time. While I tried to keep up to retain it all later; I really didn&#8217;t manage it. The (predictable) end result was that I got the basics of what it did, why it was valuable, and some basic examples of how it was used, but I missed a lot of the <em>how can this be more directly useful to me</em>. A lot of that conversation was focused on multithreaded code and making it efficient, and what could be done (or not) to avoid contention blocks.</p>



<p>Jumping back to today, when I&#8217;m not writing away on <a href="https://gum.co/usingcombine">Using Combine</a>, I am helping some friends with a project that includes real-time collaborative editing. The goal is the same kind of thing that you see in Google Docs where multiple people are editing a document at the same time &#8211; you see each other&#8217;s cursors, lives updates, etc.</p>



<p>The earliest form of this kind of functionality that I used was originally a mac program called <a href="https://en.wikipedia.org/wiki/SubEthaEdit">SubEthaEdit</a>, and a couple years later a browser-based &#8220;text editor&#8221; called <a href="https://en.wikipedia.org/wiki/Etherpad">EtherPad</a>, which I used extensively when working with the open source project <a href="https://www.openstack.org">OpenStack</a>. (This was also around five years ago &#8211; I haven&#8217;t been actively contributing to OpenStack in quite a while). </p>



<p>Google adopted this capability as a feature, and has done an outstanding job of making it work. Happily, they also talk about how they do such things. In the details I was able to find, they often used the term &#8220;<a href="https://en.wikipedia.org/wiki/Operational_transformation">operational transformation</a>&#8221; to cover most of their recent efforts. That was also the key technology behind Google&#8217;s (now dead effort): <a href="https://en.wikipedia.org/wiki/Apache_Wave">Wave</a>. Other systems have done the same thing: <a href="https://github.com/josephg/ShareJS">ShareJS</a>, Atom&#8217;s <a href="https://jaxenter.com/xray-test-dummy-innovating-atom-142127.html">Xray</a>, and the editor <a href="https://xi-editor.io">Xi</a>.</p>



<p>I spent some time researching how others had tackled the problem of how you enable this kind of feature. The single best cohesive document I read was a blog post by Alexei Baboulevitch (aka &#8220;<a href="http://github.com/archagon/crdt-playground">Archagon</a>&#8220;) entitled: <a href="http://archagon.net/blog/2018/03/24/data-laced-with-history/">Data Laced with History: Causal Trees and Operational CRDTs</a>. The article describes his own code and implementation experiments, conveniently available <a href="https://github.com/archagon/crdt-playground">on github as crdt-playground</a>. It also includes a tremendously useful primer into the related topics and links to research papers. It also helped (me) that all his code was done with <a href="https://swift.org">Swift</a>, and readily available to try out and play with.</p>



<blockquote class="wp-block-quote"><p><a href="http://archagon.net/blog/2018/03/24/data-laced-with-history/">Data Laced with History: Causal Trees and Operational CRDTs</a> is absolutely amazing, but not an easy read. While I highly recommend it if you want to know how <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type">CRDT</a> works, expect to need to read through it several times before the details all sink in. Alexei writes clearly and very accurately, and it is extremely dense. I have read the whole article many times, as well as dug through all that code repeatedly, to gain an understanding of it.</p></blockquote>



<p>While I was buried in my own learning of how this works, I had an epiphany one evening: the core of a <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type">CRDT</a> is a lock-free data structure. Once I stepped back from the specifics of it, looking at the whole thing as a general algorithm, the pieces I picked up from Shevek clicked into place. Turns out that conversation (which I had mostly forgotten and thought I lost) had a huge amount of value for me, years later. The background I gleaned after the fact gave me some ideas to understand how and why CRDTs work, and ultimately  proved incredibly useful to understand how to effectively use them.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Human Voice</title>
		<link>https://rhonabwy.com/2019/12/27/human-voice/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Fri, 27 Dec 2019 16:32:43 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[micro.blog]]></category>
		<category><![CDATA[netnewswire]]></category>
		<category><![CDATA[rss]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5675</guid>

					<description><![CDATA[When I joined twitter, it was because my friends were talking about it. Conversations that I could normally only participate in during conferences or meetups became available to me. I tried to follow it slavishly at first, and then I had an epiphany that it was more like chatting with some friends at a restaurant<a class="more-link" href="https://rhonabwy.com/2019/12/27/human-voice/">Continue reading <span class="screen-reader-text">"Human Voice"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>When I joined twitter, it was because my friends were talking about it. Conversations that I could normally only participate in during conferences or meetups became available to me. I tried to follow it slavishly at first, and then I had an epiphany that it was more like chatting with some friends at a restaurant or bar &#8211; people are coming and going, and you chat with whomever is around and available. Facebook was similar, but family and friend focused &#8211; keeping up with what my friends are doing after I moved 1000 miles away.</p>



<p>Fast forward a decade, and the human voice has been nearly extinguished in both mediums. I still have accounts in both systems, but it&#8217;s more like turning on a constant advertising stream. I ceased being able to rely on either for even slight recency of human voices, let alone the friends and conversations that I used to have. They have become the modern noise-on-the-TV that I grew up with &#8211; nothing good on. Perhaps worse, because so much of it is emotionally strident &#8211; &#8220;this one small trick&#8221;, &#8220;you&#8217;ll be shocked and amazed&#8221;, etc. So much bullshit.</p>



<p>Fortunately I&#8217;ve (re)found a place where I can get that human voice again:</p>



<div class="wp-block-media-text alignwide is-stacked-on-mobile"><figure class="wp-block-media-text__media"><img data-attachment-id="5677" data-permalink="https://rhonabwy.com/nnw-ios-icon/" data-orig-file="https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png" data-orig-size="1120,1128" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nnw-ios-icon" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png?w=298" data-large-file="https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png?w=900" src="https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png?w=1017" alt="" class="wp-image-5677" srcset="https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png?w=1017 1017w, https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png?w=150 150w, https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png?w=298 298w, https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png?w=768 768w, https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png 1120w" sizes="(max-width: 1017px) 100vw, 1017px" /></figure><div class="wp-block-media-text__content">
<p class="has-large-font-size">NetNewsWire</p>



<p><a href="https://inessential.com/2019/12/26/netnewswire_2019_year_in_review">2019 in review</a></p>



<p><a href="https://inessential.com/2019/12/26/netnewswire_2020_roadmap_schmoadmap">2020 Roadmap</a></p>



<p> </p>
</div></div>



<p>I&#8217;ve gone back to checking my RSS feeds first for reading instead of hitting twitter or facebook, which makes a huge difference. That dopamine hit isn&#8217;t the same &#8211; RSS isn&#8217;t a never-ending stream of potentially interesting content that keeps you addicted like a manic crackhead, but the content that is there tends to be pretty darned good.</p>



<p>I&#8217;ve gone back to curating &#8211; looking for mentions and links, and following those back to sources. The &#8220;X Weekly&#8221; curated newsletters are equally good for finding new people to read, as well as friends of friends. It takes some effort, but that is also making it more real. If someone goes to wonky, I can easily ignore them for a bit, or drop their feed from my set &#8211; no shaming or cancel notification, just stepping away to more of what I&#8217;m interested in. </p>



<p>If you want to pipe up and join in the conversation, you can easily host your writing at <a href="https://micro.blog">Micro.blog</a>, WordPress, or Medium. Micro.blog and WordPress are $5 and $8 per month and Medium is no direct cost.</p>



<blockquote class="wp-block-quote"><p> Remember if you&#8217;re not paying for a service, you are likely the product&#8230;</p></blockquote>



<p> I have used WordPress for years, so I stuck there, but honestly the easy to get started option is very much <a href="https://micro.blog">micro.blog</a>. Write about whatever you want,  as much as you want. A sentence, paragraph, or longer &#8211; there&#8217;s no limit, no &#8220;right way/wrong way&#8221;, and you don&#8217;t need to torture your words into some small number of characters.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2019/12/nnw-ios-icon.png?w=1017" medium="image" />
	</item>
		<item>
		<title>Using Combine &#8211; reference content complete!</title>
		<link>https://rhonabwy.com/2019/12/24/using-combine-reference-content-complete/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Tue, 24 Dec 2019 17:41:08 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[combine]]></category>
		<category><![CDATA[swift]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5669</guid>

					<description><![CDATA[I&#8217;m thrilled to be announcing that an updated version of Using Combine is now available! It has taken me nearly 6 months to draft it all, reverse engineering and writing tests for all the various publishers, operators, and pieces in between &#8211; and documenting what I found. The end result is 182 pages (in US<a class="more-link" href="https://rhonabwy.com/2019/12/24/using-combine-reference-content-complete/">Continue reading <span class="screen-reader-text">"Using Combine &#8211; reference content&#160;complete!"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>I&#8217;m thrilled to be announcing that an updated version of Using Combine is now available!</p>



<p>It has taken me nearly 6 months to draft it all, <a href="https://github.com/heckj/swiftui-notes/tree/master/UsingCombineTests">reverse engineering and writing tests</a> for all the various publishers, operators, and pieces in between &#8211; and documenting what I found. The end result is 182 pages (in US PDF format) of reference documentation the way I’d generally like to have it.</p>



<p>While the <a href="https://heckj.github.io/swiftui-notes/">live site</a> is updated automatically, updated PDF and ePub versions are now available on <a href="https://gum.co/usingcombine">Gumroad</a>. If you purchased a copy previously, you can go to Gumroad and get an updated, DRM free, content in either PDF or ePub formats.</p>



<p>This updates finishes the largest swath of reference updates, creating tests to verify all the various operators and writing the documentation reference sections for the following:</p>



<ul><li><a href="https://heckj.github.io/swiftui-notes/#reference-operators-mapping">mapping operators</a></li><li><a href="https://heckj.github.io/swiftui-notes/#reference-operators-filtering">filtering operators</a></li><li><a href="https://heckj.github.io/swiftui-notes/#reference-operators-sequence">sequence operators</a></li><li><a href="https://heckj.github.io/swiftui-notes/#reference-operators-mathematical">math operators</a></li><li><a href="https://heckj.github.io/swiftui-notes/#reference-operators-criteria">criteria operators</a></li><li><a href="https://heckj.github.io/swiftui-notes/#reference-operators-reducing">reducing operators</a></li></ul>



<p>There was also an update for Xcode 11.3 and associated iOS 13.3 and macOS 10.15.2, which included some <a href="https://rhonabwy.com/2019/12/15/combine-throttle-and-debounce/">subtle changes to the throttle operator behavior</a>, which I recently wrote about in some detail.</p>



<p>With this update, the majority of the core content is now complete, but the work is by no means finished!</p>



<p>The next steps for the book are review and editing. On the to-do list are refining the descriptions of the reference sections, reviewing all the patterns now that we have had Combine for a few months, and seeing the updates as the API changes and refines. There are some diagrams now, but more are likely needed in some sections &#8211; both in the patterns and reference sections.</p>



<p>As before, this continues as a labor of love and for the community. Meaning that the content will continue to be free, available on the <a href="https://heckj.github.io/swiftui-notes/">live site</a>, with updates being made available as I make them. The work has been financially supported by 116 people as I&#8217;m writing this, as well as a number of people providing pull requests to fix typos and grammar flaws. </p>



<p>If you want a DRM-free digital copy in either PDF or ePub format, please consider <a href="https://gum.co/usingcombine">supporting this work by purchasing a copy at gumroad</a>.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Combine: throttle and debounce</title>
		<link>https://rhonabwy.com/2019/12/15/combine-throttle-and-debounce/</link>
					<comments>https://rhonabwy.com/2019/12/15/combine-throttle-and-debounce/#comments</comments>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 15 Dec 2019 19:31:36 +0000</pubDate>
				<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[combine]]></category>
		<category><![CDATA[ios]]></category>
		<category><![CDATA[swift]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5647</guid>

					<description><![CDATA[Updated March 2020 with more thoroughly accurate timing diagrams, after vetting against iOS13.2, iOS 13.3, and iOS13.4 beta. Combine was announced and released this past summer with iOS 13. And with this recent iOS 13 update, it is still definitely settling into place. While writing Using Combine, I wrote a number of tests to verify<a class="more-link" href="https://rhonabwy.com/2019/12/15/combine-throttle-and-debounce/">Continue reading <span class="screen-reader-text">"Combine: throttle and&#160;debounce"</span></a>]]></description>
										<content:encoded><![CDATA[
<p><em>Updated March 2020 with more thoroughly accurate timing diagrams, after vetting against iOS13.2, iOS 13.3, and iOS13.4 beta.</em></p>



<p>Combine was announced and released this past summer with iOS 13. And with this recent iOS 13 update, it is still definitely settling into place. While writing <a href="https://gumroad.com/l/usingcombine">Using Combine</a>, I wrote a number of tests to verify and generally double-check my understanding of how Combine was working. With the  update to iOS 13.3, the tests showed me that a few behaviors changed once again.</p>



<p>The operator that changed and trigged my tests was <a href="https://heckj.github.io/swiftui-notes/#reference-throttle">throttle</a>. Throttle is meant to act on values being received from an upstream publisher over a sliding window. It collapses the values flowing through a pipeline over time, choosing a representative value from the set that appeared within a given time window. It also turns out that throttle has slightly different behavior when you&#8217;re working with a publisher that starts out sending down an initial value (such as a <code>@Published</code> property).</p>



<p>While I was poking at throttle to understand how it changed, I also realized that <a href="https://heckj.github.io/swiftui-notes/#reference-debounce">debounce</a> was acting differently than I had originally understood, so I took some time to write some additional tests and make a more explicit timing diagram for it as well. Since debounce didn&#8217;t change behavior between 13.2 and 13.3 (that I spotted anyway), I&#8217;ll describe it first.</p>



<h2>debounce</h2>



<p>When you set up a debounce operator, you specify a time-window for it to react within. The operator collects all values that come in from the publisher, and most notably it resets the starting point for that sliding time window when it receives a value. It won&#8217;t send any values on until that entire window has expired <strong>without</strong> <strong>any</strong> <strong>other</strong> <strong>values</strong> <strong>appearing</strong>. In effect, it&#8217;s waiting for the value to settle. The marble diagrams show this really well.</p>



<figure class="wp-block-image size-large"><img data-attachment-id="5655" data-permalink="https://rhonabwy.com/debounce-2/" data-orig-file="https://josephheck.files.wordpress.com/2019/12/debounce-2.png" data-orig-size="659,222" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="debounce-2" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/12/debounce-2.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/12/debounce-2.png?w=659" src="https://josephheck.files.wordpress.com/2019/12/debounce-2.png?w=659" alt="" class="wp-image-5655" srcset="https://josephheck.files.wordpress.com/2019/12/debounce-2.png 659w, https://josephheck.files.wordpress.com/2019/12/debounce-2.png?w=150 150w, https://josephheck.files.wordpress.com/2019/12/debounce-2.png?w=300 300w" sizes="(max-width: 659px) 100vw, 659px" /><figcaption>Without a break that lets the sliding window expire, a single value is returned.</figcaption></figure>



<figure class="wp-block-image size-large"><img data-attachment-id="5656" data-permalink="https://rhonabwy.com/debounce_break-2/" data-orig-file="https://josephheck.files.wordpress.com/2019/12/debounce_break-2.png" data-orig-size="659,223" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="debounce_break-2" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/12/debounce_break-2.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/12/debounce_break-2.png?w=659" src="https://josephheck.files.wordpress.com/2019/12/debounce_break-2.png?w=659" alt="" class="wp-image-5656" srcset="https://josephheck.files.wordpress.com/2019/12/debounce_break-2.png 659w, https://josephheck.files.wordpress.com/2019/12/debounce_break-2.png?w=150 150w, https://josephheck.files.wordpress.com/2019/12/debounce_break-2.png?w=300 300w" sizes="(max-width: 659px) 100vw, 659px" /><figcaption>With a break that lets the sliding window expire, two values are propagated.</figcaption></figure>



<blockquote class="wp-block-quote"><p>When using <code>debounce</code>, you will see a delay invoked between when the event was published and the time it is received by your subscriber that is <em>at least</em> the value that you are processing the debounce across (0.5 seconds in the above example).</p></blockquote>



<h2>throttle</h2>



<p>Throttle acts similarly to debounce, in that it collects multiple results over time and sends out a single result &#8211; but it does so with fixed time windows. Where debounce will reset the start of that window, throttle does not &#8211; so it doesn&#8217;t collapse the values entirely, but sort of &#8220;slows them down&#8221; (and that matches the name of the operator pretty well).</p>



<p>Which value from the set that arrive that&#8217;s chosen to be propagated is influenced by the parameter <code>latest</code>, which is set when you create the operator. In general, latest being set to <code>true</code> results in the last value appearing getting chosen, and latest being set to <code>false</code> results in the first value that appears. This is one of those items that is a lot easier to understand by looking at a marble diagram:</p>



<figure class="wp-block-image size-large"><img data-attachment-id="5662" data-permalink="https://rhonabwy.com/throttle_false_13_2-1/" data-orig-file="https://josephheck.files.wordpress.com/2019/12/throttle_false_13_2-1.png" data-orig-size="659,222" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="throttle_false_13_2-1" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/12/throttle_false_13_2-1.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/12/throttle_false_13_2-1.png?w=659" src="https://josephheck.files.wordpress.com/2019/12/throttle_false_13_2-1.png?w=659" alt="" class="wp-image-5662" srcset="https://josephheck.files.wordpress.com/2019/12/throttle_false_13_2-1.png 659w, https://josephheck.files.wordpress.com/2019/12/throttle_false_13_2-1.png?w=150 150w, https://josephheck.files.wordpress.com/2019/12/throttle_false_13_2-1.png?w=300 300w" sizes="(max-width: 659px) 100vw, 659px" /><figcaption>Throttle (latest=false), under iOS 13.2.2</figcaption></figure>



<figure class="wp-block-image size-large"><img data-attachment-id="5659" data-permalink="https://rhonabwy.com/throttle_true_13_2/" data-orig-file="https://josephheck.files.wordpress.com/2019/12/throttle_true_13_2.png" data-orig-size="659,222" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="throttle_true_13_2" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/12/throttle_true_13_2.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/12/throttle_true_13_2.png?w=659" src="https://josephheck.files.wordpress.com/2019/12/throttle_true_13_2.png?w=659" alt="" class="wp-image-5659" srcset="https://josephheck.files.wordpress.com/2019/12/throttle_true_13_2.png 659w, https://josephheck.files.wordpress.com/2019/12/throttle_true_13_2.png?w=150 150w, https://josephheck.files.wordpress.com/2019/12/throttle_true_13_2.png?w=300 300w" sizes="(max-width: 659px) 100vw, 659px" /><figcaption>throttle (latest=true) under iOS 13.2.2</figcaption></figure>



<p>The notable behavior change is how it handles initial values. Initial value seems to be a bit &#8220;flexible&#8221; in what is specifically initial though. When I first wrote my tests, I was using a class with a <code>@Published</code> variable, which sends a value upon subscription, and then updates when it is changed. To illustrate the 13.3 behavior change better, I re-wrote and expanded those tests to use a <a href="https://heckj.github.io/swiftui-notes/#reference-passthroughsubject">PassthroughSubject</a>, so there wasn&#8217;t an automatic initial value. </p>



<figure class="wp-block-image size-large"><img data-attachment-id="5660" data-permalink="https://rhonabwy.com/throttle_false/" data-orig-file="https://josephheck.files.wordpress.com/2019/12/throttle_false.png" data-orig-size="659,222" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="throttle_false" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/12/throttle_false.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/12/throttle_false.png?w=659" src="https://josephheck.files.wordpress.com/2019/12/throttle_false.png?w=659" alt="" class="wp-image-5660" srcset="https://josephheck.files.wordpress.com/2019/12/throttle_false.png 659w, https://josephheck.files.wordpress.com/2019/12/throttle_false.png?w=150 150w, https://josephheck.files.wordpress.com/2019/12/throttle_false.png?w=300 300w" sizes="(max-width: 659px) 100vw, 659px" /><figcaption>throttle (latest=false) under iOS 13.3</figcaption></figure>



<figure class="wp-block-image size-large"><img data-attachment-id="5661" data-permalink="https://rhonabwy.com/throttle_true/" data-orig-file="https://josephheck.files.wordpress.com/2019/12/throttle_true.png" data-orig-size="659,222" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="throttle_true" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/12/throttle_true.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/12/throttle_true.png?w=659" src="https://josephheck.files.wordpress.com/2019/12/throttle_true.png?w=659" alt="" class="wp-image-5661" srcset="https://josephheck.files.wordpress.com/2019/12/throttle_true.png 659w, https://josephheck.files.wordpress.com/2019/12/throttle_true.png?w=150 150w, https://josephheck.files.wordpress.com/2019/12/throttle_true.png?w=300 300w" sizes="(max-width: 659px) 100vw, 659px" /><figcaption>throttle (latest=true) under iOS 13.3</figcaption></figure>



<p>So under iOS 13.3, the initial value (which is sent out roughly 100ms after the creation of the pipeline in my test), is always propagated, and then the sliding window effect begins immediately after that.</p>



<blockquote class="wp-block-quote"><p>When using <code>throttle</code>, you will see almost no delay between when the event was published and the time it is received by your subscriber. In the tests where I work the timing, using throttle with <code>latest=true</code> shows almost no delay, and throttle with <code>latest=false</code> with a very short delay: 20 to 40 ms in my test environment. <code>debounce</code>, by comparison, will always include a significant delay.</p></blockquote>



<p>If you want to see the underlying tests that illustrate this, check out the following bits of code from the Using Combine project:</p>



<ul><li><a href="https://github.com/heckj/swiftui-notes/blob/master/UsingCombineTests/DebounceAndRemoveDuplicatesPublisherTests.swift">DebounceAndRemoveDuplicatesPublisherTests.swift</a><ul><li><a href="https://github.com/heckj/swiftui-notes/blob/master/UsingCombineTests/DebounceAndRemoveDuplicatesPublisherTests.swift#L655">testSubjectDebounce</a></li><li><a href="https://github.com/heckj/swiftui-notes/blob/master/UsingCombineTests/DebounceAndRemoveDuplicatesPublisherTests.swift#L711">testSubjectDebounceWithBreak</a></li><li><a href="https://github.com/heckj/swiftui-notes/blob/master/UsingCombineTests/DebounceAndRemoveDuplicatesPublisherTests.swift#L599">testSpreadoutSubjectThrottleLatestTrue</a></li><li><a href="https://github.com/heckj/swiftui-notes/blob/master/UsingCombineTests/DebounceAndRemoveDuplicatesPublisherTests.swift#L541">testSpreadoutSubjectThrottleLatestFalse</a></li></ul></li></ul>



<p></p>
]]></content:encoded>
					
					<wfw:commentRss>https://rhonabwy.com/2019/12/15/combine-throttle-and-debounce/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2019/12/debounce-2.png?w=659" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/12/debounce_break-2.png?w=659" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/12/throttle_false_13_2-1.png?w=659" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/12/throttle_true_13_2.png?w=659" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/12/throttle_false.png?w=659" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/12/throttle_true.png?w=659" medium="image" />
	</item>
		<item>
		<title>Using Combine (v0.8) update available!</title>
		<link>https://rhonabwy.com/2019/11/20/using-combine-v0-8-update-available/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Wed, 20 Nov 2019 18:59:19 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[combine]]></category>
		<category><![CDATA[swift]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5643</guid>

					<description><![CDATA[A new version of Using Combine (v0.8) is now available. The live HTML site for Using Combine is updated automatically, and the PDF and ePub versions are now available on Gumroad. This version has a number of additional notes and changes, primarily from reader feedback, and some references to Combine&#8217;s changes with the release of<a class="more-link" href="https://rhonabwy.com/2019/11/20/using-combine-v0-8-update-available/">Continue reading <span class="screen-reader-text">"Using Combine (v0.8) update&#160;available!"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>A new version of Using Combine (v0.8) is now available.</p>



<p>The live <a href="https://heckj.github.io/swiftui-notes/">HTML site for Using Combine</a> is updated automatically, and the <a href="https://gum.co/usingcombine">PDF and ePub versions</a> are now available on <a href="https://gumroad.com">Gumroad</a>.</p>



<p>This version has a number of additional notes and changes, primarily from reader feedback, and some references to Combine&#8217;s changes with the release of IOS 13.2. A few more issues have been noted in the book, along with references to feedback reports sent to Apple where they may represent bugs or unexpected behavior in the current implementation.</p>



<p>This release also includes SVG based diagrams &#8211; so the original ASCII art diagrams are now gone, which should make that content far more accessible in the ePub format.</p>



<p>In addition, I added a section on <a href="https://heckj.github.io/swiftui-notes/#coreconcepts-marblediagram">marble diagrams</a>, specifically in <a href="https://heckj.github.io/swiftui-notes/#how-to-read-a-marble-diagram">how to read them</a> and <a href="https://heckj.github.io/swiftui-notes/#marble-diagrams-for-combine">how they apply to the code and examples illustrated in this book</a>. I originally planned on generating all the marble diagrams, but after repeated efforts at that I backed off that idea and am creating them by hand, with the help of <a href="https://www.omnigroup.com">OmniGroup</a>&#8216;s wonderful tool <a href="https://www.omnigroup.com/omnigraffle">OmniGraffle</a>. And yes, the source for this is also in the <a href="http://github.com/heckj/swiftui-notes/">github repository</a>.</p>



<p>For the next release, I am planning on getting back to detailing out the as-yet-unwritten section on a number of operators. </p>



<p>The project board at <a href="https://github.com/heckj/swiftui-notes/projects/1">https://github.com/heckj/swiftui-notes/projects/1</a> also reflects all the various updates still remaining to be written.</p>



<p>A huge thank you to all who have supported this book and my efforts to provide it!</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Japan</title>
		<link>https://rhonabwy.com/2019/11/04/japan/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Mon, 04 Nov 2019 20:18:00 +0000</pubDate>
				<category><![CDATA[Ranting and Reflections]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5625</guid>

					<description><![CDATA[We recently returned from our first visit to Japan. Karen and I have been wanting to go for quite a while, and the time was right &#8211; so most of the month of October was dedicated to that endeavor. I loved our trip, and we were fortunate enough to even be in Kyoto during on<a class="more-link" href="https://rhonabwy.com/2019/11/04/japan/">Continue reading <span class="screen-reader-text">"Japan"</span></a>]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img data-attachment-id="5627" data-permalink="https://rhonabwy.com/img_1499/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg" data-orig-size="4032,3024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1571512265&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;64&quot;,&quot;shutter_speed&quot;:&quot;0.0083333333333333&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1499" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=900" src="https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=1024" alt="" class="wp-image-5627" srcset="https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=1024 1024w, https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=2048 2048w, https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=150 150w, https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=300 300w, https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption>A torii gate at Danjo Garan in Koyosan</figcaption></figure>



<p>We recently returned from our first visit to Japan. Karen and I have been wanting to go for quite a while, and the time was right &#8211; so most of the month of October was dedicated to that endeavor.</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-rich wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe class='youtube-player' width='900' height='507' src='https://www.youtube.com/embed/8vrTwmYztdo?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent' allowfullscreen='true' style='border:0;' sandbox='allow-scripts allow-same-origin allow-popups allow-presentation'></iframe>
</div></figure>



<p>I loved our trip, and we were fortunate enough to even be in Kyoto during on of the festivals &#8211; Jidai Matsuri.</p>



<figure class="wp-block-embed-youtube wp-block-embed is-type-rich wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe class='youtube-player' width='900' height='507' src='https://www.youtube.com/embed/ErJp59c0bV0?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en&#038;autohide=2&#038;wmode=transparent' allowfullscreen='true' style='border:0;' sandbox='allow-scripts allow-same-origin allow-popups allow-presentation'></iframe>
</div></figure>



<p>Our trip and tour included Kushiro, Tokyo, Nagoya, Hiroshima, Meijima Island, and Kyoto &#8211; as well as a number of places in between that will take a while to unblur in my memory.</p>



<p></p>



<figure class="wp-block-image size-large"><img data-attachment-id="5629" data-permalink="https://rhonabwy.com/img_1777/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg" data-orig-size="4032,3024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1571832108&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;20&quot;,&quot;shutter_speed&quot;:&quot;0.00087489063867017&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1777" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=900" src="https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=1024" alt="" class="wp-image-5629" srcset="https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=1024 1024w, https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=2048 2048w, https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=150 150w, https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=300 300w, https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption>Hiroshima Memorial</figcaption></figure>



<p>We took a tour through <a href="https://www.samuraitours.com">Samurai Tours</a> &#8211; <a href="https://www.samuraitours.com/tour/best-of-japan/">Best of Japan</a>, which I highly recommend if you want a good sampling of a wide swath of Japan. Their guides were excellent and informative, and I can&#8217;t thank Charlie enough for his instruction in how to navigate the signage for the Tokyo trains and subways, which more or less held us in stead through the entire trip. I do wish we could have stayed longer in some areas, but another trip in the future will have to do.</p>



<figure class="wp-block-image size-large"><img data-attachment-id="5631" data-permalink="https://rhonabwy.com/img_1930/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg" data-orig-size="4032,3024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1572010138&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;100&quot;,&quot;shutter_speed&quot;:&quot;0.090909090909091&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1930" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=900" src="https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=1024" alt="" class="wp-image-5631" srcset="https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=1024 1024w, https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=2048 2048w, https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=150 150w, https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=300 300w, https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption>Yours truly, visiting a Studio Ghibli store in Kyoto</figcaption></figure>



<figure class="wp-block-image size-large"><img data-attachment-id="5632" data-permalink="https://rhonabwy.com/img_1834/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg" data-orig-size="3024,4032" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1571914279&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;20&quot;,&quot;shutter_speed&quot;:&quot;0.0058479532163743&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1834" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg?w=225" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg?w=768" src="https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg?w=768" alt="" class="wp-image-5632" srcset="https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg?w=768 768w, https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg?w=1536 1536w, https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg?w=113 113w, https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg?w=225 225w" sizes="(max-width: 768px) 100vw, 768px" /><figcaption>The Golden Shrine, in Kyoto</figcaption></figure>



<figure class="wp-block-image size-large"><img data-attachment-id="5633" data-permalink="https://rhonabwy.com/img_1701/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg" data-orig-size="16382,3628" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1571684095&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;25&quot;,&quot;shutter_speed&quot;:&quot;0.00020798668885191&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1701" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=900" src="https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=1024" alt="" class="wp-image-5633" srcset="https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=1024 1024w, https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=2048 2048w, https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=150 150w, https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=300 300w, https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption>Looking around from the top of Miyajima Island, near Hiroshima</figcaption></figure>



<p></p>



<figure class="wp-block-image size-large"><img data-attachment-id="5635" data-permalink="https://rhonabwy.com/img_1164/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg" data-orig-size="4032,2900" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1570987481&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;40&quot;,&quot;shutter_speed&quot;:&quot;0.041666666666667&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="img_1164" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=900" src="https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=1024" alt="" class="wp-image-5635" srcset="https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=1024 1024w, https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=2048 2048w, https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=150 150w, https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=300 300w, https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption>Tokyo subway map &#8211; yep, it&#8217;s complicated &#8211; but amazing!</figcaption></figure>



<figure class="wp-block-image size-large"><img data-attachment-id="5636" data-permalink="https://rhonabwy.com/img_1212/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg" data-orig-size="4032,3024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1571094109&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;25&quot;,&quot;shutter_speed&quot;:&quot;0.03030303030303&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1212" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=900" src="https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=1024" alt="" class="wp-image-5636" srcset="https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=1024 1024w, https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=2048 2048w, https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=150 150w, https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=300 300w, https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption>Pigment, a dangerous store for artists&#8230;</figcaption></figure>



<figure class="wp-block-image size-large"><img data-attachment-id="5637" data-permalink="https://rhonabwy.com/img_1266/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg" data-orig-size="3024,4032" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1571168104&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;20&quot;,&quot;shutter_speed&quot;:&quot;0.0013550135501355&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1266" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg?w=225" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg?w=768" src="https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg?w=768" alt="" class="wp-image-5637" srcset="https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg?w=768 768w, https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg?w=1536 1536w, https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg?w=113 113w, https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg?w=225 225w" sizes="(max-width: 768px) 100vw, 768px" /><figcaption>giant buddha at Kamakura Daibatsu</figcaption></figure>



<figure class="wp-block-image size-large"><img data-attachment-id="5639" data-permalink="https://rhonabwy.com/img_1334/" data-orig-file="https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg" data-orig-size="4032,3024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;iPhone X&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1571276868&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4&quot;,&quot;iso&quot;:&quot;50&quot;,&quot;shutter_speed&quot;:&quot;0.066666666666667&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="img_1334" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=900" src="https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=1024" alt="" class="wp-image-5639" srcset="https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=1024 1024w, https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=2048 2048w, https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=150 150w, https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=300 300w, https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption>The ryokan we stayed at in Takayama</figcaption></figure>



<p>The sheer artistry in every day life in Japan was amazing, and I miss it already being back in the US. Another trip in the future, I tell myself&#8230;</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1499.jpeg?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1777.jpeg?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1930.jpeg?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1834.jpeg?w=768" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1701.jpeg?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1164.jpeg?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1212.jpeg?w=1024" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1266.jpeg?w=768" medium="image" />

		<media:content url="https://josephheck.files.wordpress.com/2019/11/img_1334.jpeg?w=1024" medium="image" />
	</item>
		<item>
		<title>It is OK to test the framework</title>
		<link>https://rhonabwy.com/2019/11/03/it-is-ok-to-test-the-framework/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 03 Nov 2019 19:28:43 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[combine]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5611</guid>

					<description><![CDATA[When I started to write the book Using Combine, I was learning the Combine framework as I went. There was a lot I was unsure about, and especially given that it was released with the beta of the operating system, the implementation was changing between beta releases as it firmed up. I chose to use<a class="more-link" href="https://rhonabwy.com/2019/11/03/it-is-ok-to-test-the-framework/">Continue reading <span class="screen-reader-text">"It is OK to test the&#160;framework"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>When I started to write the book <a href="https://gum.co/usingcombine">Using Combine</a>, I was learning the Combine framework as I went. There was a lot I was unsure about, and especially given that it was released with the beta of the operating system, the implementation was changing between beta releases as it firmed up. I chose to use a technique that I picked up years ago from <a href="https://pragmaticstudio.com/about">Mike Clark</a> &#8211; write unit tests against the framework to verify my understanding of it &#8211; while writing the book. (yes, I&#8217;m still working on it &#8211; it&#8217;s a very lengthy process)</p>



<p>While listening to a few episodes of the <a href="https://www.relay.fm/radar">Under the Radar</a> podcast, I heard a number of references to the idea of &#8220;make sure you&#8217;re not testing the framework&#8221;. It is generally good advice, in the vein of &#8220;make sure you&#8217;re testing your code first and foremost&#8221;, but as a snippet out of context and taken as a rule &#8211; I think it&#8217;s faulty. Don&#8217;t confuse what you are testing, but reliably testing underlying frameworks or libraries, especially while learning them or they evolve, can easily be worth the effort.</p>



<p>I have received a huge amount of value from testing frameworks &#8211; first in verifying that I understand what the library is doing and how it works. More over, it has been a very clear signal when regressions do happen, or intentional functionality changes.</p>



<figure class="wp-block-pullquote"><blockquote><p>If you do add tests of a framework or library into your codebase, I recommend you break them out into their own set of tests. If something does change in the library, it will be far more clear that it is a change from the library and not a cascading side effect in your code.</p></blockquote></figure>



<p>Most recently, this effort paid off when I stumbled across a regression in the Combine framework functionality with the GM release of Xcode 11.2. While I&#8217;ve been coming up to speed with the various operators, I&#8217;ve written unit tests that work the operators. In this case, the <a href="https://heckj.github.io/swiftui-notes/#reference-throttle">throttle</a> operator &#8211; which has an option parameter <code>latest</code> &#8211; changed in how it operates with this release.</p>



<p>Throttle is very similar to the debounce operator, and in fact it operates the same if you use the option <code>latest=true</code>. They both take in values over time and return a single value for a specific time window. If you want the first value that&#8217;s sent within the timeframe, theoretically you should use <code>latest=false</code> with the <a href="https://heckj.github.io/swiftui-notes/#reference-throttle">throttle</a> operator. This worked in earlier releases of Combine and Xcode &#8211; but in the latest release, it&#8217;s now disregarding that path and sending only the latest value.</p>



<p>You can see the tests I wrote to verify the functionality at <a href="https://github.com/heckj/swiftui-notes/blob/master/UsingCombineTests/DebounceAndRemoveDuplicatesPublisherTests.swift">https://github.com/heckj/swiftui-notes/blob/master/UsingCombineTests/DebounceAndRemoveDuplicatesPublisherTests.swift</a>, and right now I&#8217;m working on <a href="https://github.com/heckj/swiftui-notes/pull/115">a pull request</a> to merge in the change reflecting the current release and illustrating the regression. And before you ask, yes &#8211; I have submitted this as a bug to Apple (FB7424221). If you are relying on the specific functionality of <code>throttle</code> with <code>latest=false</code>, be aware that the latest release of Xcode &amp; Combine is likely going to mess with it.</p>



<p>If you are more curious about all the other tests that were created to support <a href="https://gum.co/usingcombine">Using Combine</a>, then feel free to check out the github repository <a href="https://github.com/heckj/swiftui-notes/">heckj/swiftui-notes</a> &#8211; the tests are in the <a href="https://github.com/heckj/swiftui-notes/tree/master/UsingCombineTests">UsingCombineTests</a> directory, and set up as they&#8217;re own test target in the Xcode project. There are more to write, as I drive down into the various operators, so I do expect more will appear. I won&#8217;t assert that they&#8217;re all amazing, well constructed tests &#8211; but they&#8217;re getting the job done in terms of helping me understand how they work &#8211; and how they don&#8217;t work.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Sharp Knives</title>
		<link>https://rhonabwy.com/2019/09/28/sharp-knives/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sat, 28 Sep 2019 21:46:01 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5608</guid>

					<description><![CDATA[After writing extensively with the Swift programming language, I recently spent time writing code in C++11. The experience has been very instructional, and I know have a much deeper appreciation for the guard rails that the language Swift has constructed. To my surprise, it has also left me with a renewed appreciation of the compiler<a class="more-link" href="https://rhonabwy.com/2019/09/28/sharp-knives/">Continue reading <span class="screen-reader-text">"Sharp Knives"</span></a>]]></description>
										<content:encoded><![CDATA[<p>After writing extensively with the Swift programming language, I recently spent time writing code in C++11. The experience has been very instructional, and I know have a much deeper appreciation for the guard rails that the language Swift has constructed. To my surprise, it has also left me with a renewed appreciation of the compiler tooling &#8211; LLVM, specifically the relative clarity of error messages.</p>
<p>C++ as a language was significantly more approachable to me after working on code using Swift for a while. There are a lot of parallels and similarities, which was particularly nice since I was porting code (re-writing the algorithms in another programming language). I think porting between programming languages is the rough equivalent of translating between human languages. In really comparing to allow you to move between the two, you become far more aware of the idiomatic conveniences and conventions.</p>
<p>One of the surprisingly pieces is my realization that I am far more attached to the swift concept of protocols than I had realized. C++ has a concept of interfaces, which is close &#8211; but definitely not the same in it&#8217;s breadth.</p>
<p>When combined with generic algorithms, the swift language feels far more capable and specific to me. In the swift programming language, you can constrain the implementation of your generic algorithms to only apply to specific protocols, which appears to be something you can&#8217;t easily do in C++ &#8211; or at least in C++11, which was the version I was working within. I found that programming with generics in C++ is far more reliant on convention, or possibly using subclasses &#8211; examples were a bit hard to come by for me.</p>
<p>My limited experience with C++ also leads me to think that the conventions followed between different groups of programmers is more diverse than Swift. The idiomatic patterns I found while reading other groups code had dramatically different patterns included within them. So much so that it was often hard to identify those patterns, and understand what was a team&#8217;s convention.</p>
<p>My time with the C++ components also makes me appreciate all the more the tendency of languages these days (to borrow from Python) to come &#8220;batteries included&#8221; with standard libraries. C++ standard library is more like tools to make tools, and some of the things commonly included with other languages (python, swift, etc) just have to be found individually and assembled.</p>
<p>While I have a bit more to do with C++ before I&#8217;ll be done with my project, I relish shifting back to the guard rails of swift. (I must admit, I&#8217;m now significantly more curious about the language Rust as well).</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>iPad Lost Mode</title>
		<link>https://rhonabwy.com/2019/09/19/ipad-lost-mode/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Thu, 19 Sep 2019 16:23:32 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5603</guid>

					<description><![CDATA[This past Saturday, I was on a 7am EDT flight from Orlando, FL to Seattle, WA &#8211; which means I was up at 3:30am east-coast-time to make the flight. That would be pretty harsh, except that my normal timezone is 4 hours later than that. The flight was smooth, and I half-slept most of the<a class="more-link" href="https://rhonabwy.com/2019/09/19/ipad-lost-mode/">Continue reading <span class="screen-reader-text">"iPad Lost Mode"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>This past Saturday, I was on a 7am EDT flight from Orlando, FL to Seattle, WA &#8211; which means I was up at 3:30am <em>east-coast-time</em> to make the flight. That would be pretty harsh, except that my normal timezone is 4 hours later than that. The flight was smooth, and I half-slept most of the time. I took out my iPad and put it into the seat back pocket to work on it or read a bit during the flight, but never used it.</p>



<p>The following day after we were home, I went to get the iPad &#8211; and realized to my horror that I&#8217;d never pulled it from the seat-back pocket. The data was fortunately backed up, but loosing the hardware &#8211; a gift last Christmas &#8211; wasn&#8217;t a hit I wanted to take. We called the airlines baggage team, and they put in a lost report. I was pretty sure it was gone and resigned myself that I&#8217;d have to replace it some months down the road. I could simply make do until then, feeling foolish for leaving it in the seat pocket when I was half-asleep.</p>



<p>I logged into the Find my iPhone app on my iPhone, and marked the iPad in &#8220;lost mode&#8221;. I wanted it found, and had never used this feature previously. I set up the message that it was lost, and included my phone number &#8211; which shows on the lock screen. It hadn&#8217;t been able to connect to any networks where it already knew the wifi, so it was just &#8220;missing&#8221; from that system, but if and when it did connect &#8211; it would drop into &#8220;locked/lost&#8221; mode.</p>



<p>On Tuesday, the airlines called me and let me know they&#8217;d found an iPad matching my description. After a brief discussion, we verified that it was indeed mine. Within an hour I had several options for getting it back; choosing to have it delivered. I was expecting it to arrive &#8220;some time before 8pm on Wednesday&#8221;.</p>



<p>On Wednesday, I was at <a href="http://www.eldiablocoffee.com">my usual coffee-house haunt</a>, and saw a notification in email &#8211; &#8220;Your iPad has been found&#8221;! I checked the location, and it was my home address! Wait &#8211; the email notification was from 30 minutes ago.. So I called my sweetie and she ran downstairs and found&#8230; <strong><em>nothing!</em></strong></p>



<p>She ran around the outside of the house three times, looking for any place where the box might have been left, but nothing. We immediately thought &#8220;OMG, it&#8217;s been stolen from our front porch&#8221; &#8211; as there&#8217;s <em>a thing</em> in our neighborhood where package theft is pretty common.</p>



<p>After looking again at the Find My Phone app, I realized that while it was &#8220;found&#8221; &#8211; it reported as being on the street in front of our house &#8211; and only for a minute, before disappearing again into the mists of who-knows-where. In hindsight, I imagine that the Fedex driver simply happened to be passing near our house at 9:17a. It was going slowly enough that the iPad was able to finally make a wifi connection it knew and register itself. Fedex, who was delivering it, reported that it was still undelivered, so after an hour or so of &#8220;theft panic&#8221;, that faded into a vague concern and hope that it really was still out for delivery. Fortunately updates from Fedex are nearly real-time, so if they&#8217;re reporting it is still in transit, there can be a reasonable amount of confidence that it is.</p>



<p>Over the course of the day, the Fedex delivery driver wandered around our neighborhood, delivering all their various packages. I got 2 more pings from the iPad, which also reported that it was &#8220;playing a noise&#8221; since it was lost. In both cases, it was near wifi networks that I&#8217;d previously connected with, and it was in those locations for only a minute or two. I can only imagine the driver being confused, if they heard the sound at all, coming from the back of their truck.</p>



<p>It was finally delivered home, around 6pm &#8211; nearly nine hours after first reporting itself in the area. It was in lost mode, and I was able to log in and recover it &#8211; everything intact. I felt extremely relieved, and even more that fortune had sided with me that day. Many friends have done or reported something similar, and devices forever disappear after that scenario.</p>



<blockquote class="wp-block-quote"><p>I should mention that the <a href="https://twitter.com/AlaskaAir/status/1174488658856087552">Alaska Airlines team</a> was fantastic through out this whole nerve-wracking scenario. They were super understanding when I called in panic, thorough about verifying the device was indeed mine when the found it, and efficient after getting the reports filed, made available online to me, and really flexible delivery options &#8211; including allowing me to come pick it up if I wanted.</p></blockquote>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet" data-width="550" data-dnt="true"><p lang="en" dir="ltr"><a href="https://twitter.com/AlaskaAir?ref_src=twsrc%5Etfw">@AlaskaAir</a> I left my iPad Pro in a seat back pocket this weekend after a red-eye flight; thought it was done and gone for good. The Alaska baggage team found it, saved my day, and it arrived back to my home today. Thank you so much!!</p>&mdash; Joseph Heck (@heckj) <a href="https://twitter.com/heckj/status/1174486914163822592?ref_src=twsrc%5Etfw">September 19, 2019</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div>
</div></figure>



<p>The &#8220;lost device&#8221; feature was lovely, except when it wasn&#8217;t. It was like hearing distant calls for help that you can reach in time; sort of like the terrible dream where you can never make it to the end of the hallway to escape whatever dread it coming. I don&#8217;t know that any capability or feature of the iPad that would have made that better, it was just hellishly nerve-wracking to wait while it yelped in the back of that truck, wandering our my neighborhood.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>A Using Combine update now available!</title>
		<link>https://rhonabwy.com/2019/09/18/a-using-combine-update-now-available/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Thu, 19 Sep 2019 00:00:41 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPad]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[combine]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5600</guid>

					<description><![CDATA[A new version of Using Combine (v0.7) is now available!&#160; The free HTML site of Using Combine has been updated automatically, and the PDF and ePub versions are available on Gumroad. This version has relatively few updates, primarily focused on some of the missing publishers and resolving the some of the egregious flaws in ePub<a class="more-link" href="https://rhonabwy.com/2019/09/18/a-using-combine-update-now-available/">Continue reading <span class="screen-reader-text">"A Using Combine update now&#160;available!"</span></a>]]></description>
										<content:encoded><![CDATA[
<p>A new version of Using Combine (v0.7) is now available!&nbsp;</p>



<p>The <a href="https://heckj.github.io/swiftui-notes/">free HTML site of Using Combine</a> has been updated automatically, and the <a href="https://gum.co/usingcombine">PDF and ePub versions are available on Gumroad</a>.</p>



<p>This version has relatively few updates, primarily focused on some of the missing publishers and resolving the some of the egregious flaws in ePub rendering. No significant changes have come with the later Xcode and IOS betas, and with Xcode 11 now in GM release, it was a good time for another update to be made available.</p>



<p>For the next release, I am focusing on fleshing out a number of the not-yet-written reference sections on operators, most of which are more specialized than the more generally used ones that have already been covered.</p>



<p>The project board at <a href="https://github.com/heckj/swiftui-notes/projects/1">https://github.com/heckj/swiftui-notes/projects/1</a> also reflects all the various updates still remaining to be written.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>
	</item>
		<item>
		<title>Using Combine (v0.6) available!</title>
		<link>https://rhonabwy.com/2019/08/11/using-combine-v0-6-available/</link>
		
		<dc:creator><![CDATA[heckj]]></dc:creator>
		<pubDate>Sun, 11 Aug 2019 17:16:53 +0000</pubDate>
				<category><![CDATA[Geekstuff]]></category>
		<category><![CDATA[iPhone]]></category>
		<category><![CDATA[mac]]></category>
		<category><![CDATA[Ranting and Reflections]]></category>
		<category><![CDATA[swift]]></category>
		<category><![CDATA[combine]]></category>
		<guid isPermaLink="false">http://rhonabwy.com/?p=5594</guid>

					<description><![CDATA[A new version of Using Combine is available! The free/online version of Using Combine is updated automatically as I merge changes, and the PDF and ePub versions are released periodically and available on Gumroad. https://gumroad.com/js/gumroad.js Purchase Using Combine The book now has some amazing cover art, designed and provided by Michael Critz, and has benefited<a class="more-link" href="https://rhonabwy.com/2019/08/11/using-combine-v0-6-available/">Continue reading <span class="screen-reader-text">"Using Combine (v0.6)&#160;available!"</span></a>]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img data-attachment-id="5596" data-permalink="https://rhonabwy.com/usingcombinewithswiftgithubsocial/" data-orig-file="https://josephheck.files.wordpress.com/2019/08/usingcombinewithswiftgithubsocial.png" data-orig-size="1280,640" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="usingcombinewithswiftgithubsocial" data-image-description="" data-medium-file="https://josephheck.files.wordpress.com/2019/08/usingcombinewithswiftgithubsocial.png?w=300" data-large-file="https://josephheck.files.wordpress.com/2019/08/usingcombinewithswiftgithubsocial.png?w=900" src="https://josephheck.files.wordpress.com/2019/08/usingcombinewithswiftgithubsocial.png" alt="" class="wp-image-5596" /><figcaption>design by Michael Critz</figcaption></figure>



<p>A new version of <a href="https://gum.co/usingcombine">Using Combine</a> is available! The <a href="https://heckj.github.io/swiftui-notes/">free/online version of Using Combine</a> is updated automatically as I merge changes, and the PDF and ePub versions are released periodically and available on <a href="https://gum.co/usingcombine">Gumroad</a>.</p>



<a href="https://gumroad.com/js/gumroad.js">https://gumroad.com/js/gumroad.js</a>
<a class="gumroad-button" href="https://gum.co/usingcombine">Purchase Using Combine</a>



<p>The book now has some amazing cover art, designed and provided by Michael Critz, and has benefited from updates provided by a number of people, now in the acknowledgements section.</p>



<p>The updates also include a section broken out focusing on developing with Combine, as well as a number of general improvements and corrections.</p>



<p>For the next release, I am going to focus on fleshing out a number of the not-yet-written reference sections:</p>



<p>&#8211; <a href="https://github.com/heckj/swiftui-notes/issues/85">the publishers I haven&#8217;t touched on yet</a></p>



<p>&#8211; starting into a number of the operators, most of which are more specialized</p>



<p>I reviewed the content prior to this release to see what was remaining to be done, and updated the <a href="https://github.com/heckj/swiftui-notes/projects/1">project planning board</a> with the various updates still remaining to be written.</p>



<p>I do expect we&#8217;ll see beta6 from Apple before too long, although exactly when is unknown. I thought it might appear last week, in which case I was planning on trying to accommodate any updates in this release. Xcode 11 beta6 hasn&#8217;t hit the streets and I wanted to get an update regardless of its inclusion.</p>
]]></content:encoded>
					
		
		
		
		<media:content url="https://0.gravatar.com/avatar/cb09f569d2572bd00baec8da68a88201?s=96&#38;d=&#38;r=G" medium="image">
			<media:title type="html">josephheck</media:title>
		</media:content>

		<media:content url="https://josephheck.files.wordpress.com/2019/08/usingcombinewithswiftgithubsocial.png" medium="image" />
	</item>
	</channel>
</rss>
